{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mercury Composable for Java Good news! We have merged our enterprise extension (\"Event Script\") into the Mercury event-driven programming foundation codebase from version 4.2 onwards. It is a comprehensive toolkit to write composable applications including microservices and serverless. The specification for this technology is documented under US Patent application 18/459,307. The source code is provided as is under the Apache 2.0 license. The project is available in both Java and Node.js languages. For Java, please visit Mercury Composable for Java For Node.js, please browse Mercury Composable for Node and Composable-example July 2025 Optimized for Human Composability methodology provides a clear path from Domain Driven Design (DDD), Event Driven Architecture (EDA) to application software design and implementation, connecting product managers, domain knowledge owners, architects and engineers together to deliver high quality products. Optimized for AI The methodology reduces the problem space for AI code assistant because each function is self-contained, independent and I/O is immutable. In addition, Event Script is a Domain Specific Language (DSL) that can be understood by AI agent with some fine-tuning, thus making the whole ecosystem AI friendly. Getting Started A composable application is designed in 3 steps: Describe your use case as an event flow diagram Create a configuration file to represent the event flow Write a user story for each user function To get started, please visit Chapter 1, Developer Guide and Methodology . We will illustrate the methodology with a composable application example. Conquer Complexity: Embrace Composable Design Introduction Software development is an ongoing battle against complexity. Over time, codebases can become tangled and unwieldy, hindering innovation and maintenance. This article introduces composable design patterns, a powerful approach to build applications that are modular, maintainable, and scalable. The Perils of Spaghetti Code We have all encountered it: code that resembles a plate of spaghetti \u2013 tangled dependencies, hidden logic, and a general sense of dread when approaching modifications. These codebases are difficult to test, debug, and update. Composable design patterns offer a solution. Evolution of Design Patterns Software development methodologies have evolved alongside hardware advancements. In the early days, developers prized efficiency, writing code from scratch due to limited libraries. The rise of frameworks brought structure and boilerplate code, but also introduced potential rigidity. Functional Programming and Event-Driven Architecture Functional programming, with its emphasis on pure functions and immutable data, paved the way for composable design. This approach encourages building applications as chains of well-defined functions, each with a clear input and output. Event-driven architecture complements this approach by using events to trigger functions. This loose coupling promotes modularity and scalability. The Power of Composable Design At its core, composable design emphasizes two principles: Self-Contained Functions : Each function is a well-defined unit, handling its own logic and transformations with minimal dependencies. Event Choreography : Functions communicate through events, allowing for loose coupling and independent execution. Benefits of Composable Design Enhanced Maintainability : Isolated functions are easier to understand, test, and modify. Improved Reusability : Self-contained functions can be easily reused across different parts of your application. Superior Performance : Loose coupling reduces bottlenecks and encourages asynchronous execution. Streamlined Testing : Well-defined functions facilitate unit testing and isolate potential issues. Simplified Debugging : Independent functions make it easier to pinpoint the source of errors. Technology Agnostic : You may use your preferred frameworks and tools to write composable code, allowing for easier future adaptations. Implementing Composable Design While seemingly simple, implementing composable design can involve some initial complexity. Here's a breakdown of the approach: Function Design : Each function serves a specific purpose, with clearly defined inputs and outputs. Event Communication : Functions communicate through well-defined events, avoiding direct dependencies. Choreography : For each event flow instance, an event manager, with a state machine and event flow configuration, sequences and triggers functions based on events. Conclusion Composable design patterns offer a powerful paradigm for building maintainable, scalable, and future-proof applications. By embracing the principles of self-contained functions and event-driven communication, you can conquer complexity and write code that is a joy to work with. Source : This article was summarized from composable technology papers using AI in January 2025","title":"Home"},{"location":"#mercury-composable-for-java","text":"Good news! We have merged our enterprise extension (\"Event Script\") into the Mercury event-driven programming foundation codebase from version 4.2 onwards. It is a comprehensive toolkit to write composable applications including microservices and serverless. The specification for this technology is documented under US Patent application 18/459,307. The source code is provided as is under the Apache 2.0 license. The project is available in both Java and Node.js languages. For Java, please visit Mercury Composable for Java For Node.js, please browse Mercury Composable for Node and Composable-example July 2025","title":"Mercury Composable for Java"},{"location":"#optimized-for-human","text":"Composability methodology provides a clear path from Domain Driven Design (DDD), Event Driven Architecture (EDA) to application software design and implementation, connecting product managers, domain knowledge owners, architects and engineers together to deliver high quality products.","title":"Optimized for Human"},{"location":"#optimized-for-ai","text":"The methodology reduces the problem space for AI code assistant because each function is self-contained, independent and I/O is immutable. In addition, Event Script is a Domain Specific Language (DSL) that can be understood by AI agent with some fine-tuning, thus making the whole ecosystem AI friendly.","title":"Optimized for AI"},{"location":"#getting-started","text":"A composable application is designed in 3 steps: Describe your use case as an event flow diagram Create a configuration file to represent the event flow Write a user story for each user function To get started, please visit Chapter 1, Developer Guide and Methodology . We will illustrate the methodology with a composable application example.","title":"Getting Started"},{"location":"#conquer-complexity-embrace-composable-design","text":"","title":"Conquer Complexity: Embrace Composable Design"},{"location":"#introduction","text":"Software development is an ongoing battle against complexity. Over time, codebases can become tangled and unwieldy, hindering innovation and maintenance. This article introduces composable design patterns, a powerful approach to build applications that are modular, maintainable, and scalable.","title":"Introduction"},{"location":"#the-perils-of-spaghetti-code","text":"We have all encountered it: code that resembles a plate of spaghetti \u2013 tangled dependencies, hidden logic, and a general sense of dread when approaching modifications. These codebases are difficult to test, debug, and update. Composable design patterns offer a solution.","title":"The Perils of Spaghetti Code"},{"location":"#evolution-of-design-patterns","text":"Software development methodologies have evolved alongside hardware advancements. In the early days, developers prized efficiency, writing code from scratch due to limited libraries. The rise of frameworks brought structure and boilerplate code, but also introduced potential rigidity.","title":"Evolution of Design Patterns"},{"location":"#functional-programming-and-event-driven-architecture","text":"Functional programming, with its emphasis on pure functions and immutable data, paved the way for composable design. This approach encourages building applications as chains of well-defined functions, each with a clear input and output. Event-driven architecture complements this approach by using events to trigger functions. This loose coupling promotes modularity and scalability.","title":"Functional Programming and Event-Driven Architecture"},{"location":"#the-power-of-composable-design","text":"At its core, composable design emphasizes two principles: Self-Contained Functions : Each function is a well-defined unit, handling its own logic and transformations with minimal dependencies. Event Choreography : Functions communicate through events, allowing for loose coupling and independent execution.","title":"The Power of Composable Design"},{"location":"#benefits-of-composable-design","text":"Enhanced Maintainability : Isolated functions are easier to understand, test, and modify. Improved Reusability : Self-contained functions can be easily reused across different parts of your application. Superior Performance : Loose coupling reduces bottlenecks and encourages asynchronous execution. Streamlined Testing : Well-defined functions facilitate unit testing and isolate potential issues. Simplified Debugging : Independent functions make it easier to pinpoint the source of errors. Technology Agnostic : You may use your preferred frameworks and tools to write composable code, allowing for easier future adaptations.","title":"Benefits of Composable Design"},{"location":"#implementing-composable-design","text":"While seemingly simple, implementing composable design can involve some initial complexity. Here's a breakdown of the approach: Function Design : Each function serves a specific purpose, with clearly defined inputs and outputs. Event Communication : Functions communicate through well-defined events, avoiding direct dependencies. Choreography : For each event flow instance, an event manager, with a state machine and event flow configuration, sequences and triggers functions based on events.","title":"Implementing Composable Design"},{"location":"#conclusion","text":"Composable design patterns offer a powerful paradigm for building maintainable, scalable, and future-proof applications. By embracing the principles of self-contained functions and event-driven communication, you can conquer complexity and write code that is a joy to work with. Source : This article was summarized from composable technology papers using AI in January 2025","title":"Conclusion"},{"location":"CHANGELOG/","text":"Changelog Release notes All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Note : Some version numbers may be skipped to align feature set with the Node.js version. Version 4.3.23, 10/6/2025 Added State machine monitor feature in EventScriptMock class Updated MultiLevelMap to support a new \"getElements\" method to retrieve elements using wildcard index \"*\". Removed N/A Changed OSS update: spring boot version 3.5.6 vertx version 5.0.4 netty version 4.2.6.Final Version 4.3.22, 9/28/2025 Added Add envInstances parameter in no.op and resilience.handler to allow developer to override the default maximum worker instance count. Removed N/A Changed N/A Version 4.3.21, 8/30/2025 Added Enable connection timeout in AsyncHttpClient with a new parameter in application.properties to support fail-fast when making HTTP requests. http.client.connection.timeout (default value of 5000, unit in milliseconds) Removed N/A Changed N/A Version 4.3.20, 8/28/2025 Added error.task in the error namespace to map to the original task that throws exception @retry keyword in the first next task list in an exception handler tells the system to automatically resolve the original task. Removed N/A Changed Chapter 4 of the Developer Guide updated for the new feature for error handling Version 4.3.19, 8/27/2025 Added Performance metrics for each task in the end-of-flow report Removed N/A Changed move \"NoOp\" composable function from event-script-engine to platform-core adjust concurrency for no op, actuator service, event over http, resilience handler and simple exception handler virtual thread optimization runs at application start-up phase Version 4.3.18, 8/26/2025 Added N/A Removed N/A Changed Streamlined subflow routing in TaskExecutor of the event-script-engine. Changed subflow RPC call to asynchronous callback for performance optimization. Version 4.3.17, 8/22/2025 Added Validation logic to filter out CR/LF for headers, cookies and session info when creating an AsyncHttpRequest from a map. For example, when using the \"AsyncHttpClient by configuration\" method, the AsyncHttpRequest is created by a map of key-values. The additional validation prevents creating headers and cookies with CR/LF accidentially. Removed N/A Changed N/A Version 4.3.16, 8/21/2025 Added log4j2.xml in standalone-kafka-server to reduce logging noise Removed Update standalone-kafka-server's pom.xml for unused dependencies: apache commons-beanutils apache commons-digester apache commons-logging Changed Update OSS versions: Spring Boot 3.5.5 Kafka client and server 4.0.0 Gson 2.13.1 junit-bom 5.13.4 Version 4.3.15, 8/20/2025 Added CR/LF are filtered as a space in the EventEnvelope's setHeader and EventEmitter's asEnvelope methods Avoid \"path traversal\" attack by rejecting relative parent file path in configuration Removed N/A Changed N/A Version 4.3.14, 8/19/2025 Added To reduce ambiguity in event script configuration, created an alias for model.parent. namespace as model.root. The alias is implemented using a memory reference for lowest memory and processing overheads. Removed N/A Changed CompileFlows and TaskExecutor classes are updated to support the model.root. namespace alias. Version 4.3.13, 8/16/2025 Added Reactor HttpClient Support the use of different upload tags for multipart file upload \"content-length\" HTTP header integrity protection logic added Removed Vertx HttpClient OutputStreamQueue class Changed Update AsyncHttpClient to change from Vertx to Reactor HttpClient Change the concurrency of AsyncHttpClient and TemporaryIndex to 500, matching the underlying HTTP client's connection pool Upgrade reactor-bom to version 2024.0.9 that fetches reactor-netty-core version 1.2.9 Upgrade netty to version 4.2.4.Final Version 4.3.12, 8/11/2025 Added N/A Removed Content-length is not required to be set when uploading multiple files using multipart/form-data protocol. Changed N/A Version 4.3.11, 8/11/2025 Added N/A Removed N/A Changed Bug fix: Environment variable resolution in the \"load\" method of the ConfigReader class. Updated model variable string substitution to pass through non-model text. Version 4.3.10, 8/7/2025 Added Support runtime string substitution using a model variable Removed N/A Changed ConfigReader refactored to reduce code complexity to 15 or less as per SonarQube's requirement Version 4.3.9, 8/6/2025 Added Support multipart upload for one or more files in both AsyncHttpClient and reactive HTTP server. Removed N/A Changed OSS updates: Spring Boot parent version 3.5.4 Classgraph version 4.8.181 Vertx version 5.0.2 Version 4.3.8, 7/29/2025 Added Static methods in EventEnvelope and PostOffice to instantiate new objects option to render input file content as \"json\" Removed N/A Changed N/A Version 4.3.7, 7/26/2025 Added N/A Removed N/A Changed Refactor event-script-engine to comply with SonarQube's code complexity requirement of 15 or less Rename DistributedTrace class to \"Telemetry\" for compatibility with node.js version Version 4.3.6, 7/23/2025 Added new \"eRequest\" APIs for EventEmitter and PostOffice Removed N/A Changed restore original return type of java.util.concurrent.Future for the \"request\" APIs that in turns call the new \"eRequest\" APIs Update netty to version 4.2.3.Final Version 4.3.5, 7/22/2025 Added Support 2nd Kafka server in the kafka-standalone subproject. Removed N/A Changed Refactor platform-core to comply with SonarQube's code complexity requirement of 15 or less Update WorkerHandler to catch NoClassDefFoundError, AssertionError and Exception PostOffice's request API provides CompletableFuture instead of Future to support both reactive and sequential coding style MsgPack updated to version 0.9.10 Version 4.3.4, 7/3/2025 Added N/A Removed Message queuing mechanism in JSON and COMPACT loggers Changed Updated OSS dependency - Kafka-client version 3.9.1 Version 4.3.3, 6/26/2025 Added Warm up logic for Java 21 virtual thread system Removed The setBodyWithDefaultSerialization() method in EventEnvelope is retired Changed Improve CompileFlow error message. The new error message will tell where the error comes from. Open source dependency update below. Spring Boot parent version 3.5.3, Classgraph version 4.8.180, Netty version 4.2.2.Final Version 4.3.2, 6/23/2025 Added Support file \"append\" mode in output data mapping Dynamic fork-n-join feature for parallel processing of a list of elements by multiple instances of the same task Removed N/A Changed N/A Version 4.3.1, 6/12/2025 Added Add kotlin example Removed N/A Changed N/A Version 4.3.0, 6/10/2025 In this version, we have retired the support of Kotlin suspend function. Therefore, the project is 100% pure Java. In addition to dropping Kotlin dependency, it also streamlines exception handling by replacing IOException with IllegalArgumentException. Your applications may need minor refactoring of try-catch that uses the ConfigReader, PostOffice and Platform APIs. In most cases, you can just remove the try-catch block. In some cases, you can change the IOException to IllegalArgumentException in the try-catch block for best compatibility with your original code. Composable applications using Event Script should not be affected because composable functions are self-contained and independent. They usually do not invoke low level PostOffice and Platform APIs. Added N/A Removed TypedKotlinFunction and kotlin dependency Serial ID for AppException Changed Update AppException to extend RuntimeException Replace IOException with IllegalArgumentException for ConfigReader, PostOffice and Platform APIs. The above changes remove the requirement to do explicit try-catch in normal use cases, thus simplifying application code. It is a best practice to let run-time exception throw through the chain. Version 4.2.46, 6/2/2025 Added N/A Removed AcknowledgeResult is not required in Vertx 5.0 Changed Improved negate type mapping logic Updated OutputStreamQueue to use new API for Vertx WriteStream Refactor HttpsTest to use Vertx 5.0 web client API OSS version updates as follows Spring Boot 3.5.0 Vertx 5.0.0 netty 4.2.1.Final Kotlin 2.1.21 reactor-bom 2024.0.6 junit-bom 5.13.0 Version 4.2.45, 5/31/2025 Added \"Empty array index\" syntax in data mapping to append an element to an array in a dataset Allow text constant in data mapping to contain any characters including the mapping signature \"->\" Removed N/A Changed Bugfix for certain edge cases in detecting a non-exist key in a MultiLevelMap Version 4.2.44, 5/28/2025 Added N/A Removed N/A Changed Add unit test for https server Version 4.2.43, 5/28/2025 Added Introduce \"datatype\" in output data mapping of event script Snake case and camel case selection in PreLoad annotation to override default serialization case strategy Removed N/A Changed Event Script improvement - avoid endless loop when top-level exception handler throws exception itself Version 4.2.42, 5/24/2025 Added SSL http server options Removed N/A Changed N/A Version 4.2.42, 5/17/2025 Added dynamic model variable as index to address an array element (support LHS and RHS mapping) updated documentation of configuration management in Appendix-I of developer guide Removed N/A Changed N/A Version 4.2.40, 5/9/2025 Added support of \"flows\" in the \"modules.autostart\" feature \"length\" type matching feature \"dynamic model variables in for-loop\" - model variable in comparator's left and/or right hand sides Removed N/A Changed N/A Version 4.2.39, 5/4/2025 Added Code example to illustrate how to write your own Flow Adapters \"modules.autostart\" feature to invoke composable functions and libaries at start up. Removed N/A Changed N/A Version 4.2.38, 4/30/2025 Added Spring auto-wiring of composable functions when leveraging the rest-spring-3 module Removed N/A Changed N/A Version 4.2.37, 4/29/2025 Added N/A Removed N/A Changed Windows compatibility in static file handling - convert Windows file path to Unix path Version 4.2.35, 4/24/2025 Added N/A Removed N/A Changed Bugfix for pipeline that contains only one task Version 4.2.33, 4/24/2025 Added N/A Removed Removed SimpleScheduler from the build script at root because SimpleScheduler has not reached production quality Changed Rename variable as per SonarCube quality gate OSS version update - revert Kafka client to version 3.9.0 for compatibility with Confluent Kafka Version 4.2.32, 4/24/2025 Added N/A Removed N/A Changed Improvement in custom content-type resolver OSS version update vertx 4.5.14 guava 33.4.8-jre junit5-bom 5.12.2 kotlin 2.1.20 Kafka client 4.0.0 gson 2.13.0 Version 4.2.31, 4/23/2025 Added N/A Removed N/A Changed Minor update to address code smells reported by SonarCube analyzer Version 4.2.30, 4/22/2025 Added Support \"matrix parameters\" and \"hash parameters\" in HTTP request URI in platform-core \"classpath\" in LHS of output data mapping for event script Removed N/A Changed N/A Version 4.2.29, 4/20/2025 Added Perform \"path traversal\" avoidance when decoding incoming HTTP requests Removed Remove \"public\" qualifier from unit tests since JUnit version 5 does not need it Changed Minor refactoring to remove majority of code smells as per SonarQube static code analysis Support custom error message in EventEnvelope Version 4.2.28, 4/17/2025 Added Unit test and updated developer guide to illustate use of AsyncHttpClient in Event Script. Removed default application.properties files in platform-core, rest-spring-3 and event-script-engine HTML escape characters in URI path handling in AsyncHttpClient Changed N/A Version 4.2.27, 3/31/2025 Added N/A Removed N/A Changed Streamline error handling in TaskExecutor to sync up with Node.js version Update developer guide's chapter-4 for the output data mapping paragraph about file Version 4.2.26, 3/25/2025 Added N/A Removed N/A Changed Use EventInterceptor pattern for the resilience handler for non-blocking deferred response Version 4.2.25, 3/24/2025 Added N/A Removed N/A Changed Rename \"alternate\" parameter in resilience handler to \"alternative\" Version 4.2.24, 3/24/2025 Added Generic resilience handler with alternative path and backoff features Removed N/A Changed The getError() method in EventEnvelope is updated to return encoded error message. This is required for distributed trace processing and proper error handling of subflows. Delete file when mapping a null value from the LHS to the RHS that is defined as a file, thus allowing clearing of temporary data files in a flow. OSS update - spring boot parent version 3.4.4 and io.project.reactor bom version 2024.0.4 Version 4.2.23, 3/12/2025 Added N/A Removed N/A Changed For security, the parent state machine (namespace \"model.parent\") is a protected resource. It can only be shared by the primary flow and all sub-flow instances that are instantiated from it. Version 4.2.22, 3/11/2025 Added N/A Removed N/A Changed All sub-flows instantiated from a primary flow can access the same parent state machine using the \"model.parent\" namespace. Version 4.2.21, 3/8/2025 Added Support flow and function for external state machine Parent state machine for sub-flow Validation rules to reject access to the whole model or parent namespace Removed N/A Changed N/A Version 4.2.20, 2/28/2025 Added \"spring.boot.main=org.platformlambda.rest.RestServer\" added to application.properties so that developer may override it with their own Spring Boot initializer. Removed property setting for Netty version 4.1.118.Final is no longer required in pom.xml because the updated spring boot parent version 3.4.3 will fetch 4.1.118 correctly. Changed upgrade spring boot version 3.4.3 Version 4.2.19, 2/26/2025 Added N/A Removed N/A Changed Allow developer to load base configuration files from the classpath or from the local file system. Version 4.2.18, 2/21/2025 Added java.sql.Timestamp data type added to SimpleMapper simple type matching feature is extended with a new string 'concat' method default REST endpoints for /api/event and actuator services Removed N/A Changed Sort REST endpoints for orderly loading Drop \"async.http.request\" RPC traces to reduce observability noise Version 4.2.17, 2/20/2025 Added LocalDate and LocalTime data type added to SimpleMapper Removed N/A Changed N/A Version 4.2.15, 2/15/2025 Added N/A Removed N/A Changed Update actuator output data structures to be consistent with Composable Node.js implementation Version 4.2.14, 2/14/2025 Added N/A Removed N/A Changed Use different route names for various actuator services to avoid hardcode of URLs Version 4.2.13, 2/13/2025 Added Actuator REST endpoints are now configurable in rest.yaml Removed The feature to shutdown, suspend and resume of an application instance is retired Changed Update actuator services to serve REST requests directly Version 4.2.12, 2/12/2025 Added N/A Removed N/A Changed Use ServerCookieEncoder.STRICT.encode() method to detect invalid cookie value Update vertx to 4.5.13 and Netty to 4.1.118.Final to address security vulnerabilities Version 4.2.11, 2/11/2025 Added Support of Spring active profiles using JVM parameter \"-Dspring.profiles.active\" or environment variable \"SPRING_PROFILES_ACTIVE\" Removed N/A Changed Developer Guide's Appendix-I updated for the Spring active profile feature Version 4.2.10, 2/10/2025 Added PoJo class hint and custom serializer in FluxConsumer Optional custom serializer in FluxPublisher Removed N/A Changed WorkerHandler and WorkerQueue classes for FluxConsumer Developer Guide's Chapter 2 Flow diagrams for sample app in Developer Guide Version 4.2.9, 2/8/2025 Added uuid generation feature in Event Script's simple type matching inputPoJoClass parameter in PreLoad annotation Removed N/A Changed For feature completeness, the system now supports list of pojo as one type of event input for sending event programmatically. However, Event Script's input data mapping is configuration driven and thus list of pojo is not permitted. Version 4.2.8, 2/6/2025 In this release, we have improved EventEnvelope and Trace Annotation features and tested interoperability with applications written in version 2 and 3. Added Support text, map and list in trace annotation Removed The \"extra\" and \"end-of-route\" fields in EventEnvelope are retired Changed Updated EventEnvelope data structure for direct support of tags and annotations Version 4.2.7, 2/4/2025 Added N/A Removed redundant and inconsistent log4j2.xml config files from subprojects Changed getBodyAsListOfPoJo method in EventEnvelope updated endFlow method of TaskExecutor sends event to distributed trace instead of logging Version 4.2.6, 2/3/2025 Added N/A Removed N/A Changed Observability bugfix to set execution time. In earlier iteration, input events to the PostOffice are set to be immutable. However, the execution time in a reply event was missing in the cloned event. Version 4.2.5, 2/2/2025 Added Add 3-part syntax for Event Script's data mapping processing. Supports the following data mapping syntax: LHS -> RHS LHS -> model.variable -> RHS Removed N/A Changed Make input event immutable to PostOffice's send and request API Consistent temporary stream folder name for Java and Node.js under /tmp/composable Version 4.2.4, 1/30/2025 Added Temporary Inbox handler for optimization of request-response event processing Removed FastRPC API removed from the Kotlin subsystem since Kotlin can use Mono/Flux for reactive programming Changed Developer guide updated accordingly Essential services are started as the first \"BeforeApplication\" using sequence 0 Event Script runs as the second \"BeforeApplication\" using sequence 2 BeforeApplication sequence 1 is reserved to handle rare use case that a module must run before event script Version 4.2.3, 1/28/2025 Added Support of negate operator of a model value in event script added to the \"simple type matching\" feature Removed N/A Changed Use virtual thread to decouple JSON logging to optimize application performance. Updated the following OSS versions Spring Boot version 3.4.2 Reactor-bom 2024.0.2 Vertx-core 4.5.12 MsgPack 0.9.9 Version 4.2.2, 1/22/2025 Added N/A Removed N/A Changed Simplify JSON logging using the parameter formatter {} with a map parameter. The map will be printed as JSON when log.format=json is set in application.properties. This also slightly improves logging performance. Version 4.2.1, 1/21/2025 Added N/A Removed N/A Changed reconfigure logger to json or compact format early when app starts Version 4.2.0, 1/20/2025 This is a milestone release for consistent features and behaviors between Java and Node.js versions Added Composable methodology in developer guide Removed URI path from errorPage.html Changed N/A Version 4.1.8, 1/17/2025 Added Notice in Javadoc to identify system reserved classes Removed N/A Changed log.format parameter changed to be case insensitive Version 4.1.7, 1/16/2025 Added Limit stack trace transport to a max of 10 lines for transport efficiency because the original stack trace can be retrieved with the getException() method Flow tests for composable-example application Removed N/A Changed Update composable-example to be consistent with Node.js version Change distributed trace to log as pretty print JSON Clean up log4j XML configuration templates in platform-core to be used as examples Version 4.1.6, 1/15/2025 Added Support 3 logging formats using the log.format parameter in application.properties text - default to text string output compact - json output without linefeed for redirection to log analytics system json - pretty print for human readers Removed N/A Changed N/A Version 4.1.5, 1/14/2025 Added N/A Removed Custom Map/List deserializers Changed Apply ToNumberPolicy.LONG_OR_DOUBLE to GSON serialization of untyped numbers in hash map. Version 4.1.4, 1/13/2025 Added Task list included in \"end of flow\" logging Pipeline-while-loop unit tests EventScriptMock helper class to override a function route for a flow task Removed Removed the \"threshold\" feature in variable HTTP payload in REST automation for consistent syntax with Node.js Composable version Changed Update the setException method in EventEnvelope to handle non-serializable exception Improved event script's pipeline for-loop-continue feature Normalize dataset when loading new configuration using ConfigReader Version 4.1.3, 1/1/2025 Added N/A Removed N/A Changed Improved configuration management and refactored AppConfigReader, ConfigReader and MultiLevelMap classes Input to MultiLevelMap is now immutable Simplified event script's pipeline condition syntax Consistent exception transport for Java and Node.js composable applications Bugfix to handle untyped map inside a PoJo OSS updates as follows. Spring Boot parent version 3.4.1 Kotlin version 2.1.0 Spring Project Reactor version 3.7.1 (BOM version 2024.0.1) Google Guava version 33.4.0-jre JUnit version 5.11.4 Version 4.1.2, 12/20/2024 Added Unit test in event-script-engine to validate that the config management system can merge parameters in application.yml and application.properties. Removed N/A Changed N/A Version 4.1.1, 12/18/2024 Added \"map\" constant type in input data mapping AppConfigReader will resolve key-values from system properties and environment variables at startup Removed N/A Changed Updated Chapter-4 for the new \"map\" constant feature. Version 4.1.0, 12/11/2024 This milestone version achieves ideal event choreography by removing additional event routing to and from the Event Manager. This would boost internal event routing performance by 50 percent. Added Performance optimization for Event Script Removed N/A Changed The platform-core module uses virtual threads to execute event.script.manager and task.executor directly to eliminate additional serialization overheads since the two functions are event routers themselves. Version 4.0.33, 12/11/2024 Added Support of custom content types in application.yml Removed N/A Changed Improved websocket housekeeping logic Use bench.add to replace bench.offer API Version 4.0.32, 12/9/2024 Added For completeness, added Boolean AND and OR operations for simple type matching. Added traceId as metadata for a flow instance Removed N/A Changed Update Chapter-4 for the new AND/OR type matching feature Consistent custom HTTP headers for event over http protocol and streaming content Version 4.0.31, 12/5/2024 Added N/A Removed N/A Changed The \"keep.original\" key is renamed as \"keep-original\" to comply with convention. Continue processing if some preload override config files are missing. Version 4.0.30, 12/5/2024 Added Implemented unique task naming feature for event flow configuration. Removed N/A Changed The \"keep_original\" key is renamed as \"keep.original\" in preload override Chapter-4 of developer guide updated with the new task alias feature Version 4.0.29, 12/3/2024 Added Added integer, long, float, double and boolean type matching for state machine. Removed N/A Changed N/A Version 4.0.28, 11/29/2024 Added Support for simple data type matching processing (text, substring, binary and b64) Optional external state machine Removed Removed \"http.input.\" and \"http.output.\" aliases from event script. Instead, use the generic \"input.\" and \"output.\" namespaces. Changed Bugfix for AsyncHttpClient to allow missing HTTP request body in POST, PUT or PATCH request Mono reactive flow control Version 4.0.27, 11/27/2024 Added Support for Mono/Flux return type for KotlinLambdaFunction Implemented Websocket handshake handler to adjust to API changes in vertx 4.5.11 Removed N/A Changed N/A Version 4.0.26, 11/26/2024 Added N/A Removed Remove pom.xml version override for netty and spring framework because Spring Boot 3.4.0 fetches the correct versions of netty and spring framework. Earlier override was done to avoid security vulnerabilities of older versions of netty and spring framework. Changed Handle the case that Mono will not return payload if the payload is null OSS update: Classgraph 4.8.179, Vertx 4.5.11, Spring Boot 3.4.0, Kafka Client 3.9.0 Version 4.0.25, 11/21/2024 Added Support more than one REST configuration files. When a duplicated REST entry is detected, the system will abort REST endpoint rendering and print out an error message in application log. If you have unit tests to cover the REST endpoints, the unit tests will fail accordingly. Removed N/A Changed Improved environment variable parsing in config reader. System will skip entries with invalid environment variable reference syntax. Version 4.0.24, 11/20/2024 Added N/A Removed N/A Changed Bugfix for an edge case in config reader to handle control character of brackets inside an environment variable reference. e.g. some.key=${ENV_VAR:something/{test1}/{test2}} Version 4.0.23, 11/19/2024 Added N/A Removed ObjectStreamWriter and AsyncObjectStreamReader are removed Changed Replace ObjectStreamWriter with FluxPublisher Replace AsyncObjectStreamReader with FluxConsumer Bugfix for FluxConsumer expiry - change type from \"data\" to \"exception\". Version 4.0.22, 11/18/2024 Added FluxPublisher and FluxConsumer for integration with Flux reactive response object Removed N/A Changed Unit tests in event streaming and post office to support Flux integration Select reactor-core version 3.7.0 using dependency management (reactor-bom version 2024.0.0) Version 4.0.21, 11/14/2024 Added Support for user function to return a Mono reactive response object Removed N/A Changed Update netty to version 4.1.115.Final to address security vulnerability in 4.1.114 Move reactor-core library from rest-spring-3 to platform-core Version 4.0.20, 11/13/2024 Added For ease of configuration, added \"com.accenture\" to the base packages so that user applications do not need to include it to use the event-script-engine module. Removed if-then-else pipeline feature in event-script Changed Update Event Script syntax for consistency Fix error in counting number of compiled flows Version 4.0.16, 11/10/2024 Added Generate unique flow instance ID as reference during flow execution. Removed N/A Changed Save the original correlation-ID from the calling party in a flow instance and return this value to the calling party at the end of flow execution. Version 4.0.15, 11/7/2024 Added N/A Removed N/A Changed renamed StartFlow to FlowExecutor Version 4.0.14, 11/7/2024 Added N/A Removed N/A Changed Health check function can return either a text string or a Map StartFlow API updates Version 4.0.13, 11/5/2024 Added Added helper class \"StartFlow\" to start a flow, including internal flows without HTTP or Kafka. Removed N/A Changed Bugfix for empty YAML file to avoid null pointer exception Sort event scripts for orderly logging in the CompileFlows validation process Version 4.0.12, 10/31/2024 Added New feature to support resolution of more than one environment variable for a parameter using the ConfigReader Removed N/A Changed Update OSS modules 1. classgraph version 4.8.177 2. kotlin version 2.0.21 3. guava version 33.3.1-jre 4. jUnit version 5 jupiter Adjusted all unit tests to use jUnit 5 Version 4.0.11, 10/28/2024 Added New features to support: 1. multiple preload override config file 2. multiple flow list config files Removed unused class \"UnauthorizedObj\" in platform-core commons-io dependency in Kafka-Standalone subproject Changed Unit test for the preload override feature JavaDoc for the MainApplication Version 4.0.10, 10/24/2024 Added N/A Removed N/A Changed OSS update - Spring Boot 3.3.5 Security patch for CR/LF exploit for HTTP cookie Version 4.0.9, 10/18/2024 Added Added Kafka Raft for the Kafka-standalone app. Removed Removed zookeeper from Kafka-standalone app. Changed Update spring framework verison 6.1.14 to avoid vulnerability in webflux Version 4.0.8, 10/9/2024 Added Partial support of Active Profile using the \"spring.profiles.active\" parameter Hierarchy of flows Removed N/A Changed N/A Version 4.0.7, 10/1/2024 Added A generic \"no-op\" function for use in event scripts. Removed Feature to ping a function without payload and headers. Changed Simplified api-playground application Version 4.0.6, 9/27/2024 Added HTTP request Cookie value filtering using RFC-6265 strict syntax Removed Automatic index page redirection filter for Spring Boot Changed Upgrade SHA-1 to SHA-512 algorithm in CryptoAPI utility Fix security vulnerability associated with HTTP request header and cookie manipulation Version 4.0.5, 9/24/2024 Added N/A Removed Feature for automatic PoJo transport in EventEnvelope and MsgPack Feature for safe.data.model deserialization Benchmark-server is no longer required Changed Update OSS versions - vertx 4.5.10, kotlin 2.0.20, spring boot 3.3.4 Version 4.0.4, 9/5/2024 Added New feature for AsyncHttpClient to render small streaming HTTP response (i.e. chunked binary data) as byte array. For details, Please refer to Appendix III, Developer Guide Removed N/A Changed Bugfix for parsing default value of environment variable in ConfigReader. This resolves an issue when the special character colon (\":\") is used more than once in the default value. Version 4.0.3, 9/4/2024 Added The \"preload override\" feature is added. This allows overriding a reusable composable library with a set of new route names that are unique for use in an event flow configuration script. For details, Please refer to Chapter 4, Developer Guide Removed N/A Changed N/A Version 4.0.2, 8/31/2024 Added New \"classpath\" namespace for input data mapping Support for input data mapping to handle subset of input request body as a Map or PoJo Removed N/A Changed Remove the class \"type\" variable from AsyncHttpRequest Improve the \"removeElement\" method in MultiLevelMap Make HTTP input request header labels key-insensitive Update Spring Boot to version 3.3.3 Version 4.0.1, 8/19/2024 Added new File read/write feature in Event Script's I/O data mapping Removed N/A Changed Update Spring Boot to version 3.3.2 Update Guava to version 33.3.0-jre Update Vertx to version 4.5.9 Update Kotlin to version 2.0.10 Change \"upstream\" to \"dependency\" in the \"/health\" endpoint Version 4.0.0, 6/24/2024 This version merges Event Script into the Mercury Composable repository. Added N/A Removed N/A Changed Update Spring Boot to version 3.3.1 Update Guava to version 33.2.1-jre Update Vertx to version 4.5.8 Update Kotlin to version 2.0.0 Update classgraph to version 4.8.174 Optional reply event for a flow configuration Kafka-standalone is still using Spring Boot 3.2.5 due to compatibility issue Version 3.1.5, 5/1/2024 This version supercedes 3.1.4 due to updated data structure for static content handling. Added Added optional static-content.no-cache-pages in rest.yaml AsyncHttpClientLoader Removed N/A Changed Updated data structure for static-content section in rest.yaml Fixed bug for setting multiple HTTP cookies Unified configuration file prefix \"yaml.\" Version 3.1.4, 4/28/2024 Added Added optional static content HTTP-GET request filter in rest.yaml Removed N/A Changed Updated syntax for static-content-filter Version 3.1.3, 4/24/2024 Added N/A Removed N/A Changed Enhanced OptionalService annotation. Version 3.1.2, 4/17/2024 Added Added \"app-config-reader.yml\" file in the resources folder so that you can override the default application configuration files. Removed N/A Changed Open sources library update (Spring Boot 3.2.5, Vertx 4.5.7) Improve AppConfigReader and ConfigReader to use the app-config-reader.yml file. Enhanced OptionalService annotation. Version 3.1.1, 2/8/2024 Added AutoStart to run application as Spring Boot if the rest-spring-3 library is packaged in app Configurable \"Event over HTTP\" - automatic forward events over HTTP using a configuration Support user defined serializer with PreLoad annotation and platform API Removed Bugfix: removed websocket client connection timeout that causes the first connection to drop after one minute Changed Open sources library update (Spring Boot 3.2.2, Vertx 4.5.3 and MsgPack 0.9.8) Rename application parameter \"event.worker.pool\" to \"kernel.thread.pool\" Version 3.1.0, 1/5/2024 Added Full integration with Java 21 Virtual Thread Default execution mode is set to \"virtual thread\" KernelThreadRunner annotation added to provide optional support of kernel threads Removed Retired Spring Boot version 2 Hazelcast and ActiveMQ network connectors Changed platform-core engine updated with virtual thread Version 3.0.7, 12/23/2023 Added Print out basic JVM information before startup for verification of base container image. Removed Removed Maven Shade packager Changed Updated open sources libraries to address security vulnerabilities Spring Boot 2/3 to version 2.7.18 and 3.2.1 respectively Tomcat 9.0.84 Vertx 4.5.1 Classgraph 4.8.165 Netty 4.1.104.Final slf4j API 2.0.9 log4j2 2.22.0 Kotlin 1.9.22 Artemis 2.31.2 Hazelcast 5.3.6 Guava 33.0.0-jre Version 3.0.6, 10/26/2023 Added Enhanced Benchmark tool to support \"Event over HTTP\" protocol to evaluate performance efficiency for commmunication between application containers using HTTP. Removed N/A Changed Updated open sources libraries Spring Boot 2/3 to version 2.7.17 and 3.1.5 respectively Kafka-client 3.6.0 Version 3.0.5, 10/21/2023 Added Support two executable JAR packaging system: 1. Maven Shade packager 2. Spring Boot packager Starting from version 3.0.5, we have replaced Spring Boot packager with Maven Shade. This avoids a classpath edge case for Spring Boot packager when running kafka-client under Java 11 or higher. Maven Shade also results in smaller executable JAR size. Removed N/A Changed Updated open sources libraries Spring-Boot 2.7.16 / 3.1.4 classgraph 4.8.163 snakeyaml 2.2 kotlin 1.9.10 vertx 4.4.6 guava 32.1.3-jre msgpack 0.9.6 slj4j 2.0.9 zookeeper 3.7.2 The \"/info/lib\" admin endpoint has been enhanced to list library dependencies for executable JAR generated by either Maven Shade or Spring Boot Packager. Improved ConfigReader to recognize both \".yml\" and \".yaml\" extensions and their uses are interchangeable. Version 3.0.4, 8/6/2023 Added N/A Removed N/A Changed Updated open sources libraries Spring-Boot 2.7.14 / 3.1.2 Kafka-client 3.5.1 classgraph 4.8.161 guava 32.1.2-jre msgpack 0.9.5 Version 3.0.3, 6/27/2023 Added File extension to MIME type mapping for static HTML file handling Removed N/A Changed Open sources library update - Kotlin version 1.9.0 Version 3.0.2, 6/9/2023 Added N/A Removed N/A Changed Consistent exception handling for Event API endpoint Open sources lib update - Vertx 4.4.4, Spring Boot 2.7.13, Spring Boot 3.1.1, classgraph 4.8.160, guava 32.0.1-jre Version 3.0.1, 6/5/2023 In this release, we have replace Google HTTP Client with vertx non-blocking WebClient. We also tested compatibility up to OpenJDK version 20 and maven 3.9.2. Added When \"x-raw-xml\" HTTP request header is set to \"true\", the AsyncHttpClient will skip the built-in XML serialization so that your application can retrieve the original XML text. Removed Retire Google HTTP client Changed Upgrade maven plugin versions. Version 3.0.0, 4/18/2023 This is a major release with some breaking changes. Please refer to Chapter-10 (Migration guide) for details. This version brings the best of preemptive and cooperating multitasking to Java (version 1.8 to 19) before Java 19 virtual thread feature becomes officially available. Added Function execution engine supporting kernel thread pool, Kotlin coroutine and suspend function \"Event over HTTP\" service for inter-container communication Support for Spring Boot version 3 and WebFlux Sample code for a pre-configured Spring Boot 3 application Removed Remove blocking APIs from platform-core Retire PM2 process manager sample script due to compatibility issue Changed Refactor \"async.http.request\" to use vertx web client for non-blocking operation Update log4j2 version 2.20.0 and slf4j version 2.0.7 in platform-core Update JBoss RestEasy JAX_RS to version 3.15.6.Final in rest-spring Update vertx to 4.4.2 Update Spring Boot parent pom to 2.7.12 and 3.1.0 for spring boot 2 and 3 respectively Remove com.fasterxml.classmate dependency from rest-spring Version 2.8.0, 3/20/2023 Added N/A Removed N/A Changed Improved load balancing in cloud-connector Filter URI to avoid XSS attack Upgrade to SnakeYaml 2.0 and patch Spring Boot 2.6.8 for compatibility with it Upgrade to Vertx 4.4.0, classgraph 4.8.157, tomcat 9.0.73 Version 2.7.1, 12/22/2022 Added standalone benchmark report app client and server benchmark apps add timeout tag to RPC events Removed N/A Changed Updated open sources dependencies Netty 4.1.86.Final Tomcat 9.0.69 Vertx 4.3.6 classgraph 4.8.152 google-http-client 1.42.3 Improved unit tests to use assertThrows to evaluate exception Enhanced AsyncHttpRequest serialization Version 2.7.0, 11/11/2022 In this version, REST automation code is moved to platform-core such that REST and Websocket service can share the same port. Added AsyncObjectStreamReader is added for non-blocking read operation from an object stream. Support of LocalDateTime in SimpleMapper Add \"removeElement\" method to MultiLevelMap Automatically convert a map to a PoJo when the sender does not specify class in event body Removed N/A Changed REST automation becomes part of platform-core and it can co-exist with Spring Web in the rest-spring module Enforce Spring Boot lifecycle management such that user apps will start after Spring Boot has loaded all components Update netty to version 4.1.84.Final Version 2.6.0, 10/13/2022 In this version, websocket notification example code has been removed from the REST automation system. If your application uses this feature, please recover the code from version 2.5.0 and refactor it as a separate library. Added N/A Removed Simplify REST automation system by removing websocket notification example in REST automation. Changed Replace Tomcat websocket server with Vertx non-blocking websocket server library Update netty to version 4.1.79.Final Update kafka client to version 2.8.2 Update snake yaml to version 1.33 Update gson to version 2.9.1 Version 2.5.0, 9/10/2022 Added New Preload annotation class to automate pre-registration of LambdaFunction. Removed Removed Spring framework and Tomcat dependencies from platform-core so that the core library can be applied to legacy J2EE application without library conflict. Changed Bugfix for proper housekeeping of future events. Make Gson and MsgPack handling of integer/long consistent Updated open sources libraries. Eclipse vertx-core version 4.3.4 MsgPack version 0.9.3 Google httpclient version 1.42.2 SnakeYaml version 1.31 Version 2.3.6, 6/21/2022 Added Support more than one event stream cluster. User application can share the same event stream cluster for pub/sub or connect to an alternative cluster for pub/sub use cases. Removed N/A Changed Cloud connector libraries update to Hazelcast 5.1.2 Version 2.3.5, 5/30/2022 Added Add tagging feature to handle language connector's routing and exception handling Removed Remove language pack's pub/sub broadcast feature Changed Update Spring Boot parent to version 2.6.8 to fetch Netty 4.1.77 and Spring Framework 5.3.20 Streamlined language connector transport protocol for compatibility with both Python and Node.js Version 2.3.4, 5/14/2022 Added N/A Removed Remove swagger-ui distribution from api-playground such that developer can clone the latest version Changed Update application.properties (from spring.resources.static-locations to spring.web.resources.static-locations) Update log4j, Tomcat and netty library version using Spring parent 2.6.6 Version 2.3.3, 3/30/2022 Added Enhanced AsyncRequest to handle non-blocking fork-n-join Removed N/A Changed Upgrade Spring Boot from 2.6.3 to 2.6.6 Version 2.3.2, 2/21/2022 Added Add support of queue API in native pub/sub module for improved ESB compatibility Removed N/A Changed N/A Version 2.3.1, 2/19/2022 Added N/A Removed N/A Changed Update Vertx to version 4.2.4 Update Tomcat to version 5.0.58 Use Tomcat websocket server for presence monitors Bugfix - Simple Scheduler's leader election searches peers correctly Version 2.3.0, 1/28/2022 Added N/A Removed N/A Changed Update copyright notice Update Vertx to version 4.2.3 Bugfix - RSA key generator supporting key length from 1024 to 4096 bits CryptoAPI - support different AES algorithms and custom IV Update Spring Boot to version 2.6.3 Version 2.2.3, 12/29/2021 Added Transaction journaling Add parameter distributed.trace.aggregation in application.properties such that trace aggregation may be disabled. Removed N/A Changed Update JBoss RestEasy library to 3.15.3.Final Improved po.search(route) to scan local and remote service registries. Added \"remoteOnly\" selection. Fix bug in releasing presence monitor topic for specific closed user group Update Apache log4j to version 2.17.1 Update Spring Boot parent to version 2.6.1 Update Netty to version 4.1.72.Final Update Vertx to version 4.2.2 Convenient class \"UserNotification\" for backend service to publish events to the UI when REST automation is deployed Version 2.2.2, 11/12/2021 Added User defined API authentication functions can be selected using custom HTTP request header \"Exception chaining\" feature in EventEnvelope New \"deferred.commit.log\" parameter for backward compatibility with older PowerMock in unit tests Removed N/A Changed Improved and streamlined SimpleXmlParser to handle arrays Bugfix for file upload in Service Gateway (REST automation library) Update Tomcat library from 9.0.50 to 9.0.54 Update Spring Boot library to 2.5.6 Update GSON library to 2.8.9 Version 2.2.1, 10/1/2021 Added Callback function can implement ServiceExceptionHandler to catch exception. It adds the onError() method. Removed N/A Changed Open sources library update - Vert.x 4.1.3, Netty 4.1.68-Final Version 2.1.1, 9/10/2021 Added User defined PoJo and Generics mapping Standardized serializers for default case, snake_case and camelCase Support of EventEnvelope as input parameter in TypedLambdaFunction so application function can inspect event's metadata Application can subscribe to life cycle events of other application instances Removed N/A Changed Replace Tomcat websocket server engine with Vertx in presence monitor for higher performance Bugfix for MsgPack transport of integer, long, BigInteger and BigDecimal Version 2.1.0, 7/25/2021 Added Multicast - application can define a multicast.yaml config to relay events to more than one target service. StreamFunction - function that allows the application to control back-pressure Removed \"object.streams.io\" route is removed from platform-core Changed Elastic Queue - Refactored using Oracle Berkeley DB Object stream I/O - simplified design using the new StreamFunction feature Open sources library update - Spring Boot 2.5.2, Tomcat 9.0.50, Vert.x 4.1.1, Netty 4.1.66-Final Version 2.0.0, 5/5/2021 Vert.x is introduced as the in-memory event bus Added ActiveMQ and Tibco connectors Admin endpoints to stop, suspend and resume an application instance Handle edge case to detect stalled application instances Add \"isStreamingPubSub\" method to the PubSub interface Removed Event Node event stream emulator has been retired. You may use standalone Kafka server as a replacement for development and testing in your laptop. Multi-tenancy namespace configuration has been retired. It is replaced by the \"closed user group\" feature. Changed Refactored Kafka and Hazelcast connectors to support virtual topics and closed user groups. Updated ConfigReader to be consistent with Spring value substitution logic for application properties Replace Akka actor system with Vert.x event bus Common code for various cloud connectors consolidated into cloud core libraries Version 1.13.0, 1/15/2021 Version 1.13.0 is the last version that uses Akka as the in-memory event system. Version 1.12.66, 1/15/2021 Added A simple websocket notification service is integrated into the REST automation system Seamless migration feature is added to the REST automation system Removed Legacy websocket notification example application Changed N/A Version 1.12.65, 12/9/2020 Added \"kafka.pubsub\" is added as a cloud service File download example in the lambda-example project \"trace.log.header\" added to application.properties - when tracing is enabled, this inserts the trace-ID of the transaction in the log context. For more details, please refer to the Developer Guide Add API to pub/sub engine to support creation of topic with partitions TypedLambdaFunction is added so that developer can predefine input and output classes in a service without casting Removed N/A Changed Decouple Kafka pub/sub from kafka connector so that native pub/sub can be used when application is running in standalone mode Rename \"relay\" to \"targetHost\" in AsyncHttpRequest data model Enhanced routing table distribution by sending a complete list of route tables, thus reducing network admin traffic. Version 1.12.64, 9/28/2020 Added If predictable topic is set, application instances will report their predictable topics as \"instance ID\" to the presence monitor. This improves visibility when a developer tests their application in \"hybrid\" mode. i.e. running the app locally and connect to the cloud remotely for event streams and cloud resources. Removed N/A Changed N/A Version 1.12.63, 8/27/2020 Added N/A Removed N/A Changed Improved Kafka producer and consumer pairing Version 1.12.62, 8/12/2020 Added New presence monitor's admin endpoint for the operator to force routing table synchronization (\"/api/ping/now\") Removed N/A Changed Improved routing table integrity check Version 1.12.61, 8/8/2020 Added Event stream systems like Kafka assume topic to be used long term. This version adds support to reuse the same topic when an application instance restarts. You can create a predictable topic using unique application name and instance ID. For example, with Kubernetes, you can use the POD name as the unique application instance topic. Removed N/A Changed N/A Version 1.12.56, 8/4/2020 Added Automate trace for fork-n-join use case Removed N/A Changed N/A Version 1.12.55, 7/19/2020 Added N/A Removed N/A Changed Improved distributed trace - set the \"from\" address in EventEnvelope automatically. Version 1.12.54, 7/10/2020 Added N/A Removed N/A Changed Application life-cycle management - User provided main application(s) will be started after Spring Boot declares web application ready. This ensures correct Spring autowiring or dependencies are available. Bugfix for locale - String.format(float) returns comma as decimal point that breaks number parser. Replace with BigDecimal decimal point scaling. Bugfix for Tomcat 9.0.35 - Change Async servlet default timeout from 30 seconds to -1 so the system can handle the whole life-cycle directly. Version 1.12.52, 6/11/2020 Added new \"search\" method in Post Office to return a list of application instances for a service simple \"cron\" job scheduler as an extension project add \"sequence\" to MainApplication annotation for orderly execution when more than one MainApplication is available support \"Optional\" object in EventEnvelope so a LambdaFunction can read and return Optional Removed N/A Changed The rest-spring library has been updated to support both JAR and WAR deployment All pom.xml files updated accordingly PersistentWsClient will back off for 10 seconds when disconnected by remote host Version 1.12.50, 5/20/2020 Added Payload segmentation For large payload in an event, the payload is automatically segmented into 64 KB segments. When there are more than one target application instances, the system ensures that the segments of the same event is delivered to exactly the same target. PersistentWsClient added - generalized persistent websocket client for Event Node, Kafka reporter and Hazelcast reporter. Removed N/A Changed Code cleaning to improve consistency Upgraded to hibernate-validator to v6.1.5.Final and Hazelcast version 4.0.1 REST automation is provided as a library and an application to handle different use cases Version 1.12.40, 5/4/2020 Added N/A Removed N/A Changed For security reason, upgrade log4j to version 2.13.2 Version 1.12.39, 5/3/2020 Added Use RestEasy JAX-RS library Removed For security reason, removed Jersey JAX-RS library Changed Updated RestLoader to initialize RestEasy servlet dispatcher Support nested arrays in MultiLevelMap Version 1.12.36, 4/16/2020 Added N/A Removed For simplicity, retire route-substitution admin endpoint. Route substitution uses a simple static table in route-substitution.yaml. Changed N/A Version 1.12.35, 4/12/2020 Added N/A Removed SimpleRBAC class is retired Changed Improved ConfigReader and AppConfigReader with automatic key-value normalization for YAML and JSON files Improved pub/sub module in kafka-connector Version 1.12.34, 3/28/2020 Added N/A Removed Retired proprietary config manager since we can use the \"BeforeApplication\" approach to load config from Kubernetes configMap or other systems of config record. Changed Added \"isZero\" method to the SimpleMapper class Convert BigDecimal to string without scientific notation (i.e. toPlainString instead of toString) Corresponding unit tests added to verify behavior Version 1.12.32, 3/14/2020 Added N/A Removed N/A Changed Kafka-connector will shutdown application instance when the EventProducer cannot send event to Kafka. This would allow the infrastructure to restart application instance automatically. Version 1.12.31, 2/26/2020 Added N/A Removed N/A Changed Kafka-connector now supports external service provider for Kafka properties and credentials. If your application implements a function with route name \"kafka.properties.provider\" before connecting to cloud, the kafka-connector will retrieve kafka credentials on demand. This addresses case when kafka credentials change after application start-up. Interceptors are designed to forward requests and thus they do not generate replies. However, if you implement a function as an EventInterceptor, your function can throw exception just like a regular function and the exception will be returned to the calling function. This makes it easier to write interceptors. Version 1.12.30, 2/6/2020 Added Expose \"async.http.request\" as a PUBLIC function (\"HttpClient as a service\") Removed N/A Changed Improved Hazelcast client connection stability Improved Kafka native pub/sub Version 1.12.29, 1/10/2020 Added Rest-automation will transport X-Trace-Id from/to Http request/response, therefore extending distributed trace across systems that support the X-Trace-Id HTTP header. Added endpoint and service to shutdown application instance. Removed N/A Changed Updated SimpleXmlParser with XML External Entity (XXE) injection prevention. Bug fix for hazelcast recovery logic - when a hazelcast node is down, the app instance will restart the hazelcast client and reset routing table correctly. HSTS header insertion is optional so that we can disable it to avoid duplicated header when API gateway is doing it. Version 1.12.26, 1/4/2020 Added Feature to disable PoJo deserialization so that caller can decide if the result set should be in PoJo or a Map. Removed N/A Changed Simplified key management for Event Node AsyncHttpRequest case insensitivity for headers, cookies, path parameters and session key-values Make built-in configuration management optional Version 1.12.19, 12/28/2019 Added Added HTTP relay feature in rest-automation project Removed N/A Changed Improved hazelcast retry and peer discovery logic Refactored rest-automation's service gateway module to use AsyncHttpRequest Info endpoint to show routing table of a peer Version 1.12.17, 12/16/2019 Added Simple configuration management is added to event-node, hazelcast-presence and kafka-presence monitors Added BeforeApplication annotation - this allows user application to execute some setup logic before the main application starts. e.g. modifying parameters in application.properties Added API playground as a convenient standalone application to render OpenAPI 2.0 and 3.0 yaml and json files Added argument parser in rest-automation helper app to use a static HTML folder in the local file system if arguments -html file_path is given when starting the JAR file. Removed N/A Changed Kafka publisher timeout value changed from 10 to 20 seconds Log a warning when Kafka takes more than 5 seconds to send an event Version 1.12.14, 11/20/2019 Added getRoute() method is added to PostOffice to facilitate RBAC The route name of the current service is added to an outgoing event when the \"from\" field is not present Simple RBAC using YAML configuration instead of code Removed N/A Changed Updated Spring Boot to v2.2.1 Version 1.12.12, 10/26/2019 Added Multi-tenancy support for event streams (Hazelcast and Kafka). This allows the use of a single event stream cluster for multiple non-prod environments. For production, it must use a separate event stream cluster for security reason. Removed N/A Changed logging framework changed from logback to log4j2 (version 2.12.1) Use JSR-356 websocket annotated ClientEndpoint Improved websocket reconnection logic Version 1.12.9, 9/14/2019 Added Distributed tracing implemented in platform-core and rest-automation Improved HTTP header transformation for rest-automation Removed N/A Changed language pack API key obtained from environment variable Version 1.12.8, 8/15/2019 Added N/A Removed rest-core subproject has been merged with rest-spring Changed N/A Version 1.12.7, 7/15/2019 Added Periodic routing table integrity check (15 minutes) Set kafka read pointer to the beginning for new application instances except presence monitor REST automation helper application in the \"extensions\" project Support service discovery of multiple routes in the updated PostOffice's exists() method logback to set log level based on environment variable LOG_LEVEL (default is INFO) Removed N/A Changed Minor refactoring of kafka-connector and hazelcast-connector to ensure that they can coexist if you want to include both of these dependencies in your project. This is for convenience of dev and testing. In production, please select only one cloud connector library to reduce memory footprint. Version 1.12.4, 6/24/2019 Added Add inactivity expiry timer to ObjectStreamIO so that house-keeper can clean up resources that are idle Removed N/A Changed Disable HTML encape sequence for GSON serializer Bug fix for GSON serialization optimization Bug fix for Object Stream housekeeper By default, GSON serializer converts all numbers to double, resulting in unwanted decimal point for integer and long. To handle custom map serialization for correct representation of numbers, an unintended side effect was introduced in earlier releases. List of inner PoJo would be incorrectly serialized as map, resulting in casting exception. This release resolves this issue. Version 1.12.1, 6/10/2019 Added Store-n-forward pub/sub API will be automatically enabled if the underlying cloud connector supports it. e.g. kafka ObjectStreamIO, a convenient wrapper class, to provide event stream I/O API. Object stream feature is now a standard feature instead of optional. Deferred delivery added to language connector. Removed N/A Changed N/A Version 1.11.40, 5/25/2019 Added Route substitution for simple versioning use case Add \"Strict Transport Security\" header if HTTPS (https://tools.ietf.org/html/rfc6797) Event stream connector for Kafka Distributed housekeeper feature for Hazelcast connector Removed System log service Changed Refactoring of Hazelcast event stream connector library to sync up with the new Kafka connector. Version 1.11.39, 4/30/2019 Added Language-support service application for Python, Node.js and Go, etc. Python language pack project is available at https://github.com/Accenture/mercury-python Removed N/A Changed replace Jackson serialization engine with Gson ( platform-core project) replace Apache HttpClient with Google Http Client ( rest-spring ) remove Jackson dependencies from Spring Boot ( rest-spring ) interceptor improvement Version 1.11.33, 3/25/2019 Added N/A Removed N/A Changed Move safe.data.models validation rules from EventEnvelope to SimpleMapper Apache fluent HTTP client downgraded to version 4.5.6 because the pom file in 4.5.7 is invalid Version 1.11.30, 3/7/2019 Added Added retry logic in persistent queue when OS cannot update local file metadata in real-time for Windows based machine. Removed N/A Changed pom.xml changes - update with latest 3rd party open sources dependencies. Version 1.11.29, 1/25/2019 Added platform-core Support for long running functions so that any long queries will not block the rest of the system. \"safe.data.models\" is available as an option in the application.properties. This is an additional security measure to protect against Jackson deserialization vulnerability. See example below: # # additional security to protect against model injection # comma separated list of model packages that are considered safe to be used for object deserialization # #safe.data.models=com.accenture.models rest-spring \"/env\" endpoint is added. See sample application.properties below: # # environment and system properties to be exposed to the \"/env\" admin endpoint # show.env.variables=USER, TEST show.application.properties=server.port, cloud.connector Removed N/A Changed platform-core Use Java Future and an elastic cached thread pool for executing user functions. Fixed N/A Version 1.11.28, 12/20/2018 Added Hazelcast support is added. This includes two projects (hazelcast-connector and hazelcast-presence). Hazelcast-connector is a cloud connector library. Hazelcast-presence is the \"Presence Monitor\" for monitoring the presence status of each application instance. Removed platform-core The \"fixed resource manager\" feature is removed because the same outcome can be achieved at the application level. e.g. The application can broadcast requests to multiple application instances with the same route name and use a callback function to receive response asynchronously. The services can provide resource metrics so that the caller can decide which is the most available instance to contact. For simplicity, resources management is better left to the cloud platform or the application itself. Changed N/A Fixed N/A","title":"Release notes"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#release-notes","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Note : Some version numbers may be skipped to align feature set with the Node.js version.","title":"Release notes"},{"location":"CHANGELOG/#version-4323-1062025","text":"","title":"Version 4.3.23, 10/6/2025"},{"location":"CHANGELOG/#added","text":"State machine monitor feature in EventScriptMock class Updated MultiLevelMap to support a new \"getElements\" method to retrieve elements using wildcard index \"*\".","title":"Added"},{"location":"CHANGELOG/#removed","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed","text":"OSS update: spring boot version 3.5.6 vertx version 5.0.4 netty version 4.2.6.Final","title":"Changed"},{"location":"CHANGELOG/#version-4322-9282025","text":"","title":"Version 4.3.22, 9/28/2025"},{"location":"CHANGELOG/#added_1","text":"Add envInstances parameter in no.op and resilience.handler to allow developer to override the default maximum worker instance count.","title":"Added"},{"location":"CHANGELOG/#removed_1","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_1","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4321-8302025","text":"","title":"Version 4.3.21, 8/30/2025"},{"location":"CHANGELOG/#added_2","text":"Enable connection timeout in AsyncHttpClient with a new parameter in application.properties to support fail-fast when making HTTP requests. http.client.connection.timeout (default value of 5000, unit in milliseconds)","title":"Added"},{"location":"CHANGELOG/#removed_2","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_2","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4320-8282025","text":"","title":"Version 4.3.20, 8/28/2025"},{"location":"CHANGELOG/#added_3","text":"error.task in the error namespace to map to the original task that throws exception @retry keyword in the first next task list in an exception handler tells the system to automatically resolve the original task.","title":"Added"},{"location":"CHANGELOG/#removed_3","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_3","text":"Chapter 4 of the Developer Guide updated for the new feature for error handling","title":"Changed"},{"location":"CHANGELOG/#version-4319-8272025","text":"","title":"Version 4.3.19, 8/27/2025"},{"location":"CHANGELOG/#added_4","text":"Performance metrics for each task in the end-of-flow report","title":"Added"},{"location":"CHANGELOG/#removed_4","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_4","text":"move \"NoOp\" composable function from event-script-engine to platform-core adjust concurrency for no op, actuator service, event over http, resilience handler and simple exception handler virtual thread optimization runs at application start-up phase","title":"Changed"},{"location":"CHANGELOG/#version-4318-8262025","text":"","title":"Version 4.3.18, 8/26/2025"},{"location":"CHANGELOG/#added_5","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_5","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_5","text":"Streamlined subflow routing in TaskExecutor of the event-script-engine. Changed subflow RPC call to asynchronous callback for performance optimization.","title":"Changed"},{"location":"CHANGELOG/#version-4317-8222025","text":"","title":"Version 4.3.17, 8/22/2025"},{"location":"CHANGELOG/#added_6","text":"Validation logic to filter out CR/LF for headers, cookies and session info when creating an AsyncHttpRequest from a map. For example, when using the \"AsyncHttpClient by configuration\" method, the AsyncHttpRequest is created by a map of key-values. The additional validation prevents creating headers and cookies with CR/LF accidentially.","title":"Added"},{"location":"CHANGELOG/#removed_6","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_6","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4316-8212025","text":"","title":"Version 4.3.16, 8/21/2025"},{"location":"CHANGELOG/#added_7","text":"log4j2.xml in standalone-kafka-server to reduce logging noise","title":"Added"},{"location":"CHANGELOG/#removed_7","text":"Update standalone-kafka-server's pom.xml for unused dependencies: apache commons-beanutils apache commons-digester apache commons-logging","title":"Removed"},{"location":"CHANGELOG/#changed_7","text":"Update OSS versions: Spring Boot 3.5.5 Kafka client and server 4.0.0 Gson 2.13.1 junit-bom 5.13.4","title":"Changed"},{"location":"CHANGELOG/#version-4315-8202025","text":"","title":"Version 4.3.15, 8/20/2025"},{"location":"CHANGELOG/#added_8","text":"CR/LF are filtered as a space in the EventEnvelope's setHeader and EventEmitter's asEnvelope methods Avoid \"path traversal\" attack by rejecting relative parent file path in configuration","title":"Added"},{"location":"CHANGELOG/#removed_8","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_8","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4314-8192025","text":"","title":"Version 4.3.14, 8/19/2025"},{"location":"CHANGELOG/#added_9","text":"To reduce ambiguity in event script configuration, created an alias for model.parent. namespace as model.root. The alias is implemented using a memory reference for lowest memory and processing overheads.","title":"Added"},{"location":"CHANGELOG/#removed_9","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_9","text":"CompileFlows and TaskExecutor classes are updated to support the model.root. namespace alias.","title":"Changed"},{"location":"CHANGELOG/#version-4313-8162025","text":"","title":"Version 4.3.13, 8/16/2025"},{"location":"CHANGELOG/#added_10","text":"Reactor HttpClient Support the use of different upload tags for multipart file upload \"content-length\" HTTP header integrity protection logic added","title":"Added"},{"location":"CHANGELOG/#removed_10","text":"Vertx HttpClient OutputStreamQueue class","title":"Removed"},{"location":"CHANGELOG/#changed_10","text":"Update AsyncHttpClient to change from Vertx to Reactor HttpClient Change the concurrency of AsyncHttpClient and TemporaryIndex to 500, matching the underlying HTTP client's connection pool Upgrade reactor-bom to version 2024.0.9 that fetches reactor-netty-core version 1.2.9 Upgrade netty to version 4.2.4.Final","title":"Changed"},{"location":"CHANGELOG/#version-4312-8112025","text":"","title":"Version 4.3.12, 8/11/2025"},{"location":"CHANGELOG/#added_11","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_11","text":"Content-length is not required to be set when uploading multiple files using multipart/form-data protocol.","title":"Removed"},{"location":"CHANGELOG/#changed_11","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4311-8112025","text":"","title":"Version 4.3.11, 8/11/2025"},{"location":"CHANGELOG/#added_12","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_12","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_12","text":"Bug fix: Environment variable resolution in the \"load\" method of the ConfigReader class. Updated model variable string substitution to pass through non-model text.","title":"Changed"},{"location":"CHANGELOG/#version-4310-872025","text":"","title":"Version 4.3.10, 8/7/2025"},{"location":"CHANGELOG/#added_13","text":"Support runtime string substitution using a model variable","title":"Added"},{"location":"CHANGELOG/#removed_13","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_13","text":"ConfigReader refactored to reduce code complexity to 15 or less as per SonarQube's requirement","title":"Changed"},{"location":"CHANGELOG/#version-439-862025","text":"","title":"Version 4.3.9, 8/6/2025"},{"location":"CHANGELOG/#added_14","text":"Support multipart upload for one or more files in both AsyncHttpClient and reactive HTTP server.","title":"Added"},{"location":"CHANGELOG/#removed_14","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_14","text":"OSS updates: Spring Boot parent version 3.5.4 Classgraph version 4.8.181 Vertx version 5.0.2","title":"Changed"},{"location":"CHANGELOG/#version-438-7292025","text":"","title":"Version 4.3.8, 7/29/2025"},{"location":"CHANGELOG/#added_15","text":"Static methods in EventEnvelope and PostOffice to instantiate new objects option to render input file content as \"json\"","title":"Added"},{"location":"CHANGELOG/#removed_15","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_15","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-437-7262025","text":"","title":"Version 4.3.7, 7/26/2025"},{"location":"CHANGELOG/#added_16","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_16","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_16","text":"Refactor event-script-engine to comply with SonarQube's code complexity requirement of 15 or less Rename DistributedTrace class to \"Telemetry\" for compatibility with node.js version","title":"Changed"},{"location":"CHANGELOG/#version-436-7232025","text":"","title":"Version 4.3.6, 7/23/2025"},{"location":"CHANGELOG/#added_17","text":"new \"eRequest\" APIs for EventEmitter and PostOffice","title":"Added"},{"location":"CHANGELOG/#removed_17","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_17","text":"restore original return type of java.util.concurrent.Future for the \"request\" APIs that in turns call the new \"eRequest\" APIs Update netty to version 4.2.3.Final","title":"Changed"},{"location":"CHANGELOG/#version-435-7222025","text":"","title":"Version 4.3.5, 7/22/2025"},{"location":"CHANGELOG/#added_18","text":"Support 2nd Kafka server in the kafka-standalone subproject.","title":"Added"},{"location":"CHANGELOG/#removed_18","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_18","text":"Refactor platform-core to comply with SonarQube's code complexity requirement of 15 or less Update WorkerHandler to catch NoClassDefFoundError, AssertionError and Exception PostOffice's request API provides CompletableFuture instead of Future to support both reactive and sequential coding style MsgPack updated to version 0.9.10","title":"Changed"},{"location":"CHANGELOG/#version-434-732025","text":"","title":"Version 4.3.4, 7/3/2025"},{"location":"CHANGELOG/#added_19","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_19","text":"Message queuing mechanism in JSON and COMPACT loggers","title":"Removed"},{"location":"CHANGELOG/#changed_19","text":"Updated OSS dependency - Kafka-client version 3.9.1","title":"Changed"},{"location":"CHANGELOG/#version-433-6262025","text":"","title":"Version 4.3.3, 6/26/2025"},{"location":"CHANGELOG/#added_20","text":"Warm up logic for Java 21 virtual thread system","title":"Added"},{"location":"CHANGELOG/#removed_20","text":"The setBodyWithDefaultSerialization() method in EventEnvelope is retired","title":"Removed"},{"location":"CHANGELOG/#changed_20","text":"Improve CompileFlow error message. The new error message will tell where the error comes from. Open source dependency update below. Spring Boot parent version 3.5.3, Classgraph version 4.8.180, Netty version 4.2.2.Final","title":"Changed"},{"location":"CHANGELOG/#version-432-6232025","text":"","title":"Version 4.3.2, 6/23/2025"},{"location":"CHANGELOG/#added_21","text":"Support file \"append\" mode in output data mapping Dynamic fork-n-join feature for parallel processing of a list of elements by multiple instances of the same task","title":"Added"},{"location":"CHANGELOG/#removed_21","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_21","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-431-6122025","text":"","title":"Version 4.3.1, 6/12/2025"},{"location":"CHANGELOG/#added_22","text":"Add kotlin example","title":"Added"},{"location":"CHANGELOG/#removed_22","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_22","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-430-6102025","text":"In this version, we have retired the support of Kotlin suspend function. Therefore, the project is 100% pure Java. In addition to dropping Kotlin dependency, it also streamlines exception handling by replacing IOException with IllegalArgumentException. Your applications may need minor refactoring of try-catch that uses the ConfigReader, PostOffice and Platform APIs. In most cases, you can just remove the try-catch block. In some cases, you can change the IOException to IllegalArgumentException in the try-catch block for best compatibility with your original code. Composable applications using Event Script should not be affected because composable functions are self-contained and independent. They usually do not invoke low level PostOffice and Platform APIs.","title":"Version 4.3.0, 6/10/2025"},{"location":"CHANGELOG/#added_23","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_23","text":"TypedKotlinFunction and kotlin dependency Serial ID for AppException","title":"Removed"},{"location":"CHANGELOG/#changed_23","text":"Update AppException to extend RuntimeException Replace IOException with IllegalArgumentException for ConfigReader, PostOffice and Platform APIs. The above changes remove the requirement to do explicit try-catch in normal use cases, thus simplifying application code. It is a best practice to let run-time exception throw through the chain.","title":"Changed"},{"location":"CHANGELOG/#version-4246-622025","text":"","title":"Version 4.2.46, 6/2/2025"},{"location":"CHANGELOG/#added_24","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_24","text":"AcknowledgeResult is not required in Vertx 5.0","title":"Removed"},{"location":"CHANGELOG/#changed_24","text":"Improved negate type mapping logic Updated OutputStreamQueue to use new API for Vertx WriteStream Refactor HttpsTest to use Vertx 5.0 web client API OSS version updates as follows Spring Boot 3.5.0 Vertx 5.0.0 netty 4.2.1.Final Kotlin 2.1.21 reactor-bom 2024.0.6 junit-bom 5.13.0","title":"Changed"},{"location":"CHANGELOG/#version-4245-5312025","text":"","title":"Version 4.2.45, 5/31/2025"},{"location":"CHANGELOG/#added_25","text":"\"Empty array index\" syntax in data mapping to append an element to an array in a dataset Allow text constant in data mapping to contain any characters including the mapping signature \"->\"","title":"Added"},{"location":"CHANGELOG/#removed_25","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_25","text":"Bugfix for certain edge cases in detecting a non-exist key in a MultiLevelMap","title":"Changed"},{"location":"CHANGELOG/#version-4244-5282025","text":"","title":"Version 4.2.44, 5/28/2025"},{"location":"CHANGELOG/#added_26","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_26","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_26","text":"Add unit test for https server","title":"Changed"},{"location":"CHANGELOG/#version-4243-5282025","text":"","title":"Version 4.2.43, 5/28/2025"},{"location":"CHANGELOG/#added_27","text":"Introduce \"datatype\" in output data mapping of event script Snake case and camel case selection in PreLoad annotation to override default serialization case strategy","title":"Added"},{"location":"CHANGELOG/#removed_27","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_27","text":"Event Script improvement - avoid endless loop when top-level exception handler throws exception itself","title":"Changed"},{"location":"CHANGELOG/#version-4242-5242025","text":"","title":"Version 4.2.42, 5/24/2025"},{"location":"CHANGELOG/#added_28","text":"SSL http server options","title":"Added"},{"location":"CHANGELOG/#removed_28","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_28","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4242-5172025","text":"","title":"Version 4.2.42, 5/17/2025"},{"location":"CHANGELOG/#added_29","text":"dynamic model variable as index to address an array element (support LHS and RHS mapping) updated documentation of configuration management in Appendix-I of developer guide","title":"Added"},{"location":"CHANGELOG/#removed_29","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_29","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4240-592025","text":"","title":"Version 4.2.40, 5/9/2025"},{"location":"CHANGELOG/#added_30","text":"support of \"flows\" in the \"modules.autostart\" feature \"length\" type matching feature \"dynamic model variables in for-loop\" - model variable in comparator's left and/or right hand sides","title":"Added"},{"location":"CHANGELOG/#removed_30","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_30","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4239-542025","text":"","title":"Version 4.2.39, 5/4/2025"},{"location":"CHANGELOG/#added_31","text":"Code example to illustrate how to write your own Flow Adapters \"modules.autostart\" feature to invoke composable functions and libaries at start up.","title":"Added"},{"location":"CHANGELOG/#removed_31","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_31","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4238-4302025","text":"","title":"Version 4.2.38, 4/30/2025"},{"location":"CHANGELOG/#added_32","text":"Spring auto-wiring of composable functions when leveraging the rest-spring-3 module","title":"Added"},{"location":"CHANGELOG/#removed_32","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_32","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4237-4292025","text":"","title":"Version 4.2.37, 4/29/2025"},{"location":"CHANGELOG/#added_33","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_33","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_33","text":"Windows compatibility in static file handling - convert Windows file path to Unix path","title":"Changed"},{"location":"CHANGELOG/#version-4235-4242025","text":"","title":"Version 4.2.35, 4/24/2025"},{"location":"CHANGELOG/#added_34","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_34","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_34","text":"Bugfix for pipeline that contains only one task","title":"Changed"},{"location":"CHANGELOG/#version-4233-4242025","text":"","title":"Version 4.2.33, 4/24/2025"},{"location":"CHANGELOG/#added_35","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_35","text":"Removed SimpleScheduler from the build script at root because SimpleScheduler has not reached production quality","title":"Removed"},{"location":"CHANGELOG/#changed_35","text":"Rename variable as per SonarCube quality gate OSS version update - revert Kafka client to version 3.9.0 for compatibility with Confluent Kafka","title":"Changed"},{"location":"CHANGELOG/#version-4232-4242025","text":"","title":"Version 4.2.32, 4/24/2025"},{"location":"CHANGELOG/#added_36","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_36","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_36","text":"Improvement in custom content-type resolver OSS version update vertx 4.5.14 guava 33.4.8-jre junit5-bom 5.12.2 kotlin 2.1.20 Kafka client 4.0.0 gson 2.13.0","title":"Changed"},{"location":"CHANGELOG/#version-4231-4232025","text":"","title":"Version 4.2.31, 4/23/2025"},{"location":"CHANGELOG/#added_37","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_37","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_37","text":"Minor update to address code smells reported by SonarCube analyzer","title":"Changed"},{"location":"CHANGELOG/#version-4230-4222025","text":"","title":"Version 4.2.30, 4/22/2025"},{"location":"CHANGELOG/#added_38","text":"Support \"matrix parameters\" and \"hash parameters\" in HTTP request URI in platform-core \"classpath\" in LHS of output data mapping for event script","title":"Added"},{"location":"CHANGELOG/#removed_38","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_38","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4229-4202025","text":"","title":"Version 4.2.29, 4/20/2025"},{"location":"CHANGELOG/#added_39","text":"Perform \"path traversal\" avoidance when decoding incoming HTTP requests","title":"Added"},{"location":"CHANGELOG/#removed_39","text":"Remove \"public\" qualifier from unit tests since JUnit version 5 does not need it","title":"Removed"},{"location":"CHANGELOG/#changed_39","text":"Minor refactoring to remove majority of code smells as per SonarQube static code analysis Support custom error message in EventEnvelope","title":"Changed"},{"location":"CHANGELOG/#version-4228-4172025","text":"","title":"Version 4.2.28, 4/17/2025"},{"location":"CHANGELOG/#added_40","text":"Unit test and updated developer guide to illustate use of AsyncHttpClient in Event Script.","title":"Added"},{"location":"CHANGELOG/#removed_40","text":"default application.properties files in platform-core, rest-spring-3 and event-script-engine HTML escape characters in URI path handling in AsyncHttpClient","title":"Removed"},{"location":"CHANGELOG/#changed_40","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4227-3312025","text":"","title":"Version 4.2.27, 3/31/2025"},{"location":"CHANGELOG/#added_41","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_41","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_41","text":"Streamline error handling in TaskExecutor to sync up with Node.js version Update developer guide's chapter-4 for the output data mapping paragraph about file","title":"Changed"},{"location":"CHANGELOG/#version-4226-3252025","text":"","title":"Version 4.2.26, 3/25/2025"},{"location":"CHANGELOG/#added_42","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_42","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_42","text":"Use EventInterceptor pattern for the resilience handler for non-blocking deferred response","title":"Changed"},{"location":"CHANGELOG/#version-4225-3242025","text":"","title":"Version 4.2.25, 3/24/2025"},{"location":"CHANGELOG/#added_43","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_43","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_43","text":"Rename \"alternate\" parameter in resilience handler to \"alternative\"","title":"Changed"},{"location":"CHANGELOG/#version-4224-3242025","text":"","title":"Version 4.2.24, 3/24/2025"},{"location":"CHANGELOG/#added_44","text":"Generic resilience handler with alternative path and backoff features","title":"Added"},{"location":"CHANGELOG/#removed_44","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_44","text":"The getError() method in EventEnvelope is updated to return encoded error message. This is required for distributed trace processing and proper error handling of subflows. Delete file when mapping a null value from the LHS to the RHS that is defined as a file, thus allowing clearing of temporary data files in a flow. OSS update - spring boot parent version 3.4.4 and io.project.reactor bom version 2024.0.4","title":"Changed"},{"location":"CHANGELOG/#version-4223-3122025","text":"","title":"Version 4.2.23, 3/12/2025"},{"location":"CHANGELOG/#added_45","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_45","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_45","text":"For security, the parent state machine (namespace \"model.parent\") is a protected resource. It can only be shared by the primary flow and all sub-flow instances that are instantiated from it.","title":"Changed"},{"location":"CHANGELOG/#version-4222-3112025","text":"","title":"Version 4.2.22, 3/11/2025"},{"location":"CHANGELOG/#added_46","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_46","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_46","text":"All sub-flows instantiated from a primary flow can access the same parent state machine using the \"model.parent\" namespace.","title":"Changed"},{"location":"CHANGELOG/#version-4221-382025","text":"","title":"Version 4.2.21, 3/8/2025"},{"location":"CHANGELOG/#added_47","text":"Support flow and function for external state machine Parent state machine for sub-flow Validation rules to reject access to the whole model or parent namespace","title":"Added"},{"location":"CHANGELOG/#removed_47","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_47","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4220-2282025","text":"","title":"Version 4.2.20, 2/28/2025"},{"location":"CHANGELOG/#added_48","text":"\"spring.boot.main=org.platformlambda.rest.RestServer\" added to application.properties so that developer may override it with their own Spring Boot initializer.","title":"Added"},{"location":"CHANGELOG/#removed_48","text":"property setting for Netty version 4.1.118.Final is no longer required in pom.xml because the updated spring boot parent version 3.4.3 will fetch 4.1.118 correctly.","title":"Removed"},{"location":"CHANGELOG/#changed_48","text":"upgrade spring boot version 3.4.3","title":"Changed"},{"location":"CHANGELOG/#version-4219-2262025","text":"","title":"Version 4.2.19, 2/26/2025"},{"location":"CHANGELOG/#added_49","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_49","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_49","text":"Allow developer to load base configuration files from the classpath or from the local file system.","title":"Changed"},{"location":"CHANGELOG/#version-4218-2212025","text":"","title":"Version 4.2.18, 2/21/2025"},{"location":"CHANGELOG/#added_50","text":"java.sql.Timestamp data type added to SimpleMapper simple type matching feature is extended with a new string 'concat' method default REST endpoints for /api/event and actuator services","title":"Added"},{"location":"CHANGELOG/#removed_50","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_50","text":"Sort REST endpoints for orderly loading Drop \"async.http.request\" RPC traces to reduce observability noise","title":"Changed"},{"location":"CHANGELOG/#version-4217-2202025","text":"","title":"Version 4.2.17, 2/20/2025"},{"location":"CHANGELOG/#added_51","text":"LocalDate and LocalTime data type added to SimpleMapper","title":"Added"},{"location":"CHANGELOG/#removed_51","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_51","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4215-2152025","text":"","title":"Version 4.2.15, 2/15/2025"},{"location":"CHANGELOG/#added_52","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_52","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_52","text":"Update actuator output data structures to be consistent with Composable Node.js implementation","title":"Changed"},{"location":"CHANGELOG/#version-4214-2142025","text":"","title":"Version 4.2.14, 2/14/2025"},{"location":"CHANGELOG/#added_53","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_53","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_53","text":"Use different route names for various actuator services to avoid hardcode of URLs","title":"Changed"},{"location":"CHANGELOG/#version-4213-2132025","text":"","title":"Version 4.2.13, 2/13/2025"},{"location":"CHANGELOG/#added_54","text":"Actuator REST endpoints are now configurable in rest.yaml","title":"Added"},{"location":"CHANGELOG/#removed_54","text":"The feature to shutdown, suspend and resume of an application instance is retired","title":"Removed"},{"location":"CHANGELOG/#changed_54","text":"Update actuator services to serve REST requests directly","title":"Changed"},{"location":"CHANGELOG/#version-4212-2122025","text":"","title":"Version 4.2.12, 2/12/2025"},{"location":"CHANGELOG/#added_55","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_55","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_55","text":"Use ServerCookieEncoder.STRICT.encode() method to detect invalid cookie value Update vertx to 4.5.13 and Netty to 4.1.118.Final to address security vulnerabilities","title":"Changed"},{"location":"CHANGELOG/#version-4211-2112025","text":"","title":"Version 4.2.11, 2/11/2025"},{"location":"CHANGELOG/#added_56","text":"Support of Spring active profiles using JVM parameter \"-Dspring.profiles.active\" or environment variable \"SPRING_PROFILES_ACTIVE\"","title":"Added"},{"location":"CHANGELOG/#removed_56","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_56","text":"Developer Guide's Appendix-I updated for the Spring active profile feature","title":"Changed"},{"location":"CHANGELOG/#version-4210-2102025","text":"","title":"Version 4.2.10, 2/10/2025"},{"location":"CHANGELOG/#added_57","text":"PoJo class hint and custom serializer in FluxConsumer Optional custom serializer in FluxPublisher","title":"Added"},{"location":"CHANGELOG/#removed_57","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_57","text":"WorkerHandler and WorkerQueue classes for FluxConsumer Developer Guide's Chapter 2 Flow diagrams for sample app in Developer Guide","title":"Changed"},{"location":"CHANGELOG/#version-429-282025","text":"","title":"Version 4.2.9, 2/8/2025"},{"location":"CHANGELOG/#added_58","text":"uuid generation feature in Event Script's simple type matching inputPoJoClass parameter in PreLoad annotation","title":"Added"},{"location":"CHANGELOG/#removed_58","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_58","text":"For feature completeness, the system now supports list of pojo as one type of event input for sending event programmatically. However, Event Script's input data mapping is configuration driven and thus list of pojo is not permitted.","title":"Changed"},{"location":"CHANGELOG/#version-428-262025","text":"In this release, we have improved EventEnvelope and Trace Annotation features and tested interoperability with applications written in version 2 and 3.","title":"Version 4.2.8, 2/6/2025"},{"location":"CHANGELOG/#added_59","text":"Support text, map and list in trace annotation","title":"Added"},{"location":"CHANGELOG/#removed_59","text":"The \"extra\" and \"end-of-route\" fields in EventEnvelope are retired","title":"Removed"},{"location":"CHANGELOG/#changed_59","text":"Updated EventEnvelope data structure for direct support of tags and annotations","title":"Changed"},{"location":"CHANGELOG/#version-427-242025","text":"","title":"Version 4.2.7, 2/4/2025"},{"location":"CHANGELOG/#added_60","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_60","text":"redundant and inconsistent log4j2.xml config files from subprojects","title":"Removed"},{"location":"CHANGELOG/#changed_60","text":"getBodyAsListOfPoJo method in EventEnvelope updated endFlow method of TaskExecutor sends event to distributed trace instead of logging","title":"Changed"},{"location":"CHANGELOG/#version-426-232025","text":"","title":"Version 4.2.6, 2/3/2025"},{"location":"CHANGELOG/#added_61","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_61","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_61","text":"Observability bugfix to set execution time. In earlier iteration, input events to the PostOffice are set to be immutable. However, the execution time in a reply event was missing in the cloned event.","title":"Changed"},{"location":"CHANGELOG/#version-425-222025","text":"","title":"Version 4.2.5, 2/2/2025"},{"location":"CHANGELOG/#added_62","text":"Add 3-part syntax for Event Script's data mapping processing. Supports the following data mapping syntax: LHS -> RHS LHS -> model.variable -> RHS","title":"Added"},{"location":"CHANGELOG/#removed_62","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_62","text":"Make input event immutable to PostOffice's send and request API Consistent temporary stream folder name for Java and Node.js under /tmp/composable","title":"Changed"},{"location":"CHANGELOG/#version-424-1302025","text":"","title":"Version 4.2.4, 1/30/2025"},{"location":"CHANGELOG/#added_63","text":"Temporary Inbox handler for optimization of request-response event processing","title":"Added"},{"location":"CHANGELOG/#removed_63","text":"FastRPC API removed from the Kotlin subsystem since Kotlin can use Mono/Flux for reactive programming","title":"Removed"},{"location":"CHANGELOG/#changed_63","text":"Developer guide updated accordingly Essential services are started as the first \"BeforeApplication\" using sequence 0 Event Script runs as the second \"BeforeApplication\" using sequence 2 BeforeApplication sequence 1 is reserved to handle rare use case that a module must run before event script","title":"Changed"},{"location":"CHANGELOG/#version-423-1282025","text":"","title":"Version 4.2.3, 1/28/2025"},{"location":"CHANGELOG/#added_64","text":"Support of negate operator of a model value in event script added to the \"simple type matching\" feature","title":"Added"},{"location":"CHANGELOG/#removed_64","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_64","text":"Use virtual thread to decouple JSON logging to optimize application performance. Updated the following OSS versions Spring Boot version 3.4.2 Reactor-bom 2024.0.2 Vertx-core 4.5.12 MsgPack 0.9.9","title":"Changed"},{"location":"CHANGELOG/#version-422-1222025","text":"","title":"Version 4.2.2, 1/22/2025"},{"location":"CHANGELOG/#added_65","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_65","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_65","text":"Simplify JSON logging using the parameter formatter {} with a map parameter. The map will be printed as JSON when log.format=json is set in application.properties. This also slightly improves logging performance.","title":"Changed"},{"location":"CHANGELOG/#version-421-1212025","text":"","title":"Version 4.2.1, 1/21/2025"},{"location":"CHANGELOG/#added_66","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_66","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_66","text":"reconfigure logger to json or compact format early when app starts","title":"Changed"},{"location":"CHANGELOG/#version-420-1202025","text":"This is a milestone release for consistent features and behaviors between Java and Node.js versions","title":"Version 4.2.0, 1/20/2025"},{"location":"CHANGELOG/#added_67","text":"Composable methodology in developer guide","title":"Added"},{"location":"CHANGELOG/#removed_67","text":"URI path from errorPage.html","title":"Removed"},{"location":"CHANGELOG/#changed_67","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-418-1172025","text":"","title":"Version 4.1.8, 1/17/2025"},{"location":"CHANGELOG/#added_68","text":"Notice in Javadoc to identify system reserved classes","title":"Added"},{"location":"CHANGELOG/#removed_68","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_68","text":"log.format parameter changed to be case insensitive","title":"Changed"},{"location":"CHANGELOG/#version-417-1162025","text":"","title":"Version 4.1.7, 1/16/2025"},{"location":"CHANGELOG/#added_69","text":"Limit stack trace transport to a max of 10 lines for transport efficiency because the original stack trace can be retrieved with the getException() method Flow tests for composable-example application","title":"Added"},{"location":"CHANGELOG/#removed_69","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_69","text":"Update composable-example to be consistent with Node.js version Change distributed trace to log as pretty print JSON Clean up log4j XML configuration templates in platform-core to be used as examples","title":"Changed"},{"location":"CHANGELOG/#version-416-1152025","text":"","title":"Version 4.1.6, 1/15/2025"},{"location":"CHANGELOG/#added_70","text":"Support 3 logging formats using the log.format parameter in application.properties text - default to text string output compact - json output without linefeed for redirection to log analytics system json - pretty print for human readers","title":"Added"},{"location":"CHANGELOG/#removed_70","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_70","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-415-1142025","text":"","title":"Version 4.1.5, 1/14/2025"},{"location":"CHANGELOG/#added_71","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_71","text":"Custom Map/List deserializers","title":"Removed"},{"location":"CHANGELOG/#changed_71","text":"Apply ToNumberPolicy.LONG_OR_DOUBLE to GSON serialization of untyped numbers in hash map.","title":"Changed"},{"location":"CHANGELOG/#version-414-1132025","text":"","title":"Version 4.1.4, 1/13/2025"},{"location":"CHANGELOG/#added_72","text":"Task list included in \"end of flow\" logging Pipeline-while-loop unit tests EventScriptMock helper class to override a function route for a flow task","title":"Added"},{"location":"CHANGELOG/#removed_72","text":"Removed the \"threshold\" feature in variable HTTP payload in REST automation for consistent syntax with Node.js Composable version","title":"Removed"},{"location":"CHANGELOG/#changed_72","text":"Update the setException method in EventEnvelope to handle non-serializable exception Improved event script's pipeline for-loop-continue feature Normalize dataset when loading new configuration using ConfigReader","title":"Changed"},{"location":"CHANGELOG/#version-413-112025","text":"","title":"Version 4.1.3, 1/1/2025"},{"location":"CHANGELOG/#added_73","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_73","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_73","text":"Improved configuration management and refactored AppConfigReader, ConfigReader and MultiLevelMap classes Input to MultiLevelMap is now immutable Simplified event script's pipeline condition syntax Consistent exception transport for Java and Node.js composable applications Bugfix to handle untyped map inside a PoJo OSS updates as follows. Spring Boot parent version 3.4.1 Kotlin version 2.1.0 Spring Project Reactor version 3.7.1 (BOM version 2024.0.1) Google Guava version 33.4.0-jre JUnit version 5.11.4","title":"Changed"},{"location":"CHANGELOG/#version-412-12202024","text":"","title":"Version 4.1.2, 12/20/2024"},{"location":"CHANGELOG/#added_74","text":"Unit test in event-script-engine to validate that the config management system can merge parameters in application.yml and application.properties.","title":"Added"},{"location":"CHANGELOG/#removed_74","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_74","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-411-12182024","text":"","title":"Version 4.1.1, 12/18/2024"},{"location":"CHANGELOG/#added_75","text":"\"map\" constant type in input data mapping AppConfigReader will resolve key-values from system properties and environment variables at startup","title":"Added"},{"location":"CHANGELOG/#removed_75","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_75","text":"Updated Chapter-4 for the new \"map\" constant feature.","title":"Changed"},{"location":"CHANGELOG/#version-410-12112024","text":"This milestone version achieves ideal event choreography by removing additional event routing to and from the Event Manager. This would boost internal event routing performance by 50 percent.","title":"Version 4.1.0, 12/11/2024"},{"location":"CHANGELOG/#added_76","text":"Performance optimization for Event Script","title":"Added"},{"location":"CHANGELOG/#removed_76","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_76","text":"The platform-core module uses virtual threads to execute event.script.manager and task.executor directly to eliminate additional serialization overheads since the two functions are event routers themselves.","title":"Changed"},{"location":"CHANGELOG/#version-4033-12112024","text":"","title":"Version 4.0.33, 12/11/2024"},{"location":"CHANGELOG/#added_77","text":"Support of custom content types in application.yml","title":"Added"},{"location":"CHANGELOG/#removed_77","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_77","text":"Improved websocket housekeeping logic Use bench.add to replace bench.offer API","title":"Changed"},{"location":"CHANGELOG/#version-4032-1292024","text":"","title":"Version 4.0.32, 12/9/2024"},{"location":"CHANGELOG/#added_78","text":"For completeness, added Boolean AND and OR operations for simple type matching. Added traceId as metadata for a flow instance","title":"Added"},{"location":"CHANGELOG/#removed_78","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_78","text":"Update Chapter-4 for the new AND/OR type matching feature Consistent custom HTTP headers for event over http protocol and streaming content","title":"Changed"},{"location":"CHANGELOG/#version-4031-1252024","text":"","title":"Version 4.0.31, 12/5/2024"},{"location":"CHANGELOG/#added_79","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_79","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_79","text":"The \"keep.original\" key is renamed as \"keep-original\" to comply with convention. Continue processing if some preload override config files are missing.","title":"Changed"},{"location":"CHANGELOG/#version-4030-1252024","text":"","title":"Version 4.0.30, 12/5/2024"},{"location":"CHANGELOG/#added_80","text":"Implemented unique task naming feature for event flow configuration.","title":"Added"},{"location":"CHANGELOG/#removed_80","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_80","text":"The \"keep_original\" key is renamed as \"keep.original\" in preload override Chapter-4 of developer guide updated with the new task alias feature","title":"Changed"},{"location":"CHANGELOG/#version-4029-1232024","text":"","title":"Version 4.0.29, 12/3/2024"},{"location":"CHANGELOG/#added_81","text":"Added integer, long, float, double and boolean type matching for state machine.","title":"Added"},{"location":"CHANGELOG/#removed_81","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_81","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4028-11292024","text":"","title":"Version 4.0.28, 11/29/2024"},{"location":"CHANGELOG/#added_82","text":"Support for simple data type matching processing (text, substring, binary and b64) Optional external state machine","title":"Added"},{"location":"CHANGELOG/#removed_82","text":"Removed \"http.input.\" and \"http.output.\" aliases from event script. Instead, use the generic \"input.\" and \"output.\" namespaces.","title":"Removed"},{"location":"CHANGELOG/#changed_82","text":"Bugfix for AsyncHttpClient to allow missing HTTP request body in POST, PUT or PATCH request Mono reactive flow control","title":"Changed"},{"location":"CHANGELOG/#version-4027-11272024","text":"","title":"Version 4.0.27, 11/27/2024"},{"location":"CHANGELOG/#added_83","text":"Support for Mono/Flux return type for KotlinLambdaFunction Implemented Websocket handshake handler to adjust to API changes in vertx 4.5.11","title":"Added"},{"location":"CHANGELOG/#removed_83","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_83","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-4026-11262024","text":"","title":"Version 4.0.26, 11/26/2024"},{"location":"CHANGELOG/#added_84","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_84","text":"Remove pom.xml version override for netty and spring framework because Spring Boot 3.4.0 fetches the correct versions of netty and spring framework. Earlier override was done to avoid security vulnerabilities of older versions of netty and spring framework.","title":"Removed"},{"location":"CHANGELOG/#changed_84","text":"Handle the case that Mono will not return payload if the payload is null OSS update: Classgraph 4.8.179, Vertx 4.5.11, Spring Boot 3.4.0, Kafka Client 3.9.0","title":"Changed"},{"location":"CHANGELOG/#version-4025-11212024","text":"","title":"Version 4.0.25, 11/21/2024"},{"location":"CHANGELOG/#added_85","text":"Support more than one REST configuration files. When a duplicated REST entry is detected, the system will abort REST endpoint rendering and print out an error message in application log. If you have unit tests to cover the REST endpoints, the unit tests will fail accordingly.","title":"Added"},{"location":"CHANGELOG/#removed_85","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_85","text":"Improved environment variable parsing in config reader. System will skip entries with invalid environment variable reference syntax.","title":"Changed"},{"location":"CHANGELOG/#version-4024-11202024","text":"","title":"Version 4.0.24, 11/20/2024"},{"location":"CHANGELOG/#added_86","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_86","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_86","text":"Bugfix for an edge case in config reader to handle control character of brackets inside an environment variable reference. e.g. some.key=${ENV_VAR:something/{test1}/{test2}}","title":"Changed"},{"location":"CHANGELOG/#version-4023-11192024","text":"","title":"Version 4.0.23, 11/19/2024"},{"location":"CHANGELOG/#added_87","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_87","text":"ObjectStreamWriter and AsyncObjectStreamReader are removed","title":"Removed"},{"location":"CHANGELOG/#changed_87","text":"Replace ObjectStreamWriter with FluxPublisher Replace AsyncObjectStreamReader with FluxConsumer Bugfix for FluxConsumer expiry - change type from \"data\" to \"exception\".","title":"Changed"},{"location":"CHANGELOG/#version-4022-11182024","text":"","title":"Version 4.0.22, 11/18/2024"},{"location":"CHANGELOG/#added_88","text":"FluxPublisher and FluxConsumer for integration with Flux reactive response object","title":"Added"},{"location":"CHANGELOG/#removed_88","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_88","text":"Unit tests in event streaming and post office to support Flux integration Select reactor-core version 3.7.0 using dependency management (reactor-bom version 2024.0.0)","title":"Changed"},{"location":"CHANGELOG/#version-4021-11142024","text":"","title":"Version 4.0.21, 11/14/2024"},{"location":"CHANGELOG/#added_89","text":"Support for user function to return a Mono reactive response object","title":"Added"},{"location":"CHANGELOG/#removed_89","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_89","text":"Update netty to version 4.1.115.Final to address security vulnerability in 4.1.114 Move reactor-core library from rest-spring-3 to platform-core","title":"Changed"},{"location":"CHANGELOG/#version-4020-11132024","text":"","title":"Version 4.0.20, 11/13/2024"},{"location":"CHANGELOG/#added_90","text":"For ease of configuration, added \"com.accenture\" to the base packages so that user applications do not need to include it to use the event-script-engine module.","title":"Added"},{"location":"CHANGELOG/#removed_90","text":"if-then-else pipeline feature in event-script","title":"Removed"},{"location":"CHANGELOG/#changed_90","text":"Update Event Script syntax for consistency Fix error in counting number of compiled flows","title":"Changed"},{"location":"CHANGELOG/#version-4016-11102024","text":"","title":"Version 4.0.16, 11/10/2024"},{"location":"CHANGELOG/#added_91","text":"Generate unique flow instance ID as reference during flow execution.","title":"Added"},{"location":"CHANGELOG/#removed_91","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_91","text":"Save the original correlation-ID from the calling party in a flow instance and return this value to the calling party at the end of flow execution.","title":"Changed"},{"location":"CHANGELOG/#version-4015-1172024","text":"","title":"Version 4.0.15, 11/7/2024"},{"location":"CHANGELOG/#added_92","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_92","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_92","text":"renamed StartFlow to FlowExecutor","title":"Changed"},{"location":"CHANGELOG/#version-4014-1172024","text":"","title":"Version 4.0.14, 11/7/2024"},{"location":"CHANGELOG/#added_93","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_93","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_93","text":"Health check function can return either a text string or a Map StartFlow API updates","title":"Changed"},{"location":"CHANGELOG/#version-4013-1152024","text":"","title":"Version 4.0.13, 11/5/2024"},{"location":"CHANGELOG/#added_94","text":"Added helper class \"StartFlow\" to start a flow, including internal flows without HTTP or Kafka.","title":"Added"},{"location":"CHANGELOG/#removed_94","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_94","text":"Bugfix for empty YAML file to avoid null pointer exception Sort event scripts for orderly logging in the CompileFlows validation process","title":"Changed"},{"location":"CHANGELOG/#version-4012-10312024","text":"","title":"Version 4.0.12, 10/31/2024"},{"location":"CHANGELOG/#added_95","text":"New feature to support resolution of more than one environment variable for a parameter using the ConfigReader","title":"Added"},{"location":"CHANGELOG/#removed_95","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_95","text":"Update OSS modules 1. classgraph version 4.8.177 2. kotlin version 2.0.21 3. guava version 33.3.1-jre 4. jUnit version 5 jupiter Adjusted all unit tests to use jUnit 5","title":"Changed"},{"location":"CHANGELOG/#version-4011-10282024","text":"","title":"Version 4.0.11, 10/28/2024"},{"location":"CHANGELOG/#added_96","text":"New features to support: 1. multiple preload override config file 2. multiple flow list config files","title":"Added"},{"location":"CHANGELOG/#removed_96","text":"unused class \"UnauthorizedObj\" in platform-core commons-io dependency in Kafka-Standalone subproject","title":"Removed"},{"location":"CHANGELOG/#changed_96","text":"Unit test for the preload override feature JavaDoc for the MainApplication","title":"Changed"},{"location":"CHANGELOG/#version-4010-10242024","text":"","title":"Version 4.0.10, 10/24/2024"},{"location":"CHANGELOG/#added_97","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_97","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_97","text":"OSS update - Spring Boot 3.3.5 Security patch for CR/LF exploit for HTTP cookie","title":"Changed"},{"location":"CHANGELOG/#version-409-10182024","text":"","title":"Version 4.0.9, 10/18/2024"},{"location":"CHANGELOG/#added_98","text":"Added Kafka Raft for the Kafka-standalone app.","title":"Added"},{"location":"CHANGELOG/#removed_98","text":"Removed zookeeper from Kafka-standalone app.","title":"Removed"},{"location":"CHANGELOG/#changed_98","text":"Update spring framework verison 6.1.14 to avoid vulnerability in webflux","title":"Changed"},{"location":"CHANGELOG/#version-408-1092024","text":"","title":"Version 4.0.8, 10/9/2024"},{"location":"CHANGELOG/#added_99","text":"Partial support of Active Profile using the \"spring.profiles.active\" parameter Hierarchy of flows","title":"Added"},{"location":"CHANGELOG/#removed_99","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_99","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-407-1012024","text":"","title":"Version 4.0.7, 10/1/2024"},{"location":"CHANGELOG/#added_100","text":"A generic \"no-op\" function for use in event scripts.","title":"Added"},{"location":"CHANGELOG/#removed_100","text":"Feature to ping a function without payload and headers.","title":"Removed"},{"location":"CHANGELOG/#changed_100","text":"Simplified api-playground application","title":"Changed"},{"location":"CHANGELOG/#version-406-9272024","text":"","title":"Version 4.0.6, 9/27/2024"},{"location":"CHANGELOG/#added_101","text":"HTTP request Cookie value filtering using RFC-6265 strict syntax","title":"Added"},{"location":"CHANGELOG/#removed_101","text":"Automatic index page redirection filter for Spring Boot","title":"Removed"},{"location":"CHANGELOG/#changed_101","text":"Upgrade SHA-1 to SHA-512 algorithm in CryptoAPI utility Fix security vulnerability associated with HTTP request header and cookie manipulation","title":"Changed"},{"location":"CHANGELOG/#version-405-9242024","text":"","title":"Version 4.0.5, 9/24/2024"},{"location":"CHANGELOG/#added_102","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_102","text":"Feature for automatic PoJo transport in EventEnvelope and MsgPack Feature for safe.data.model deserialization Benchmark-server is no longer required","title":"Removed"},{"location":"CHANGELOG/#changed_102","text":"Update OSS versions - vertx 4.5.10, kotlin 2.0.20, spring boot 3.3.4","title":"Changed"},{"location":"CHANGELOG/#version-404-952024","text":"","title":"Version 4.0.4, 9/5/2024"},{"location":"CHANGELOG/#added_103","text":"New feature for AsyncHttpClient to render small streaming HTTP response (i.e. chunked binary data) as byte array. For details, Please refer to Appendix III, Developer Guide","title":"Added"},{"location":"CHANGELOG/#removed_103","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_103","text":"Bugfix for parsing default value of environment variable in ConfigReader. This resolves an issue when the special character colon (\":\") is used more than once in the default value.","title":"Changed"},{"location":"CHANGELOG/#version-403-942024","text":"","title":"Version 4.0.3, 9/4/2024"},{"location":"CHANGELOG/#added_104","text":"The \"preload override\" feature is added. This allows overriding a reusable composable library with a set of new route names that are unique for use in an event flow configuration script. For details, Please refer to Chapter 4, Developer Guide","title":"Added"},{"location":"CHANGELOG/#removed_104","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_104","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-402-8312024","text":"","title":"Version 4.0.2, 8/31/2024"},{"location":"CHANGELOG/#added_105","text":"New \"classpath\" namespace for input data mapping Support for input data mapping to handle subset of input request body as a Map or PoJo","title":"Added"},{"location":"CHANGELOG/#removed_105","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_105","text":"Remove the class \"type\" variable from AsyncHttpRequest Improve the \"removeElement\" method in MultiLevelMap Make HTTP input request header labels key-insensitive Update Spring Boot to version 3.3.3","title":"Changed"},{"location":"CHANGELOG/#version-401-8192024","text":"","title":"Version 4.0.1, 8/19/2024"},{"location":"CHANGELOG/#added_106","text":"new File read/write feature in Event Script's I/O data mapping","title":"Added"},{"location":"CHANGELOG/#removed_106","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_106","text":"Update Spring Boot to version 3.3.2 Update Guava to version 33.3.0-jre Update Vertx to version 4.5.9 Update Kotlin to version 2.0.10 Change \"upstream\" to \"dependency\" in the \"/health\" endpoint","title":"Changed"},{"location":"CHANGELOG/#version-400-6242024","text":"This version merges Event Script into the Mercury Composable repository.","title":"Version 4.0.0, 6/24/2024"},{"location":"CHANGELOG/#added_107","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_107","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_107","text":"Update Spring Boot to version 3.3.1 Update Guava to version 33.2.1-jre Update Vertx to version 4.5.8 Update Kotlin to version 2.0.0 Update classgraph to version 4.8.174 Optional reply event for a flow configuration Kafka-standalone is still using Spring Boot 3.2.5 due to compatibility issue","title":"Changed"},{"location":"CHANGELOG/#version-315-512024","text":"This version supercedes 3.1.4 due to updated data structure for static content handling.","title":"Version 3.1.5, 5/1/2024"},{"location":"CHANGELOG/#added_108","text":"Added optional static-content.no-cache-pages in rest.yaml AsyncHttpClientLoader","title":"Added"},{"location":"CHANGELOG/#removed_108","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_108","text":"Updated data structure for static-content section in rest.yaml Fixed bug for setting multiple HTTP cookies Unified configuration file prefix \"yaml.\"","title":"Changed"},{"location":"CHANGELOG/#version-314-4282024","text":"","title":"Version 3.1.4, 4/28/2024"},{"location":"CHANGELOG/#added_109","text":"Added optional static content HTTP-GET request filter in rest.yaml","title":"Added"},{"location":"CHANGELOG/#removed_109","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_109","text":"Updated syntax for static-content-filter","title":"Changed"},{"location":"CHANGELOG/#version-313-4242024","text":"","title":"Version 3.1.3, 4/24/2024"},{"location":"CHANGELOG/#added_110","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_110","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_110","text":"Enhanced OptionalService annotation.","title":"Changed"},{"location":"CHANGELOG/#version-312-4172024","text":"","title":"Version 3.1.2, 4/17/2024"},{"location":"CHANGELOG/#added_111","text":"Added \"app-config-reader.yml\" file in the resources folder so that you can override the default application configuration files.","title":"Added"},{"location":"CHANGELOG/#removed_111","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_111","text":"Open sources library update (Spring Boot 3.2.5, Vertx 4.5.7) Improve AppConfigReader and ConfigReader to use the app-config-reader.yml file. Enhanced OptionalService annotation.","title":"Changed"},{"location":"CHANGELOG/#version-311-282024","text":"","title":"Version 3.1.1, 2/8/2024"},{"location":"CHANGELOG/#added_112","text":"AutoStart to run application as Spring Boot if the rest-spring-3 library is packaged in app Configurable \"Event over HTTP\" - automatic forward events over HTTP using a configuration Support user defined serializer with PreLoad annotation and platform API","title":"Added"},{"location":"CHANGELOG/#removed_112","text":"Bugfix: removed websocket client connection timeout that causes the first connection to drop after one minute","title":"Removed"},{"location":"CHANGELOG/#changed_112","text":"Open sources library update (Spring Boot 3.2.2, Vertx 4.5.3 and MsgPack 0.9.8) Rename application parameter \"event.worker.pool\" to \"kernel.thread.pool\"","title":"Changed"},{"location":"CHANGELOG/#version-310-152024","text":"","title":"Version 3.1.0, 1/5/2024"},{"location":"CHANGELOG/#added_113","text":"Full integration with Java 21 Virtual Thread Default execution mode is set to \"virtual thread\" KernelThreadRunner annotation added to provide optional support of kernel threads","title":"Added"},{"location":"CHANGELOG/#removed_113","text":"Retired Spring Boot version 2 Hazelcast and ActiveMQ network connectors","title":"Removed"},{"location":"CHANGELOG/#changed_113","text":"platform-core engine updated with virtual thread","title":"Changed"},{"location":"CHANGELOG/#version-307-12232023","text":"","title":"Version 3.0.7, 12/23/2023"},{"location":"CHANGELOG/#added_114","text":"Print out basic JVM information before startup for verification of base container image.","title":"Added"},{"location":"CHANGELOG/#removed_114","text":"Removed Maven Shade packager","title":"Removed"},{"location":"CHANGELOG/#changed_114","text":"Updated open sources libraries to address security vulnerabilities Spring Boot 2/3 to version 2.7.18 and 3.2.1 respectively Tomcat 9.0.84 Vertx 4.5.1 Classgraph 4.8.165 Netty 4.1.104.Final slf4j API 2.0.9 log4j2 2.22.0 Kotlin 1.9.22 Artemis 2.31.2 Hazelcast 5.3.6 Guava 33.0.0-jre","title":"Changed"},{"location":"CHANGELOG/#version-306-10262023","text":"","title":"Version 3.0.6, 10/26/2023"},{"location":"CHANGELOG/#added_115","text":"Enhanced Benchmark tool to support \"Event over HTTP\" protocol to evaluate performance efficiency for commmunication between application containers using HTTP.","title":"Added"},{"location":"CHANGELOG/#removed_115","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_115","text":"Updated open sources libraries Spring Boot 2/3 to version 2.7.17 and 3.1.5 respectively Kafka-client 3.6.0","title":"Changed"},{"location":"CHANGELOG/#version-305-10212023","text":"","title":"Version 3.0.5, 10/21/2023"},{"location":"CHANGELOG/#added_116","text":"Support two executable JAR packaging system: 1. Maven Shade packager 2. Spring Boot packager Starting from version 3.0.5, we have replaced Spring Boot packager with Maven Shade. This avoids a classpath edge case for Spring Boot packager when running kafka-client under Java 11 or higher. Maven Shade also results in smaller executable JAR size.","title":"Added"},{"location":"CHANGELOG/#removed_116","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_116","text":"Updated open sources libraries Spring-Boot 2.7.16 / 3.1.4 classgraph 4.8.163 snakeyaml 2.2 kotlin 1.9.10 vertx 4.4.6 guava 32.1.3-jre msgpack 0.9.6 slj4j 2.0.9 zookeeper 3.7.2 The \"/info/lib\" admin endpoint has been enhanced to list library dependencies for executable JAR generated by either Maven Shade or Spring Boot Packager. Improved ConfigReader to recognize both \".yml\" and \".yaml\" extensions and their uses are interchangeable.","title":"Changed"},{"location":"CHANGELOG/#version-304-862023","text":"","title":"Version 3.0.4, 8/6/2023"},{"location":"CHANGELOG/#added_117","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_117","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_117","text":"Updated open sources libraries Spring-Boot 2.7.14 / 3.1.2 Kafka-client 3.5.1 classgraph 4.8.161 guava 32.1.2-jre msgpack 0.9.5","title":"Changed"},{"location":"CHANGELOG/#version-303-6272023","text":"","title":"Version 3.0.3, 6/27/2023"},{"location":"CHANGELOG/#added_118","text":"File extension to MIME type mapping for static HTML file handling","title":"Added"},{"location":"CHANGELOG/#removed_118","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_118","text":"Open sources library update - Kotlin version 1.9.0","title":"Changed"},{"location":"CHANGELOG/#version-302-692023","text":"","title":"Version 3.0.2, 6/9/2023"},{"location":"CHANGELOG/#added_119","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_119","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_119","text":"Consistent exception handling for Event API endpoint Open sources lib update - Vertx 4.4.4, Spring Boot 2.7.13, Spring Boot 3.1.1, classgraph 4.8.160, guava 32.0.1-jre","title":"Changed"},{"location":"CHANGELOG/#version-301-652023","text":"In this release, we have replace Google HTTP Client with vertx non-blocking WebClient. We also tested compatibility up to OpenJDK version 20 and maven 3.9.2.","title":"Version 3.0.1, 6/5/2023"},{"location":"CHANGELOG/#added_120","text":"When \"x-raw-xml\" HTTP request header is set to \"true\", the AsyncHttpClient will skip the built-in XML serialization so that your application can retrieve the original XML text.","title":"Added"},{"location":"CHANGELOG/#removed_120","text":"Retire Google HTTP client","title":"Removed"},{"location":"CHANGELOG/#changed_120","text":"Upgrade maven plugin versions.","title":"Changed"},{"location":"CHANGELOG/#version-300-4182023","text":"This is a major release with some breaking changes. Please refer to Chapter-10 (Migration guide) for details. This version brings the best of preemptive and cooperating multitasking to Java (version 1.8 to 19) before Java 19 virtual thread feature becomes officially available.","title":"Version 3.0.0, 4/18/2023"},{"location":"CHANGELOG/#added_121","text":"Function execution engine supporting kernel thread pool, Kotlin coroutine and suspend function \"Event over HTTP\" service for inter-container communication Support for Spring Boot version 3 and WebFlux Sample code for a pre-configured Spring Boot 3 application","title":"Added"},{"location":"CHANGELOG/#removed_121","text":"Remove blocking APIs from platform-core Retire PM2 process manager sample script due to compatibility issue","title":"Removed"},{"location":"CHANGELOG/#changed_121","text":"Refactor \"async.http.request\" to use vertx web client for non-blocking operation Update log4j2 version 2.20.0 and slf4j version 2.0.7 in platform-core Update JBoss RestEasy JAX_RS to version 3.15.6.Final in rest-spring Update vertx to 4.4.2 Update Spring Boot parent pom to 2.7.12 and 3.1.0 for spring boot 2 and 3 respectively Remove com.fasterxml.classmate dependency from rest-spring","title":"Changed"},{"location":"CHANGELOG/#version-280-3202023","text":"","title":"Version 2.8.0, 3/20/2023"},{"location":"CHANGELOG/#added_122","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_122","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_122","text":"Improved load balancing in cloud-connector Filter URI to avoid XSS attack Upgrade to SnakeYaml 2.0 and patch Spring Boot 2.6.8 for compatibility with it Upgrade to Vertx 4.4.0, classgraph 4.8.157, tomcat 9.0.73","title":"Changed"},{"location":"CHANGELOG/#version-271-12222022","text":"","title":"Version 2.7.1, 12/22/2022"},{"location":"CHANGELOG/#added_123","text":"standalone benchmark report app client and server benchmark apps add timeout tag to RPC events","title":"Added"},{"location":"CHANGELOG/#removed_123","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_123","text":"Updated open sources dependencies Netty 4.1.86.Final Tomcat 9.0.69 Vertx 4.3.6 classgraph 4.8.152 google-http-client 1.42.3 Improved unit tests to use assertThrows to evaluate exception Enhanced AsyncHttpRequest serialization","title":"Changed"},{"location":"CHANGELOG/#version-270-11112022","text":"In this version, REST automation code is moved to platform-core such that REST and Websocket service can share the same port.","title":"Version 2.7.0, 11/11/2022"},{"location":"CHANGELOG/#added_124","text":"AsyncObjectStreamReader is added for non-blocking read operation from an object stream. Support of LocalDateTime in SimpleMapper Add \"removeElement\" method to MultiLevelMap Automatically convert a map to a PoJo when the sender does not specify class in event body","title":"Added"},{"location":"CHANGELOG/#removed_124","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_124","text":"REST automation becomes part of platform-core and it can co-exist with Spring Web in the rest-spring module Enforce Spring Boot lifecycle management such that user apps will start after Spring Boot has loaded all components Update netty to version 4.1.84.Final","title":"Changed"},{"location":"CHANGELOG/#version-260-10132022","text":"In this version, websocket notification example code has been removed from the REST automation system. If your application uses this feature, please recover the code from version 2.5.0 and refactor it as a separate library.","title":"Version 2.6.0, 10/13/2022"},{"location":"CHANGELOG/#added_125","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_125","text":"Simplify REST automation system by removing websocket notification example in REST automation.","title":"Removed"},{"location":"CHANGELOG/#changed_125","text":"Replace Tomcat websocket server with Vertx non-blocking websocket server library Update netty to version 4.1.79.Final Update kafka client to version 2.8.2 Update snake yaml to version 1.33 Update gson to version 2.9.1","title":"Changed"},{"location":"CHANGELOG/#version-250-9102022","text":"","title":"Version 2.5.0, 9/10/2022"},{"location":"CHANGELOG/#added_126","text":"New Preload annotation class to automate pre-registration of LambdaFunction.","title":"Added"},{"location":"CHANGELOG/#removed_126","text":"Removed Spring framework and Tomcat dependencies from platform-core so that the core library can be applied to legacy J2EE application without library conflict.","title":"Removed"},{"location":"CHANGELOG/#changed_126","text":"Bugfix for proper housekeeping of future events. Make Gson and MsgPack handling of integer/long consistent Updated open sources libraries. Eclipse vertx-core version 4.3.4 MsgPack version 0.9.3 Google httpclient version 1.42.2 SnakeYaml version 1.31","title":"Changed"},{"location":"CHANGELOG/#version-236-6212022","text":"","title":"Version 2.3.6, 6/21/2022"},{"location":"CHANGELOG/#added_127","text":"Support more than one event stream cluster. User application can share the same event stream cluster for pub/sub or connect to an alternative cluster for pub/sub use cases.","title":"Added"},{"location":"CHANGELOG/#removed_127","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_127","text":"Cloud connector libraries update to Hazelcast 5.1.2","title":"Changed"},{"location":"CHANGELOG/#version-235-5302022","text":"","title":"Version 2.3.5, 5/30/2022"},{"location":"CHANGELOG/#added_128","text":"Add tagging feature to handle language connector's routing and exception handling","title":"Added"},{"location":"CHANGELOG/#removed_128","text":"Remove language pack's pub/sub broadcast feature","title":"Removed"},{"location":"CHANGELOG/#changed_128","text":"Update Spring Boot parent to version 2.6.8 to fetch Netty 4.1.77 and Spring Framework 5.3.20 Streamlined language connector transport protocol for compatibility with both Python and Node.js","title":"Changed"},{"location":"CHANGELOG/#version-234-5142022","text":"","title":"Version 2.3.4, 5/14/2022"},{"location":"CHANGELOG/#added_129","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_129","text":"Remove swagger-ui distribution from api-playground such that developer can clone the latest version","title":"Removed"},{"location":"CHANGELOG/#changed_129","text":"Update application.properties (from spring.resources.static-locations to spring.web.resources.static-locations) Update log4j, Tomcat and netty library version using Spring parent 2.6.6","title":"Changed"},{"location":"CHANGELOG/#version-233-3302022","text":"","title":"Version 2.3.3, 3/30/2022"},{"location":"CHANGELOG/#added_130","text":"Enhanced AsyncRequest to handle non-blocking fork-n-join","title":"Added"},{"location":"CHANGELOG/#removed_130","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_130","text":"Upgrade Spring Boot from 2.6.3 to 2.6.6","title":"Changed"},{"location":"CHANGELOG/#version-232-2212022","text":"","title":"Version 2.3.2, 2/21/2022"},{"location":"CHANGELOG/#added_131","text":"Add support of queue API in native pub/sub module for improved ESB compatibility","title":"Added"},{"location":"CHANGELOG/#removed_131","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_131","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-231-2192022","text":"","title":"Version 2.3.1, 2/19/2022"},{"location":"CHANGELOG/#added_132","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_132","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_132","text":"Update Vertx to version 4.2.4 Update Tomcat to version 5.0.58 Use Tomcat websocket server for presence monitors Bugfix - Simple Scheduler's leader election searches peers correctly","title":"Changed"},{"location":"CHANGELOG/#version-230-1282022","text":"","title":"Version 2.3.0, 1/28/2022"},{"location":"CHANGELOG/#added_133","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_133","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_133","text":"Update copyright notice Update Vertx to version 4.2.3 Bugfix - RSA key generator supporting key length from 1024 to 4096 bits CryptoAPI - support different AES algorithms and custom IV Update Spring Boot to version 2.6.3","title":"Changed"},{"location":"CHANGELOG/#version-223-12292021","text":"","title":"Version 2.2.3, 12/29/2021"},{"location":"CHANGELOG/#added_134","text":"Transaction journaling Add parameter distributed.trace.aggregation in application.properties such that trace aggregation may be disabled.","title":"Added"},{"location":"CHANGELOG/#removed_134","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_134","text":"Update JBoss RestEasy library to 3.15.3.Final Improved po.search(route) to scan local and remote service registries. Added \"remoteOnly\" selection. Fix bug in releasing presence monitor topic for specific closed user group Update Apache log4j to version 2.17.1 Update Spring Boot parent to version 2.6.1 Update Netty to version 4.1.72.Final Update Vertx to version 4.2.2 Convenient class \"UserNotification\" for backend service to publish events to the UI when REST automation is deployed","title":"Changed"},{"location":"CHANGELOG/#version-222-11122021","text":"","title":"Version 2.2.2, 11/12/2021"},{"location":"CHANGELOG/#added_135","text":"User defined API authentication functions can be selected using custom HTTP request header \"Exception chaining\" feature in EventEnvelope New \"deferred.commit.log\" parameter for backward compatibility with older PowerMock in unit tests","title":"Added"},{"location":"CHANGELOG/#removed_135","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_135","text":"Improved and streamlined SimpleXmlParser to handle arrays Bugfix for file upload in Service Gateway (REST automation library) Update Tomcat library from 9.0.50 to 9.0.54 Update Spring Boot library to 2.5.6 Update GSON library to 2.8.9","title":"Changed"},{"location":"CHANGELOG/#version-221-1012021","text":"","title":"Version 2.2.1, 10/1/2021"},{"location":"CHANGELOG/#added_136","text":"Callback function can implement ServiceExceptionHandler to catch exception. It adds the onError() method.","title":"Added"},{"location":"CHANGELOG/#removed_136","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_136","text":"Open sources library update - Vert.x 4.1.3, Netty 4.1.68-Final","title":"Changed"},{"location":"CHANGELOG/#version-211-9102021","text":"","title":"Version 2.1.1, 9/10/2021"},{"location":"CHANGELOG/#added_137","text":"User defined PoJo and Generics mapping Standardized serializers for default case, snake_case and camelCase Support of EventEnvelope as input parameter in TypedLambdaFunction so application function can inspect event's metadata Application can subscribe to life cycle events of other application instances","title":"Added"},{"location":"CHANGELOG/#removed_137","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_137","text":"Replace Tomcat websocket server engine with Vertx in presence monitor for higher performance Bugfix for MsgPack transport of integer, long, BigInteger and BigDecimal","title":"Changed"},{"location":"CHANGELOG/#version-210-7252021","text":"","title":"Version 2.1.0, 7/25/2021"},{"location":"CHANGELOG/#added_138","text":"Multicast - application can define a multicast.yaml config to relay events to more than one target service. StreamFunction - function that allows the application to control back-pressure","title":"Added"},{"location":"CHANGELOG/#removed_138","text":"\"object.streams.io\" route is removed from platform-core","title":"Removed"},{"location":"CHANGELOG/#changed_138","text":"Elastic Queue - Refactored using Oracle Berkeley DB Object stream I/O - simplified design using the new StreamFunction feature Open sources library update - Spring Boot 2.5.2, Tomcat 9.0.50, Vert.x 4.1.1, Netty 4.1.66-Final","title":"Changed"},{"location":"CHANGELOG/#version-200-552021","text":"Vert.x is introduced as the in-memory event bus","title":"Version 2.0.0, 5/5/2021"},{"location":"CHANGELOG/#added_139","text":"ActiveMQ and Tibco connectors Admin endpoints to stop, suspend and resume an application instance Handle edge case to detect stalled application instances Add \"isStreamingPubSub\" method to the PubSub interface","title":"Added"},{"location":"CHANGELOG/#removed_139","text":"Event Node event stream emulator has been retired. You may use standalone Kafka server as a replacement for development and testing in your laptop. Multi-tenancy namespace configuration has been retired. It is replaced by the \"closed user group\" feature.","title":"Removed"},{"location":"CHANGELOG/#changed_139","text":"Refactored Kafka and Hazelcast connectors to support virtual topics and closed user groups. Updated ConfigReader to be consistent with Spring value substitution logic for application properties Replace Akka actor system with Vert.x event bus Common code for various cloud connectors consolidated into cloud core libraries","title":"Changed"},{"location":"CHANGELOG/#version-1130-1152021","text":"Version 1.13.0 is the last version that uses Akka as the in-memory event system.","title":"Version 1.13.0, 1/15/2021"},{"location":"CHANGELOG/#version-11266-1152021","text":"","title":"Version 1.12.66, 1/15/2021"},{"location":"CHANGELOG/#added_140","text":"A simple websocket notification service is integrated into the REST automation system Seamless migration feature is added to the REST automation system","title":"Added"},{"location":"CHANGELOG/#removed_140","text":"Legacy websocket notification example application","title":"Removed"},{"location":"CHANGELOG/#changed_140","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-11265-1292020","text":"","title":"Version 1.12.65, 12/9/2020"},{"location":"CHANGELOG/#added_141","text":"\"kafka.pubsub\" is added as a cloud service File download example in the lambda-example project \"trace.log.header\" added to application.properties - when tracing is enabled, this inserts the trace-ID of the transaction in the log context. For more details, please refer to the Developer Guide Add API to pub/sub engine to support creation of topic with partitions TypedLambdaFunction is added so that developer can predefine input and output classes in a service without casting","title":"Added"},{"location":"CHANGELOG/#removed_141","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_141","text":"Decouple Kafka pub/sub from kafka connector so that native pub/sub can be used when application is running in standalone mode Rename \"relay\" to \"targetHost\" in AsyncHttpRequest data model Enhanced routing table distribution by sending a complete list of route tables, thus reducing network admin traffic.","title":"Changed"},{"location":"CHANGELOG/#version-11264-9282020","text":"","title":"Version 1.12.64, 9/28/2020"},{"location":"CHANGELOG/#added_142","text":"If predictable topic is set, application instances will report their predictable topics as \"instance ID\" to the presence monitor. This improves visibility when a developer tests their application in \"hybrid\" mode. i.e. running the app locally and connect to the cloud remotely for event streams and cloud resources.","title":"Added"},{"location":"CHANGELOG/#removed_142","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_142","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-11263-8272020","text":"","title":"Version 1.12.63, 8/27/2020"},{"location":"CHANGELOG/#added_143","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_143","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_143","text":"Improved Kafka producer and consumer pairing","title":"Changed"},{"location":"CHANGELOG/#version-11262-8122020","text":"","title":"Version 1.12.62, 8/12/2020"},{"location":"CHANGELOG/#added_144","text":"New presence monitor's admin endpoint for the operator to force routing table synchronization (\"/api/ping/now\")","title":"Added"},{"location":"CHANGELOG/#removed_144","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_144","text":"Improved routing table integrity check","title":"Changed"},{"location":"CHANGELOG/#version-11261-882020","text":"","title":"Version 1.12.61, 8/8/2020"},{"location":"CHANGELOG/#added_145","text":"Event stream systems like Kafka assume topic to be used long term. This version adds support to reuse the same topic when an application instance restarts. You can create a predictable topic using unique application name and instance ID. For example, with Kubernetes, you can use the POD name as the unique application instance topic.","title":"Added"},{"location":"CHANGELOG/#removed_145","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_145","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-11256-842020","text":"","title":"Version 1.12.56, 8/4/2020"},{"location":"CHANGELOG/#added_146","text":"Automate trace for fork-n-join use case","title":"Added"},{"location":"CHANGELOG/#removed_146","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_146","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-11255-7192020","text":"","title":"Version 1.12.55, 7/19/2020"},{"location":"CHANGELOG/#added_147","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_147","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_147","text":"Improved distributed trace - set the \"from\" address in EventEnvelope automatically.","title":"Changed"},{"location":"CHANGELOG/#version-11254-7102020","text":"","title":"Version 1.12.54, 7/10/2020"},{"location":"CHANGELOG/#added_148","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_148","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_148","text":"Application life-cycle management - User provided main application(s) will be started after Spring Boot declares web application ready. This ensures correct Spring autowiring or dependencies are available. Bugfix for locale - String.format(float) returns comma as decimal point that breaks number parser. Replace with BigDecimal decimal point scaling. Bugfix for Tomcat 9.0.35 - Change Async servlet default timeout from 30 seconds to -1 so the system can handle the whole life-cycle directly.","title":"Changed"},{"location":"CHANGELOG/#version-11252-6112020","text":"","title":"Version 1.12.52, 6/11/2020"},{"location":"CHANGELOG/#added_149","text":"new \"search\" method in Post Office to return a list of application instances for a service simple \"cron\" job scheduler as an extension project add \"sequence\" to MainApplication annotation for orderly execution when more than one MainApplication is available support \"Optional\" object in EventEnvelope so a LambdaFunction can read and return Optional","title":"Added"},{"location":"CHANGELOG/#removed_149","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_149","text":"The rest-spring library has been updated to support both JAR and WAR deployment All pom.xml files updated accordingly PersistentWsClient will back off for 10 seconds when disconnected by remote host","title":"Changed"},{"location":"CHANGELOG/#version-11250-5202020","text":"","title":"Version 1.12.50, 5/20/2020"},{"location":"CHANGELOG/#added_150","text":"Payload segmentation For large payload in an event, the payload is automatically segmented into 64 KB segments. When there are more than one target application instances, the system ensures that the segments of the same event is delivered to exactly the same target. PersistentWsClient added - generalized persistent websocket client for Event Node, Kafka reporter and Hazelcast reporter.","title":"Added"},{"location":"CHANGELOG/#removed_150","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_150","text":"Code cleaning to improve consistency Upgraded to hibernate-validator to v6.1.5.Final and Hazelcast version 4.0.1 REST automation is provided as a library and an application to handle different use cases","title":"Changed"},{"location":"CHANGELOG/#version-11240-542020","text":"","title":"Version 1.12.40, 5/4/2020"},{"location":"CHANGELOG/#added_151","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_151","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_151","text":"For security reason, upgrade log4j to version 2.13.2","title":"Changed"},{"location":"CHANGELOG/#version-11239-532020","text":"","title":"Version 1.12.39, 5/3/2020"},{"location":"CHANGELOG/#added_152","text":"Use RestEasy JAX-RS library","title":"Added"},{"location":"CHANGELOG/#removed_152","text":"For security reason, removed Jersey JAX-RS library","title":"Removed"},{"location":"CHANGELOG/#changed_152","text":"Updated RestLoader to initialize RestEasy servlet dispatcher Support nested arrays in MultiLevelMap","title":"Changed"},{"location":"CHANGELOG/#version-11236-4162020","text":"","title":"Version 1.12.36, 4/16/2020"},{"location":"CHANGELOG/#added_153","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_153","text":"For simplicity, retire route-substitution admin endpoint. Route substitution uses a simple static table in route-substitution.yaml.","title":"Removed"},{"location":"CHANGELOG/#changed_153","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-11235-4122020","text":"","title":"Version 1.12.35, 4/12/2020"},{"location":"CHANGELOG/#added_154","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_154","text":"SimpleRBAC class is retired","title":"Removed"},{"location":"CHANGELOG/#changed_154","text":"Improved ConfigReader and AppConfigReader with automatic key-value normalization for YAML and JSON files Improved pub/sub module in kafka-connector","title":"Changed"},{"location":"CHANGELOG/#version-11234-3282020","text":"","title":"Version 1.12.34, 3/28/2020"},{"location":"CHANGELOG/#added_155","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_155","text":"Retired proprietary config manager since we can use the \"BeforeApplication\" approach to load config from Kubernetes configMap or other systems of config record.","title":"Removed"},{"location":"CHANGELOG/#changed_155","text":"Added \"isZero\" method to the SimpleMapper class Convert BigDecimal to string without scientific notation (i.e. toPlainString instead of toString) Corresponding unit tests added to verify behavior","title":"Changed"},{"location":"CHANGELOG/#version-11232-3142020","text":"","title":"Version 1.12.32, 3/14/2020"},{"location":"CHANGELOG/#added_156","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_156","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_156","text":"Kafka-connector will shutdown application instance when the EventProducer cannot send event to Kafka. This would allow the infrastructure to restart application instance automatically.","title":"Changed"},{"location":"CHANGELOG/#version-11231-2262020","text":"","title":"Version 1.12.31, 2/26/2020"},{"location":"CHANGELOG/#added_157","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_157","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_157","text":"Kafka-connector now supports external service provider for Kafka properties and credentials. If your application implements a function with route name \"kafka.properties.provider\" before connecting to cloud, the kafka-connector will retrieve kafka credentials on demand. This addresses case when kafka credentials change after application start-up. Interceptors are designed to forward requests and thus they do not generate replies. However, if you implement a function as an EventInterceptor, your function can throw exception just like a regular function and the exception will be returned to the calling function. This makes it easier to write interceptors.","title":"Changed"},{"location":"CHANGELOG/#version-11230-262020","text":"","title":"Version 1.12.30, 2/6/2020"},{"location":"CHANGELOG/#added_158","text":"Expose \"async.http.request\" as a PUBLIC function (\"HttpClient as a service\")","title":"Added"},{"location":"CHANGELOG/#removed_158","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_158","text":"Improved Hazelcast client connection stability Improved Kafka native pub/sub","title":"Changed"},{"location":"CHANGELOG/#version-11229-1102020","text":"","title":"Version 1.12.29, 1/10/2020"},{"location":"CHANGELOG/#added_159","text":"Rest-automation will transport X-Trace-Id from/to Http request/response, therefore extending distributed trace across systems that support the X-Trace-Id HTTP header. Added endpoint and service to shutdown application instance.","title":"Added"},{"location":"CHANGELOG/#removed_159","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_159","text":"Updated SimpleXmlParser with XML External Entity (XXE) injection prevention. Bug fix for hazelcast recovery logic - when a hazelcast node is down, the app instance will restart the hazelcast client and reset routing table correctly. HSTS header insertion is optional so that we can disable it to avoid duplicated header when API gateway is doing it.","title":"Changed"},{"location":"CHANGELOG/#version-11226-142020","text":"","title":"Version 1.12.26, 1/4/2020"},{"location":"CHANGELOG/#added_160","text":"Feature to disable PoJo deserialization so that caller can decide if the result set should be in PoJo or a Map.","title":"Added"},{"location":"CHANGELOG/#removed_160","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_160","text":"Simplified key management for Event Node AsyncHttpRequest case insensitivity for headers, cookies, path parameters and session key-values Make built-in configuration management optional","title":"Changed"},{"location":"CHANGELOG/#version-11219-12282019","text":"","title":"Version 1.12.19, 12/28/2019"},{"location":"CHANGELOG/#added_161","text":"Added HTTP relay feature in rest-automation project","title":"Added"},{"location":"CHANGELOG/#removed_161","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_161","text":"Improved hazelcast retry and peer discovery logic Refactored rest-automation's service gateway module to use AsyncHttpRequest Info endpoint to show routing table of a peer","title":"Changed"},{"location":"CHANGELOG/#version-11217-12162019","text":"","title":"Version 1.12.17, 12/16/2019"},{"location":"CHANGELOG/#added_162","text":"Simple configuration management is added to event-node, hazelcast-presence and kafka-presence monitors Added BeforeApplication annotation - this allows user application to execute some setup logic before the main application starts. e.g. modifying parameters in application.properties Added API playground as a convenient standalone application to render OpenAPI 2.0 and 3.0 yaml and json files Added argument parser in rest-automation helper app to use a static HTML folder in the local file system if arguments -html file_path is given when starting the JAR file.","title":"Added"},{"location":"CHANGELOG/#removed_162","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_162","text":"Kafka publisher timeout value changed from 10 to 20 seconds Log a warning when Kafka takes more than 5 seconds to send an event","title":"Changed"},{"location":"CHANGELOG/#version-11214-11202019","text":"","title":"Version 1.12.14, 11/20/2019"},{"location":"CHANGELOG/#added_163","text":"getRoute() method is added to PostOffice to facilitate RBAC The route name of the current service is added to an outgoing event when the \"from\" field is not present Simple RBAC using YAML configuration instead of code","title":"Added"},{"location":"CHANGELOG/#removed_163","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_163","text":"Updated Spring Boot to v2.2.1","title":"Changed"},{"location":"CHANGELOG/#version-11212-10262019","text":"","title":"Version 1.12.12, 10/26/2019"},{"location":"CHANGELOG/#added_164","text":"Multi-tenancy support for event streams (Hazelcast and Kafka). This allows the use of a single event stream cluster for multiple non-prod environments. For production, it must use a separate event stream cluster for security reason.","title":"Added"},{"location":"CHANGELOG/#removed_164","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_164","text":"logging framework changed from logback to log4j2 (version 2.12.1) Use JSR-356 websocket annotated ClientEndpoint Improved websocket reconnection logic","title":"Changed"},{"location":"CHANGELOG/#version-1129-9142019","text":"","title":"Version 1.12.9, 9/14/2019"},{"location":"CHANGELOG/#added_165","text":"Distributed tracing implemented in platform-core and rest-automation Improved HTTP header transformation for rest-automation","title":"Added"},{"location":"CHANGELOG/#removed_165","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_165","text":"language pack API key obtained from environment variable","title":"Changed"},{"location":"CHANGELOG/#version-1128-8152019","text":"","title":"Version 1.12.8, 8/15/2019"},{"location":"CHANGELOG/#added_166","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_166","text":"rest-core subproject has been merged with rest-spring","title":"Removed"},{"location":"CHANGELOG/#changed_166","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-1127-7152019","text":"","title":"Version 1.12.7, 7/15/2019"},{"location":"CHANGELOG/#added_167","text":"Periodic routing table integrity check (15 minutes) Set kafka read pointer to the beginning for new application instances except presence monitor REST automation helper application in the \"extensions\" project Support service discovery of multiple routes in the updated PostOffice's exists() method logback to set log level based on environment variable LOG_LEVEL (default is INFO)","title":"Added"},{"location":"CHANGELOG/#removed_167","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_167","text":"Minor refactoring of kafka-connector and hazelcast-connector to ensure that they can coexist if you want to include both of these dependencies in your project. This is for convenience of dev and testing. In production, please select only one cloud connector library to reduce memory footprint.","title":"Changed"},{"location":"CHANGELOG/#version-1124-6242019","text":"","title":"Version 1.12.4, 6/24/2019"},{"location":"CHANGELOG/#added_168","text":"Add inactivity expiry timer to ObjectStreamIO so that house-keeper can clean up resources that are idle","title":"Added"},{"location":"CHANGELOG/#removed_168","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_168","text":"Disable HTML encape sequence for GSON serializer Bug fix for GSON serialization optimization Bug fix for Object Stream housekeeper By default, GSON serializer converts all numbers to double, resulting in unwanted decimal point for integer and long. To handle custom map serialization for correct representation of numbers, an unintended side effect was introduced in earlier releases. List of inner PoJo would be incorrectly serialized as map, resulting in casting exception. This release resolves this issue.","title":"Changed"},{"location":"CHANGELOG/#version-1121-6102019","text":"","title":"Version 1.12.1, 6/10/2019"},{"location":"CHANGELOG/#added_169","text":"Store-n-forward pub/sub API will be automatically enabled if the underlying cloud connector supports it. e.g. kafka ObjectStreamIO, a convenient wrapper class, to provide event stream I/O API. Object stream feature is now a standard feature instead of optional. Deferred delivery added to language connector.","title":"Added"},{"location":"CHANGELOG/#removed_169","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_169","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#version-11140-5252019","text":"","title":"Version 1.11.40, 5/25/2019"},{"location":"CHANGELOG/#added_170","text":"Route substitution for simple versioning use case Add \"Strict Transport Security\" header if HTTPS (https://tools.ietf.org/html/rfc6797) Event stream connector for Kafka Distributed housekeeper feature for Hazelcast connector","title":"Added"},{"location":"CHANGELOG/#removed_170","text":"System log service","title":"Removed"},{"location":"CHANGELOG/#changed_170","text":"Refactoring of Hazelcast event stream connector library to sync up with the new Kafka connector.","title":"Changed"},{"location":"CHANGELOG/#version-11139-4302019","text":"","title":"Version 1.11.39, 4/30/2019"},{"location":"CHANGELOG/#added_171","text":"Language-support service application for Python, Node.js and Go, etc. Python language pack project is available at https://github.com/Accenture/mercury-python","title":"Added"},{"location":"CHANGELOG/#removed_171","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_171","text":"replace Jackson serialization engine with Gson ( platform-core project) replace Apache HttpClient with Google Http Client ( rest-spring ) remove Jackson dependencies from Spring Boot ( rest-spring ) interceptor improvement","title":"Changed"},{"location":"CHANGELOG/#version-11133-3252019","text":"","title":"Version 1.11.33, 3/25/2019"},{"location":"CHANGELOG/#added_172","text":"N/A","title":"Added"},{"location":"CHANGELOG/#removed_172","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_172","text":"Move safe.data.models validation rules from EventEnvelope to SimpleMapper Apache fluent HTTP client downgraded to version 4.5.6 because the pom file in 4.5.7 is invalid","title":"Changed"},{"location":"CHANGELOG/#version-11130-372019","text":"","title":"Version 1.11.30, 3/7/2019"},{"location":"CHANGELOG/#added_173","text":"Added retry logic in persistent queue when OS cannot update local file metadata in real-time for Windows based machine.","title":"Added"},{"location":"CHANGELOG/#removed_173","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_173","text":"pom.xml changes - update with latest 3rd party open sources dependencies.","title":"Changed"},{"location":"CHANGELOG/#version-11129-1252019","text":"","title":"Version 1.11.29, 1/25/2019"},{"location":"CHANGELOG/#added_174","text":"platform-core Support for long running functions so that any long queries will not block the rest of the system. \"safe.data.models\" is available as an option in the application.properties. This is an additional security measure to protect against Jackson deserialization vulnerability. See example below: # # additional security to protect against model injection # comma separated list of model packages that are considered safe to be used for object deserialization # #safe.data.models=com.accenture.models rest-spring \"/env\" endpoint is added. See sample application.properties below: # # environment and system properties to be exposed to the \"/env\" admin endpoint # show.env.variables=USER, TEST show.application.properties=server.port, cloud.connector","title":"Added"},{"location":"CHANGELOG/#removed_174","text":"N/A","title":"Removed"},{"location":"CHANGELOG/#changed_174","text":"platform-core Use Java Future and an elastic cached thread pool for executing user functions.","title":"Changed"},{"location":"CHANGELOG/#fixed","text":"N/A","title":"Fixed"},{"location":"CHANGELOG/#version-11128-12202018","text":"","title":"Version 1.11.28, 12/20/2018"},{"location":"CHANGELOG/#added_175","text":"Hazelcast support is added. This includes two projects (hazelcast-connector and hazelcast-presence). Hazelcast-connector is a cloud connector library. Hazelcast-presence is the \"Presence Monitor\" for monitoring the presence status of each application instance.","title":"Added"},{"location":"CHANGELOG/#removed_175","text":"platform-core The \"fixed resource manager\" feature is removed because the same outcome can be achieved at the application level. e.g. The application can broadcast requests to multiple application instances with the same route name and use a callback function to receive response asynchronously. The services can provide resource metrics so that the caller can decide which is the most available instance to contact. For simplicity, resources management is better left to the cloud platform or the application itself.","title":"Removed"},{"location":"CHANGELOG/#changed_175","text":"N/A","title":"Changed"},{"location":"CHANGELOG/#fixed_1","text":"N/A","title":"Fixed"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting Kevin Bader (the current project maintainer). All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html","title":"Code of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting Kevin Bader (the current project maintainer). All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html","title":"Attribution"},{"location":"CONTRIBUTING/","text":"Contributing to the Mercury framework Thanks for taking the time to contribute! The following is a set of guidelines for contributing to Mercury and its packages, which are hosted in the Accenture Organization on GitHub. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request. Code of Conduct This project and everyone participating in it is governed by our Code of Conduct . By participating, you are expected to uphold this code. Please report unacceptable behavior to Kevin Bader, who is the current project maintainer. What should I know before I get started? We follow the standard GitHub workflow . Before submitting a Pull Request: Please write tests. Make sure you run all tests and check for warnings. Think about whether it makes sense to document the change in some way. For smaller, internal changes, inline documentation might be sufficient, while more visible ones might warrant a change to the developer's guide or the README . Update CHANGELOG.md file with your current change in form of [Type of change e.g. Config, Kafka, .etc] with a short description of what it is all about and a link to issue or pull request, and choose a suitable section (i.e., changed, added, fixed, removed, deprecated). Design Decisions When we make a significant decision in how to write code, or how to maintain the project and what we can or cannot support, we will document it using Architecture Decision Records (ADR) . Take a look at the design notes for existing ADRs. If you have a question around how we do things, check to see if it is documented there. If it is not documented there, please ask us - chances are you're not the only one wondering. Of course, also feel free to challenge the decisions by starting a discussion on the mailing list.","title":"Contribution"},{"location":"CONTRIBUTING/#contributing-to-the-mercury-framework","text":"Thanks for taking the time to contribute! The following is a set of guidelines for contributing to Mercury and its packages, which are hosted in the Accenture Organization on GitHub. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.","title":"Contributing to the Mercury framework"},{"location":"CONTRIBUTING/#code-of-conduct","text":"This project and everyone participating in it is governed by our Code of Conduct . By participating, you are expected to uphold this code. Please report unacceptable behavior to Kevin Bader, who is the current project maintainer.","title":"Code of Conduct"},{"location":"CONTRIBUTING/#what-should-i-know-before-i-get-started","text":"We follow the standard GitHub workflow . Before submitting a Pull Request: Please write tests. Make sure you run all tests and check for warnings. Think about whether it makes sense to document the change in some way. For smaller, internal changes, inline documentation might be sufficient, while more visible ones might warrant a change to the developer's guide or the README . Update CHANGELOG.md file with your current change in form of [Type of change e.g. Config, Kafka, .etc] with a short description of what it is all about and a link to issue or pull request, and choose a suitable section (i.e., changed, added, fixed, removed, deprecated).","title":"What should I know before I get started?"},{"location":"CONTRIBUTING/#design-decisions","text":"When we make a significant decision in how to write code, or how to maintain the project and what we can or cannot support, we will document it using Architecture Decision Records (ADR) . Take a look at the design notes for existing ADRs. If you have a question around how we do things, check to see if it is documented there. If it is not documented there, please ask us - chances are you're not the only one wondering. Of course, also feel free to challenge the decisions by starting a discussion on the mailing list.","title":"Design Decisions"},{"location":"INCLUSIVITY/","text":"TECHNOLOGY INCLUSIVE LANGUAGE GUIDEBOOK As an organization, Accenture believes in building an inclusive workplace and contributing to a world where equality thrives. Certain terms or expressions can unintentionally harm, perpetuate damaging stereotypes, and insult people. Inclusive language avoids bias, slang terms, and word choices which express derision of groups of people based on race, gender, sexuality, or socioeconomic status. The Accenture North America Technology team created this guidebook to provide Accenture employees with a view into inclusive language and guidance for working to avoid its use\u2014helping to ensure that we communicate with respect, dignity and fairness. How to use this guide? As of 8/2023, Accenture has over 730,000 employees from diverse backgrounds, who perform consulting and delivery work for an equally diverse set of clients and partners. When communicating with your colleagues and representing Accenture, consider the connotation, however unintended, of certain terms in your written and verbal communication. The guidelines are intended to help you recognize non-inclusive words and understand potential meanings that these words might convey. Our goal with these recommendations is not to require you to use specific words, but to ask you to take a moment to consider how your audience may be affected by the language you choose. Inclusive Categories Non-inclusive term Replacement Explanation Race, Ethnicity & National Origin master primary client source leader Using the terms \u201cmaster/slave\u201d in this context inappropriately normalizes and minimizes the very large magnitude that slavery and its effects have had in our history. slave secondary replica follower blacklist deny list block list The term \u201cblacklist\u201d was first used in the early 1600s to describe a list of those who were under suspicion and thus not to be trusted, whereas \u201cwhitelist\u201d referred to those considered acceptable. Accenture does not want to promote the association of \u201cblack\u201d and negative, nor the connotation of \u201cwhite\u201d being the inverse, or positive. whitelist allow list approved list native original core feature Referring to \u201cnative\u201d vs \u201cnon-native\u201d to describe technology platforms carries overtones of minimizing the impact of colonialism on native people, and thus minimizes the negative associations the terminology has in the latter context. non-native non-original non-core feature Gender & Sexuality man-hours work-hours business-hours When people read the words \u2018man\u2019 or \u2018he,\u2019 people often picture males only. Usage of the male terminology subtly suggests that only males can perform certain work or hold certain jobs. Gender-neutral terms include the whole audience, and thus using terms such as \u201cbusiness executive\u201d instead of \u201cbusinessman,\u201d or informally, \u201cfolks\u201d instead of \u201cguys\u201d is preferable because it is inclusive. man-days work-days business-days Ability Status & (Dis)abilities sanity check insanity check confidence check quality check rationality check Using the \u201cHuman Engagement, People First\u2019 approach, putting people - all people - at the center is important. Denoting ability status in the context of inferior or problematic work implies that people with mental illnesses are inferior, wrong, or incorrect. dummy variables indicator variables Violence STONITH, kill, hit conclude cease discontinue Using the \u201cHuman Engagement, People First\u2019 approach, putting people - all people - at the center is important. Denoting ability status in the context of inferior or problematic work implies that people with mental illnesses are inferior, wrong, or incorrect. one throat to choke single point of contact primary contact This guidebook is a living document and will be updated as terminology evolves. We encourage our users to provide feedback on the effectiveness of this document and we welcome additional suggestions. Contact us at Technology_ProjectElevate@accenture.com .","title":"Inclusivity"},{"location":"INCLUSIVITY/#technology-inclusive-language-guidebook","text":"As an organization, Accenture believes in building an inclusive workplace and contributing to a world where equality thrives. Certain terms or expressions can unintentionally harm, perpetuate damaging stereotypes, and insult people. Inclusive language avoids bias, slang terms, and word choices which express derision of groups of people based on race, gender, sexuality, or socioeconomic status. The Accenture North America Technology team created this guidebook to provide Accenture employees with a view into inclusive language and guidance for working to avoid its use\u2014helping to ensure that we communicate with respect, dignity and fairness. How to use this guide? As of 8/2023, Accenture has over 730,000 employees from diverse backgrounds, who perform consulting and delivery work for an equally diverse set of clients and partners. When communicating with your colleagues and representing Accenture, consider the connotation, however unintended, of certain terms in your written and verbal communication. The guidelines are intended to help you recognize non-inclusive words and understand potential meanings that these words might convey. Our goal with these recommendations is not to require you to use specific words, but to ask you to take a moment to consider how your audience may be affected by the language you choose. Inclusive Categories Non-inclusive term Replacement Explanation Race, Ethnicity & National Origin master primary client source leader Using the terms \u201cmaster/slave\u201d in this context inappropriately normalizes and minimizes the very large magnitude that slavery and its effects have had in our history. slave secondary replica follower blacklist deny list block list The term \u201cblacklist\u201d was first used in the early 1600s to describe a list of those who were under suspicion and thus not to be trusted, whereas \u201cwhitelist\u201d referred to those considered acceptable. Accenture does not want to promote the association of \u201cblack\u201d and negative, nor the connotation of \u201cwhite\u201d being the inverse, or positive. whitelist allow list approved list native original core feature Referring to \u201cnative\u201d vs \u201cnon-native\u201d to describe technology platforms carries overtones of minimizing the impact of colonialism on native people, and thus minimizes the negative associations the terminology has in the latter context. non-native non-original non-core feature Gender & Sexuality man-hours work-hours business-hours When people read the words \u2018man\u2019 or \u2018he,\u2019 people often picture males only. Usage of the male terminology subtly suggests that only males can perform certain work or hold certain jobs. Gender-neutral terms include the whole audience, and thus using terms such as \u201cbusiness executive\u201d instead of \u201cbusinessman,\u201d or informally, \u201cfolks\u201d instead of \u201cguys\u201d is preferable because it is inclusive. man-days work-days business-days Ability Status & (Dis)abilities sanity check insanity check confidence check quality check rationality check Using the \u201cHuman Engagement, People First\u2019 approach, putting people - all people - at the center is important. Denoting ability status in the context of inferior or problematic work implies that people with mental illnesses are inferior, wrong, or incorrect. dummy variables indicator variables Violence STONITH, kill, hit conclude cease discontinue Using the \u201cHuman Engagement, People First\u2019 approach, putting people - all people - at the center is important. Denoting ability status in the context of inferior or problematic work implies that people with mental illnesses are inferior, wrong, or incorrect. one throat to choke single point of contact primary contact This guidebook is a living document and will be updated as terminology evolves. We encourage our users to provide feedback on the effectiveness of this document and we welcome additional suggestions. Contact us at Technology_ProjectElevate@accenture.com .","title":"TECHNOLOGY INCLUSIVE LANGUAGE GUIDEBOOK"},{"location":"arch-decisions/DESIGN-NOTES/","text":"Design notes Event choreography by configuration The recommended way to write a composable application is event choreography by configuration using \"Event Script\". This would potentially reduce code size by half. Support sequential synchronous RPC in a non-blocking fashion The foundation library (platform-core) has been integrated with Java 21 virtual thread and Kotlin suspend function features. When a user function makes a RPC call using virtual thread or suspend function, the user function appears to be \"blocked\" so that the code can execute sequentially. Behind the curtain, the function is actually \"suspended\". This makes sequential code with RPC performs as good as reactive code. More importantly, the sequential code represents the intent of the application clearly, thus making code easier to read and maintain. Low level control of function execution strategies You can precisely control how your functions execute, using virtual threads or kernel thread pools to yield the highest performance and throughput. Serialization Gson We are using Gson for its minimalist design. We have customized the serialization behavior to be similar to Jackson and other serializers. i.e. Integer and long values are kept without decimal points. For API functional compatibility with Jackson, we have added the writeValueAsString, writeValueAsBytes and readValue methods. The convertValue method has been consolidated into the readValue method. MsgPack For efficient and serialization performance, we use MsgPack as schemaless binary transport for EventEnvelope that contains event metadata, headers and payload. Handling numbers in a Map The system assumes each key of a Map object to be a text string. If you use integer as a key, it will be converted to a text string. The assumed Map class is Map<String, Object> . Numbers in a value are handled differently in two cases. Serialization of an event envelope : this is done using the MsgPack protocol for binary JSON. The serialization process is optimized for performance and payload size. As a result, a small number that is declared as Long will be serialized as an Integer (Long uses 8 bytes and Integer uses 2 or 4 bytes). Serialization of nested Map in a PoJo : this is done using the GSON library. It is optimized for type matching. Integers are treated as Long numbers. If you want to enforce Integer or Long, please design a PoJo to fit your use case. However, floating point numbers (Float and Double) are rendered without type matching. For untyped numbers, you may use the convenient type conversion methods in the platform-core's Utility class. For examples, util.str2int and util.str2long. Input using Map or PoJo The input to a TypedLambdaFunction should be a Map or PoJo. A map allows you to use flexible data structure and a PoJo would enforce the interface contract. List of PoJo is not supported. First, this design improves the readability of \"input data mapping\" configuration in Event Script. Second, this avoids edge cases in serialization. Keys of Map The system enforces the use of strings as keys in a map for reliable serialization. For example, the configuration management module will convert integers and other types as strings for keys in a configuration. User provided serializers This provides more flexibility for user function to take full control of their PoJo serialization needs. Custom JSON and XML serializers For consistency, we have customized Spring Boot and Servlet serialization and exception handlers. Reactive design Mercury uses the temporary local file system ( /tmp ) as an overflow area for events when the consumer is slower than the producer. This event buffering design means that user application does not have to handle back-pressure logic directly. However, it does not restrict you from implementing your flow-control logic. In-memory event system In Mercury version 1, the Akka actor system is used as the in-memory event bus. Since Mercury version 2, we have migrated from Akka to Eclipse Vertx. In Mercury version 3, we extend the engine to be fully non-blocking with low-level control of application performance and throughput. In Mercury version 3.1, the platform core engine is fully integrated with Java 21 virtual thread. Since Mercury version 4, the event script engine is integrated with the platform-core. This adds event choreography capability directly in the event system. Event script describes a transaction as an event flow configuration that drives composable functions to work together as a single application. A composable function, by design, is self-contained with I/O immutability. Spring Boot 3 The platform-core includes a non-blocking HTTP and websocket server for standalone operation without Spring Boot. The rest-spring-3 library is designed to turn your code to be a Spring Boot application. You may also use the platform-core library with a regular Spring Boot application without the rest-spring-3 library if you prefer. Support of Mono and Flux results A user function may return a regular result that can be a PoJo, HashMap or Java primitive. It can also return a Mono or Flux reactive response object for a future result or a future series of results. Other reactive response objects must be converted to a Mono or Flux object.","title":"Design notes"},{"location":"arch-decisions/DESIGN-NOTES/#design-notes","text":"","title":"Design notes"},{"location":"arch-decisions/DESIGN-NOTES/#event-choreography-by-configuration","text":"The recommended way to write a composable application is event choreography by configuration using \"Event Script\". This would potentially reduce code size by half.","title":"Event choreography by configuration"},{"location":"arch-decisions/DESIGN-NOTES/#support-sequential-synchronous-rpc-in-a-non-blocking-fashion","text":"The foundation library (platform-core) has been integrated with Java 21 virtual thread and Kotlin suspend function features. When a user function makes a RPC call using virtual thread or suspend function, the user function appears to be \"blocked\" so that the code can execute sequentially. Behind the curtain, the function is actually \"suspended\". This makes sequential code with RPC performs as good as reactive code. More importantly, the sequential code represents the intent of the application clearly, thus making code easier to read and maintain.","title":"Support sequential synchronous RPC in a non-blocking fashion"},{"location":"arch-decisions/DESIGN-NOTES/#low-level-control-of-function-execution-strategies","text":"You can precisely control how your functions execute, using virtual threads or kernel thread pools to yield the highest performance and throughput.","title":"Low level control of function execution strategies"},{"location":"arch-decisions/DESIGN-NOTES/#serialization","text":"","title":"Serialization"},{"location":"arch-decisions/DESIGN-NOTES/#gson","text":"We are using Gson for its minimalist design. We have customized the serialization behavior to be similar to Jackson and other serializers. i.e. Integer and long values are kept without decimal points. For API functional compatibility with Jackson, we have added the writeValueAsString, writeValueAsBytes and readValue methods. The convertValue method has been consolidated into the readValue method.","title":"Gson"},{"location":"arch-decisions/DESIGN-NOTES/#msgpack","text":"For efficient and serialization performance, we use MsgPack as schemaless binary transport for EventEnvelope that contains event metadata, headers and payload.","title":"MsgPack"},{"location":"arch-decisions/DESIGN-NOTES/#handling-numbers-in-a-map","text":"The system assumes each key of a Map object to be a text string. If you use integer as a key, it will be converted to a text string. The assumed Map class is Map<String, Object> . Numbers in a value are handled differently in two cases. Serialization of an event envelope : this is done using the MsgPack protocol for binary JSON. The serialization process is optimized for performance and payload size. As a result, a small number that is declared as Long will be serialized as an Integer (Long uses 8 bytes and Integer uses 2 or 4 bytes). Serialization of nested Map in a PoJo : this is done using the GSON library. It is optimized for type matching. Integers are treated as Long numbers. If you want to enforce Integer or Long, please design a PoJo to fit your use case. However, floating point numbers (Float and Double) are rendered without type matching. For untyped numbers, you may use the convenient type conversion methods in the platform-core's Utility class. For examples, util.str2int and util.str2long.","title":"Handling numbers in a Map"},{"location":"arch-decisions/DESIGN-NOTES/#input-using-map-or-pojo","text":"The input to a TypedLambdaFunction should be a Map or PoJo. A map allows you to use flexible data structure and a PoJo would enforce the interface contract. List of PoJo is not supported. First, this design improves the readability of \"input data mapping\" configuration in Event Script. Second, this avoids edge cases in serialization.","title":"Input using Map or PoJo"},{"location":"arch-decisions/DESIGN-NOTES/#keys-of-map","text":"The system enforces the use of strings as keys in a map for reliable serialization. For example, the configuration management module will convert integers and other types as strings for keys in a configuration.","title":"Keys of Map"},{"location":"arch-decisions/DESIGN-NOTES/#user-provided-serializers","text":"This provides more flexibility for user function to take full control of their PoJo serialization needs.","title":"User provided serializers"},{"location":"arch-decisions/DESIGN-NOTES/#custom-json-and-xml-serializers","text":"For consistency, we have customized Spring Boot and Servlet serialization and exception handlers.","title":"Custom JSON and XML serializers"},{"location":"arch-decisions/DESIGN-NOTES/#reactive-design","text":"Mercury uses the temporary local file system ( /tmp ) as an overflow area for events when the consumer is slower than the producer. This event buffering design means that user application does not have to handle back-pressure logic directly. However, it does not restrict you from implementing your flow-control logic.","title":"Reactive design"},{"location":"arch-decisions/DESIGN-NOTES/#in-memory-event-system","text":"In Mercury version 1, the Akka actor system is used as the in-memory event bus. Since Mercury version 2, we have migrated from Akka to Eclipse Vertx. In Mercury version 3, we extend the engine to be fully non-blocking with low-level control of application performance and throughput. In Mercury version 3.1, the platform core engine is fully integrated with Java 21 virtual thread. Since Mercury version 4, the event script engine is integrated with the platform-core. This adds event choreography capability directly in the event system. Event script describes a transaction as an event flow configuration that drives composable functions to work together as a single application. A composable function, by design, is self-contained with I/O immutability.","title":"In-memory event system"},{"location":"arch-decisions/DESIGN-NOTES/#spring-boot-3","text":"The platform-core includes a non-blocking HTTP and websocket server for standalone operation without Spring Boot. The rest-spring-3 library is designed to turn your code to be a Spring Boot application. You may also use the platform-core library with a regular Spring Boot application without the rest-spring-3 library if you prefer.","title":"Spring Boot 3"},{"location":"arch-decisions/DESIGN-NOTES/#support-of-mono-and-flux-results","text":"A user function may return a regular result that can be a PoJo, HashMap or Java primitive. It can also return a Mono or Flux reactive response object for a future result or a future series of results. Other reactive response objects must be converted to a Mono or Flux object.","title":"Support of Mono and Flux results"},{"location":"guides/APPENDIX-I/","text":"Application Configuration The following parameters are used by the system. You can define them in either the application.properties or application.yml file. When you use both application.properties and application.yml, the parameters in application.properties will take precedence. Key Value (example) Required application.name Application name Yes spring.application.name Alias for application name Yes* info.app.version major.minor.build (e.g. 1.0.0) Yes info.app.description Something about your application Yes web.component.scan your own package path or parent path Yes server.port e.g. 8083 Yes* rest.server.port e.g. 8085 Optional websocket.server.port Alias for rest.server.port Optional rest.automation true if you want to enable automation Optional rest.server.ssl-enabled Enable SSL for reactive HTTP server (Default: false) Optional rest.server.ssl.cert X.509 certificate in PEM format. filepath prefix classpath: or file: Optional rest.server.ssl.key Private key in PEM format. filepath prefix classpath: or file: Optional http.client.connection.timeout default 5000 (unit in milliseconds) Optional yaml.rest.automation Config location e.g. classpath:/rest.yaml Optional yaml.event.over.http Config location classpath:/event-over-http.yaml Optional yaml.multicast Config location classpath:/multicast.yaml Optional yaml.journal Config location classpath:/journal.yaml Optional yaml.route.substitution Config location Optional yaml.topic.substitution Config location Optional yaml.cron Config location Optional yaml.flow.automation Config location. e.g. classpath:/flows.yaml EventScript static.html.folder classpath:/public/ Yes spring.web.resources.static-locations (alias for static.html.folder) Yes* spring.mvc.static-path-pattern /** Yes* show.env.variables comma separated list of variable names Optional show.application.properties comma separated list of property names Optional cloud.connector kafka, none, etc. Optional cloud.services e.g. some.interesting.service Optional mime.types Map of file extensions to MIME types (application.yml only) Optional snake.case.serialization Default: true (recommended) Optional trace.http.header comma separated list. Default: \"X-Trace-Id\" Optional hsts.feature Default: true Optional* protect.info.endpoints Default: false true to disable actuators Optional* application.feature.route.substitution Default: false Optional application.feature.topic.substitution Default: false Optional kafka.replication.factor 3 Kafka cloud.client.properties e.g. classpath:/kafka.properties Connector user.cloud.client.properties e.g. classpath:/second-kafka.properties Connector default.app.group.id groupId for an app instance (Default: appGroup) Connector default.monitor.group.id groupId for presence-monitor (Default: monitorGroup) Connector monitor.topic topic for presence-monitor. (Default: service.monitor) Connector app.topic.prefix Default: multiplex (DO NOT change) Connector app.partitions.per.topic Max Kafka partitions per topic (Default: 32) Connector max.virtual.topics Max virtual topics = partitions * topics (Default: 288) Connector max.closed.user.groups Number of closed user groups (Default: 10, range: 3 - 30) Connector closed.user.group Closed user group. (Default: 1) Connector transient.data.store Default: \"/tmp/reactive\" Optional running.in.cloud Default: false (set to true if containerized) Optional deferred.commit.log Default: false (for unit tests only) Optional kernel.thread.pool Default: 100. Not more than 200. Optional modules.autostart list of composable functions to start Optional max.model.array.size max size of a dynamic model variable as index (Default: 1000) Optional stack.trace.transport.size Depth of stack trace in EventEnvelope (Default: 10) Optional worker.instances.no.op Maximum instances for no.op function (Default: 500) Optional worker.instances.resilience.handler Maximum instances for resilience.handler (Default: 500) Optional worker.instances.simple.exception.handler Maximum instances for simple.exception.handler (Default: 250) Optional spring.boot.main Default main class: org.platformlambda.rest.RestServer Spring Boot * - applies to the \"rest-spring\" library only Base configuration files By default, the system assumes the following application configuration files: application.properties application.yml The bootstrap.properties and bootstrap.yml are optional. You can change this behavior by adding the app-config-reader.yml in your project's resources folder. The default configuration is shown as below. resources: - classpath:/bootstrap.properties - classpath:/bootstrap.yml - classpath:/application.properties - classpath:/application.yml profiles: 'classpath:/application-' For compatibility with Spring Boot configuration system, keep the default bootstrap and application configuration files. You may add more configuration files as needed. To load configuration file from the local file system, use \"file:/\" instead of \"classpath:/\". The \"profiles\" parameter defines the file prefix to load profile related configuration files. Note : The order of the filenames defines the loading sequence where subsequent configuration parameters will override prior ones. Configuration management The configuration management system will discover configuration files with the following order of precedence: test/resources src/resources (library-1)/resources (library-2)/resources (library-n)/resources For example, if a config file is not found in the test/resources folder in a unit test, it will search the \"src/resources\" folder. If still not found, it will search the list of libraries for their resources folders. Note : The search order for libraries is non-deterministic using the JVM class search path. Therefore, please use unique filenames for resource files in a library that may be used by an application. The resource file path must be prefixed with the keyword classpath: . This discovery mechanism applies to all types of files including config files. Enabling HTTPS transport Optionally, TLS (SSL) transport can be enabled by setting the parameter rest.server.ssl-enabled to true and adding the rest.server.ssl.cert and rest.server.ssl.key to point to the certificate and private key files. Setting up TLS/SSL transport at application level is not recommended. It is provided as a convenient feature in case you need it for some use cases. For cloud native applications, the best practice is using a TLS/SSL API gateway as a gatekeeper to your applications. Partial support of Spring Active Profiles With JVM runtime parameter \"-Dspring.profiles.active\" or environment variable \"SPRING_PROFILES_ACTIVE\", the AppConfigReader will try to load the additional configuration files. For example, if \"spring.profiles.active=dev\", the system will load \"application-dev.properties\" and \"application-dev.yml\" accordingly. When more than one active profile is needed, you can use a comma separated list of profiles in \"spring.profiles.active\". For Spring Boot compatibility, the filename prefix \"application-\" is fixed. This is defined in the app-config-reader.yml file above. Special handling for PROPERTIES file Since application.properties and application.yml can be used together, the system must enforce keyspace uniqueness because YAML keyspaces are hierarchical. For example, if you have x.y and x.y.z, x.y is the parent of x.y.z. Therefore, you cannot set a value for the parent key since the parent is a key-value container. This hierarchical rule is enforced for PROPERTIES files. If you have x.y=3 and x.y.z=2 in the same PROPERTIES file, x.y will become a parent of x.y.z and its intended value of 3 will be lost. Optional Service The OptionalService annotation may be used with the following class annotations: BeforeApplication MainApplication PreLoad WebSocketService When the OptionalService annotation is available, the system will evaluate the annotation value as a conditional statement where it supports one or more simple condition using a key-value in the application configuration. For examples: OptionalService(\"rest.automation\") - the class will be loaded when rest.automation=true OptionalService(\"!rest.automation\") - the class will be loaded when rest.automation is false or non-exist OptionalService(\"interesting.key=100\") - the system will load the class when \"interesting.key\" is set to 100 in application configuration. To specify more than one condition, use a comma separated list as the value like this: OptionalService(\"web.socket.enabled, rest.automation\") - this tells the system to load the class when either web.socket.enabled or rest.automation is true. Static HTML contents You can place static HTML files (e.g. the HTML bundle for a UI program) in the \"resources/public\" folder or in the local file system using the \"static.html.folder\" parameter. MIME types The system supports a bare minimal list of file extensions to MIME types in the mime-types.yml configuration file in the platform-core's resources folder. If your use case requires additional MIME type mapping, you may define them in the application.yml configuration file under the mime.types section like this: mime.types: pdf: 'application/pdf' doc: 'application/msword' Note : application.properties file cannot be used for the \"mime.types\" section because it only supports text key-values. You may also provide a mime.types section in the mime-types.yml configuration under the resources folder to override the default configuration in the platform-core library. Custom content types If you use custom content types in your application, you may add the following section in the application.yml configuration file. For example, custom.content.types: - 'application/vnd.my.org-v2.0+json -> application/json' - 'application/vnd.my.org-v2.1+xml -> application/xml' In the \"custom.content.types\" section, you can configure a list of content-type mappings. The left-hand-side is the custom content-type and the right-hand-side is a standard content-type. The content-type mapping tells the system to treat the custom content type as if it is the standard content type. In the above example, the HTTP payload with the custom content type \"application/vnd.my.org-v2.0+json\" is treated as a regular JSON content. If you want to put the custom content types in a separate configuration file, please put them in a file named custom-content-types.yml under your application resources folder. HTTP and websocket port assignment If rest.automation=true and rest.server.port or server.port are configured, the system will start a lightweight non-blocking HTTP server. If rest.server.port is not available, it will fall back to server.port . If rest.automation=false and you have a websocket server endpoint annotated as WebsocketService , the system will start a non-blocking Websocket server with a minimalist HTTP server that provides actuator services. If websocket.server.port is not available, it will fall back to rest.server.port or server.port . If you add Spring Boot dependency, Spring Boot will use server.port to start Tomcat or similar HTTP server. The built-in lightweight non-blocking HTTP server and Spring Boot can co-exist when you configure rest.server.port and server.port to use different ports. Note that the websocket.server.port parameter is an alias of rest.server.port . Transient data store The system handles back-pressure automatically by overflowing events from memory to a transient data store. As a cloud native best practice, the folder must be under \"/tmp\". The default is \"/tmp/reactive\". The \"running.in.cloud\" parameter must be set to false when your apps are running in IDE or in your laptop. When running in kubernetes, it can be set to true. Snake or Camel case serializers Serialization and de-serialization of events are performed automatically. If there is a genuine need to programmatically perform serialization, you may use the pre-configured serializer so that the serialization behavior is consistent. You can get an instance of the serializer with SimpleMapper.getInstance().getMapper() . The serializer may perform snake case or camel serialization depending on the parameter snake.case.serialization . If you want to ensure snake case or camel, you can select the serializer like this: SimpleObjectMapper snakeCaseMapper = SimpleMapper.getInstance().getSnakeCaseMapper(); SimpleObjectMapper camelCaseMapper = SimpleMapper.getInstance().getCamelCaseMapper(); The trace.http.header parameter The trace.http.header parameter sets the HTTP header for trace ID. When configured with more than one label, the system will retrieve trace ID from the corresponding HTTP header and propagate it through the transaction that may be served by multiple services. If trace ID is presented in an HTTP request, the system will use the same label to set HTTP response traceId header. X-Trace-Id: a9a4e1ec-1663-4c52-b4c3-7b34b3e33697 or X-Correlation-Id: a9a4e1ec-1663-4c52-b4c3-7b34b3e33697 Kafka specific configuration If you use the kafka-connector (cloud connector) and kafka-presence (presence monitor), you may want to externalize kafka.properties like this: cloud.client.properties=file:/tmp/config/kafka.properties Note that \"classpath\" refers to embedded config file in the \"resources\" folder in your source code and \"file\" refers to an external config file. You want also use the embedded config file as a backup like this: cloud.client.properties=file:/tmp/config/kafka.properties, classpath:/kafka.properties Distributed trace To enable distributed trace logging, please set this in log4j2.xml: <logger name=\"org.platformlambda.core.services.Telemetry\" level=\"INFO\" /> Built-in XML serializer The platform-core includes built-in serializers for JSON and XML in the AsyncHttpClient and Spring RestController. The XML serializer is designed for simple use cases. If you need to handle more complex XML data structure, you can disable the built-in XML serializer by adding the following HTTP request header. X-Raw-Xml=true Chapter-9 Home Appendix-II API Overview Table of Contents Reserved names and headers","title":"Appendix-I"},{"location":"guides/APPENDIX-I/#application-configuration","text":"The following parameters are used by the system. You can define them in either the application.properties or application.yml file. When you use both application.properties and application.yml, the parameters in application.properties will take precedence. Key Value (example) Required application.name Application name Yes spring.application.name Alias for application name Yes* info.app.version major.minor.build (e.g. 1.0.0) Yes info.app.description Something about your application Yes web.component.scan your own package path or parent path Yes server.port e.g. 8083 Yes* rest.server.port e.g. 8085 Optional websocket.server.port Alias for rest.server.port Optional rest.automation true if you want to enable automation Optional rest.server.ssl-enabled Enable SSL for reactive HTTP server (Default: false) Optional rest.server.ssl.cert X.509 certificate in PEM format. filepath prefix classpath: or file: Optional rest.server.ssl.key Private key in PEM format. filepath prefix classpath: or file: Optional http.client.connection.timeout default 5000 (unit in milliseconds) Optional yaml.rest.automation Config location e.g. classpath:/rest.yaml Optional yaml.event.over.http Config location classpath:/event-over-http.yaml Optional yaml.multicast Config location classpath:/multicast.yaml Optional yaml.journal Config location classpath:/journal.yaml Optional yaml.route.substitution Config location Optional yaml.topic.substitution Config location Optional yaml.cron Config location Optional yaml.flow.automation Config location. e.g. classpath:/flows.yaml EventScript static.html.folder classpath:/public/ Yes spring.web.resources.static-locations (alias for static.html.folder) Yes* spring.mvc.static-path-pattern /** Yes* show.env.variables comma separated list of variable names Optional show.application.properties comma separated list of property names Optional cloud.connector kafka, none, etc. Optional cloud.services e.g. some.interesting.service Optional mime.types Map of file extensions to MIME types (application.yml only) Optional snake.case.serialization Default: true (recommended) Optional trace.http.header comma separated list. Default: \"X-Trace-Id\" Optional hsts.feature Default: true Optional* protect.info.endpoints Default: false true to disable actuators Optional* application.feature.route.substitution Default: false Optional application.feature.topic.substitution Default: false Optional kafka.replication.factor 3 Kafka cloud.client.properties e.g. classpath:/kafka.properties Connector user.cloud.client.properties e.g. classpath:/second-kafka.properties Connector default.app.group.id groupId for an app instance (Default: appGroup) Connector default.monitor.group.id groupId for presence-monitor (Default: monitorGroup) Connector monitor.topic topic for presence-monitor. (Default: service.monitor) Connector app.topic.prefix Default: multiplex (DO NOT change) Connector app.partitions.per.topic Max Kafka partitions per topic (Default: 32) Connector max.virtual.topics Max virtual topics = partitions * topics (Default: 288) Connector max.closed.user.groups Number of closed user groups (Default: 10, range: 3 - 30) Connector closed.user.group Closed user group. (Default: 1) Connector transient.data.store Default: \"/tmp/reactive\" Optional running.in.cloud Default: false (set to true if containerized) Optional deferred.commit.log Default: false (for unit tests only) Optional kernel.thread.pool Default: 100. Not more than 200. Optional modules.autostart list of composable functions to start Optional max.model.array.size max size of a dynamic model variable as index (Default: 1000) Optional stack.trace.transport.size Depth of stack trace in EventEnvelope (Default: 10) Optional worker.instances.no.op Maximum instances for no.op function (Default: 500) Optional worker.instances.resilience.handler Maximum instances for resilience.handler (Default: 500) Optional worker.instances.simple.exception.handler Maximum instances for simple.exception.handler (Default: 250) Optional spring.boot.main Default main class: org.platformlambda.rest.RestServer Spring Boot * - applies to the \"rest-spring\" library only","title":"Application Configuration"},{"location":"guides/APPENDIX-I/#base-configuration-files","text":"By default, the system assumes the following application configuration files: application.properties application.yml The bootstrap.properties and bootstrap.yml are optional. You can change this behavior by adding the app-config-reader.yml in your project's resources folder. The default configuration is shown as below. resources: - classpath:/bootstrap.properties - classpath:/bootstrap.yml - classpath:/application.properties - classpath:/application.yml profiles: 'classpath:/application-' For compatibility with Spring Boot configuration system, keep the default bootstrap and application configuration files. You may add more configuration files as needed. To load configuration file from the local file system, use \"file:/\" instead of \"classpath:/\". The \"profiles\" parameter defines the file prefix to load profile related configuration files. Note : The order of the filenames defines the loading sequence where subsequent configuration parameters will override prior ones.","title":"Base configuration files"},{"location":"guides/APPENDIX-I/#configuration-management","text":"The configuration management system will discover configuration files with the following order of precedence: test/resources src/resources (library-1)/resources (library-2)/resources (library-n)/resources For example, if a config file is not found in the test/resources folder in a unit test, it will search the \"src/resources\" folder. If still not found, it will search the list of libraries for their resources folders. Note : The search order for libraries is non-deterministic using the JVM class search path. Therefore, please use unique filenames for resource files in a library that may be used by an application. The resource file path must be prefixed with the keyword classpath: . This discovery mechanism applies to all types of files including config files.","title":"Configuration management"},{"location":"guides/APPENDIX-I/#enabling-https-transport","text":"Optionally, TLS (SSL) transport can be enabled by setting the parameter rest.server.ssl-enabled to true and adding the rest.server.ssl.cert and rest.server.ssl.key to point to the certificate and private key files. Setting up TLS/SSL transport at application level is not recommended. It is provided as a convenient feature in case you need it for some use cases. For cloud native applications, the best practice is using a TLS/SSL API gateway as a gatekeeper to your applications.","title":"Enabling HTTPS transport"},{"location":"guides/APPENDIX-I/#partial-support-of-spring-active-profiles","text":"With JVM runtime parameter \"-Dspring.profiles.active\" or environment variable \"SPRING_PROFILES_ACTIVE\", the AppConfigReader will try to load the additional configuration files. For example, if \"spring.profiles.active=dev\", the system will load \"application-dev.properties\" and \"application-dev.yml\" accordingly. When more than one active profile is needed, you can use a comma separated list of profiles in \"spring.profiles.active\". For Spring Boot compatibility, the filename prefix \"application-\" is fixed. This is defined in the app-config-reader.yml file above.","title":"Partial support of Spring Active Profiles"},{"location":"guides/APPENDIX-I/#special-handling-for-properties-file","text":"Since application.properties and application.yml can be used together, the system must enforce keyspace uniqueness because YAML keyspaces are hierarchical. For example, if you have x.y and x.y.z, x.y is the parent of x.y.z. Therefore, you cannot set a value for the parent key since the parent is a key-value container. This hierarchical rule is enforced for PROPERTIES files. If you have x.y=3 and x.y.z=2 in the same PROPERTIES file, x.y will become a parent of x.y.z and its intended value of 3 will be lost.","title":"Special handling for PROPERTIES file"},{"location":"guides/APPENDIX-I/#optional-service","text":"The OptionalService annotation may be used with the following class annotations: BeforeApplication MainApplication PreLoad WebSocketService When the OptionalService annotation is available, the system will evaluate the annotation value as a conditional statement where it supports one or more simple condition using a key-value in the application configuration. For examples: OptionalService(\"rest.automation\") - the class will be loaded when rest.automation=true OptionalService(\"!rest.automation\") - the class will be loaded when rest.automation is false or non-exist OptionalService(\"interesting.key=100\") - the system will load the class when \"interesting.key\" is set to 100 in application configuration. To specify more than one condition, use a comma separated list as the value like this: OptionalService(\"web.socket.enabled, rest.automation\") - this tells the system to load the class when either web.socket.enabled or rest.automation is true.","title":"Optional Service"},{"location":"guides/APPENDIX-I/#static-html-contents","text":"You can place static HTML files (e.g. the HTML bundle for a UI program) in the \"resources/public\" folder or in the local file system using the \"static.html.folder\" parameter.","title":"Static HTML contents"},{"location":"guides/APPENDIX-I/#mime-types","text":"The system supports a bare minimal list of file extensions to MIME types in the mime-types.yml configuration file in the platform-core's resources folder. If your use case requires additional MIME type mapping, you may define them in the application.yml configuration file under the mime.types section like this: mime.types: pdf: 'application/pdf' doc: 'application/msword' Note : application.properties file cannot be used for the \"mime.types\" section because it only supports text key-values. You may also provide a mime.types section in the mime-types.yml configuration under the resources folder to override the default configuration in the platform-core library.","title":"MIME types"},{"location":"guides/APPENDIX-I/#custom-content-types","text":"If you use custom content types in your application, you may add the following section in the application.yml configuration file. For example, custom.content.types: - 'application/vnd.my.org-v2.0+json -> application/json' - 'application/vnd.my.org-v2.1+xml -> application/xml' In the \"custom.content.types\" section, you can configure a list of content-type mappings. The left-hand-side is the custom content-type and the right-hand-side is a standard content-type. The content-type mapping tells the system to treat the custom content type as if it is the standard content type. In the above example, the HTTP payload with the custom content type \"application/vnd.my.org-v2.0+json\" is treated as a regular JSON content. If you want to put the custom content types in a separate configuration file, please put them in a file named custom-content-types.yml under your application resources folder.","title":"Custom content types"},{"location":"guides/APPENDIX-I/#http-and-websocket-port-assignment","text":"If rest.automation=true and rest.server.port or server.port are configured, the system will start a lightweight non-blocking HTTP server. If rest.server.port is not available, it will fall back to server.port . If rest.automation=false and you have a websocket server endpoint annotated as WebsocketService , the system will start a non-blocking Websocket server with a minimalist HTTP server that provides actuator services. If websocket.server.port is not available, it will fall back to rest.server.port or server.port . If you add Spring Boot dependency, Spring Boot will use server.port to start Tomcat or similar HTTP server. The built-in lightweight non-blocking HTTP server and Spring Boot can co-exist when you configure rest.server.port and server.port to use different ports. Note that the websocket.server.port parameter is an alias of rest.server.port .","title":"HTTP and websocket port assignment"},{"location":"guides/APPENDIX-I/#transient-data-store","text":"The system handles back-pressure automatically by overflowing events from memory to a transient data store. As a cloud native best practice, the folder must be under \"/tmp\". The default is \"/tmp/reactive\". The \"running.in.cloud\" parameter must be set to false when your apps are running in IDE or in your laptop. When running in kubernetes, it can be set to true.","title":"Transient data store"},{"location":"guides/APPENDIX-I/#snake-or-camel-case-serializers","text":"Serialization and de-serialization of events are performed automatically. If there is a genuine need to programmatically perform serialization, you may use the pre-configured serializer so that the serialization behavior is consistent. You can get an instance of the serializer with SimpleMapper.getInstance().getMapper() . The serializer may perform snake case or camel serialization depending on the parameter snake.case.serialization . If you want to ensure snake case or camel, you can select the serializer like this: SimpleObjectMapper snakeCaseMapper = SimpleMapper.getInstance().getSnakeCaseMapper(); SimpleObjectMapper camelCaseMapper = SimpleMapper.getInstance().getCamelCaseMapper();","title":"Snake or Camel case serializers"},{"location":"guides/APPENDIX-I/#the-tracehttpheader-parameter","text":"The trace.http.header parameter sets the HTTP header for trace ID. When configured with more than one label, the system will retrieve trace ID from the corresponding HTTP header and propagate it through the transaction that may be served by multiple services. If trace ID is presented in an HTTP request, the system will use the same label to set HTTP response traceId header. X-Trace-Id: a9a4e1ec-1663-4c52-b4c3-7b34b3e33697 or X-Correlation-Id: a9a4e1ec-1663-4c52-b4c3-7b34b3e33697","title":"The trace.http.header parameter"},{"location":"guides/APPENDIX-I/#kafka-specific-configuration","text":"If you use the kafka-connector (cloud connector) and kafka-presence (presence monitor), you may want to externalize kafka.properties like this: cloud.client.properties=file:/tmp/config/kafka.properties Note that \"classpath\" refers to embedded config file in the \"resources\" folder in your source code and \"file\" refers to an external config file. You want also use the embedded config file as a backup like this: cloud.client.properties=file:/tmp/config/kafka.properties, classpath:/kafka.properties","title":"Kafka specific configuration"},{"location":"guides/APPENDIX-I/#distributed-trace","text":"To enable distributed trace logging, please set this in log4j2.xml: <logger name=\"org.platformlambda.core.services.Telemetry\" level=\"INFO\" />","title":"Distributed trace"},{"location":"guides/APPENDIX-I/#built-in-xml-serializer","text":"The platform-core includes built-in serializers for JSON and XML in the AsyncHttpClient and Spring RestController. The XML serializer is designed for simple use cases. If you need to handle more complex XML data structure, you can disable the built-in XML serializer by adding the following HTTP request header. X-Raw-Xml=true Chapter-9 Home Appendix-II API Overview Table of Contents Reserved names and headers","title":"Built-in XML serializer"},{"location":"guides/APPENDIX-II/","text":"Reserved names The system reserves some route names and headers for routing purpose. System route names The Mercury foundation code is written using the same core API and each function has a route name. The following route names are reserved. Please DO NOT overload them in your application functions to avoid breaking the system unintentionally. Route Purpose Modules actuator.services Actuator endpoint services platform-core info.actuator.service Info actuator endpoint platform-core lib.actuator.service Library actuator endpoint platform-core routes.actuator.service Route info actuator endpoint platform-core env.actuator.service Environment actuator endpoint platform-core health.actuator.service Health actuator endpoint platform-core liveness.actuator.service Liveness actuator endpoint platform-core elastic.queue.cleanup Elastic event buffer clean up task platform-core distributed.tracing Distributed tracing logger platform-core system.ws.server.cleanup Websocket server cleanup service platform-core http.auth.handler REST automation authentication router platform-core event.api.service Event API service platform-core temporary.inbox Event listener for RPC platform-core event.script.manager Instantiate new event flow instance event-script task.executor Perform event choreography event-script http.flow.adapter Built-in flow adapter event-script no.op no-operation placeholder function event-script system.service.registry Distributed routing registry Connector system.service.query Distributed routing query Connector cloud.connector.health Cloud connector health service Connector cloud.health.inbox Event listerner for loopback test Connector cloud.manager Cloud manager service Connector presence.service Presence signal service Connector presence.housekeeper Presence keep-alive service Connector cloud.connector Cloud event emitter Connector init.multiplex.* reserved for event stream startup Connector completion.multiplex.* reserved for event stream clean up Connector async.http.request HTTP request event handler REST automation async.http.response HTTP response event handler REST automation cron.scheduler Cron job scheduler Simple Scheduler init.service.monitor.* reserved for event stream startup Service monitor completion.service.monitor.* reserved for event stream clean up Service monitor Optional user defined functions The following optional route names will be detected by the system for additional user defined features. Route Purpose additional.info User application function to return information about your application status distributed.trace.forwarder Custom function to forward performance metrics to a telemetry system transaction.journal.recorder Custom function to record transaction request-response payloads into an audit DB The additional.info function, if implemented, will be invoked from the \"/info\" endpoint and its response will be merged into the \"/info\" response. For distributed.trace.forwarder and transaction.journal.recorder , please refer to Chapter-5 for details. No-op function The \"no.op\" function is used as a placeholder for building skeleton or simple decision function for an event flow use case. Simple exception handler The \"simple.exception.handler\" is a placeholder for a user defined exception handler for rapid prototyping. For more sophisticated error handling, please use the \"resilience.handler\" or write your own composable function as an exception handler. For more details, refer to Chapter 4 Reserved event header names The following event headers are injected by the system as READ only metadata. They are available from the input \"headers\". However, they are not part of the EventEnvelope. Header Purpose my_route route name of your function my_trace_id trace ID, if any, for the incoming event my_trace_path trace path, if any, for the incoming event You can create a trackable PostOffice using the \"headers\" and the \"instance\" parameters in the input arguments of your function. var po = new PostOffice(headers, instance); Reserved HTTP header names Header Purpose X-Stream-Id Temporal route name for streaming content X-TTL Time to live in milliseconds for a streaming content X-Small-Payload-As-Bytes This header, if set to true, tells system to render stream content as bytes X-Event-Api The system uses this header to indicate that the request is sent over HTTP X-Async This header, if set to true, indicates it is a drop-n-forget request X-Trace-Id This allows the system to propagate trace ID X-Correlation-Id Alternative to X-Trace-Id X-Content-Length If present, it is the expected length of a streaming content X-Raw-Xml This header, if set to true, tells to system to skip XML rendering X-Flow-Id This tells the event manager to select a flow configuration by ID X-App-Instance This header is used by some protected actuator REST endpoints To support traceId that is stored in X-Correlation-Id HTTP header, set this in application.properties. # list of supported traceId headers where the first one is the default label trace.http.header=X-Correlation-Id, X-Trace-Id Transient data store The system uses a temp folder in \"/tmp/composable/java/temp-streams\" to hold temporary data blocks for streaming I/O. Appendix-I Home Appendix-III Application Configuration Table of Contents Actuators, HTTP client and More","title":"Appendix-II"},{"location":"guides/APPENDIX-II/#reserved-names","text":"The system reserves some route names and headers for routing purpose.","title":"Reserved names"},{"location":"guides/APPENDIX-II/#system-route-names","text":"The Mercury foundation code is written using the same core API and each function has a route name. The following route names are reserved. Please DO NOT overload them in your application functions to avoid breaking the system unintentionally. Route Purpose Modules actuator.services Actuator endpoint services platform-core info.actuator.service Info actuator endpoint platform-core lib.actuator.service Library actuator endpoint platform-core routes.actuator.service Route info actuator endpoint platform-core env.actuator.service Environment actuator endpoint platform-core health.actuator.service Health actuator endpoint platform-core liveness.actuator.service Liveness actuator endpoint platform-core elastic.queue.cleanup Elastic event buffer clean up task platform-core distributed.tracing Distributed tracing logger platform-core system.ws.server.cleanup Websocket server cleanup service platform-core http.auth.handler REST automation authentication router platform-core event.api.service Event API service platform-core temporary.inbox Event listener for RPC platform-core event.script.manager Instantiate new event flow instance event-script task.executor Perform event choreography event-script http.flow.adapter Built-in flow adapter event-script no.op no-operation placeholder function event-script system.service.registry Distributed routing registry Connector system.service.query Distributed routing query Connector cloud.connector.health Cloud connector health service Connector cloud.health.inbox Event listerner for loopback test Connector cloud.manager Cloud manager service Connector presence.service Presence signal service Connector presence.housekeeper Presence keep-alive service Connector cloud.connector Cloud event emitter Connector init.multiplex.* reserved for event stream startup Connector completion.multiplex.* reserved for event stream clean up Connector async.http.request HTTP request event handler REST automation async.http.response HTTP response event handler REST automation cron.scheduler Cron job scheduler Simple Scheduler init.service.monitor.* reserved for event stream startup Service monitor completion.service.monitor.* reserved for event stream clean up Service monitor","title":"System route names"},{"location":"guides/APPENDIX-II/#optional-user-defined-functions","text":"The following optional route names will be detected by the system for additional user defined features. Route Purpose additional.info User application function to return information about your application status distributed.trace.forwarder Custom function to forward performance metrics to a telemetry system transaction.journal.recorder Custom function to record transaction request-response payloads into an audit DB The additional.info function, if implemented, will be invoked from the \"/info\" endpoint and its response will be merged into the \"/info\" response. For distributed.trace.forwarder and transaction.journal.recorder , please refer to Chapter-5 for details.","title":"Optional user defined functions"},{"location":"guides/APPENDIX-II/#no-op-function","text":"The \"no.op\" function is used as a placeholder for building skeleton or simple decision function for an event flow use case.","title":"No-op function"},{"location":"guides/APPENDIX-II/#simple-exception-handler","text":"The \"simple.exception.handler\" is a placeholder for a user defined exception handler for rapid prototyping. For more sophisticated error handling, please use the \"resilience.handler\" or write your own composable function as an exception handler. For more details, refer to Chapter 4","title":"Simple exception handler"},{"location":"guides/APPENDIX-II/#reserved-event-header-names","text":"The following event headers are injected by the system as READ only metadata. They are available from the input \"headers\". However, they are not part of the EventEnvelope. Header Purpose my_route route name of your function my_trace_id trace ID, if any, for the incoming event my_trace_path trace path, if any, for the incoming event You can create a trackable PostOffice using the \"headers\" and the \"instance\" parameters in the input arguments of your function. var po = new PostOffice(headers, instance);","title":"Reserved event header names"},{"location":"guides/APPENDIX-II/#reserved-http-header-names","text":"Header Purpose X-Stream-Id Temporal route name for streaming content X-TTL Time to live in milliseconds for a streaming content X-Small-Payload-As-Bytes This header, if set to true, tells system to render stream content as bytes X-Event-Api The system uses this header to indicate that the request is sent over HTTP X-Async This header, if set to true, indicates it is a drop-n-forget request X-Trace-Id This allows the system to propagate trace ID X-Correlation-Id Alternative to X-Trace-Id X-Content-Length If present, it is the expected length of a streaming content X-Raw-Xml This header, if set to true, tells to system to skip XML rendering X-Flow-Id This tells the event manager to select a flow configuration by ID X-App-Instance This header is used by some protected actuator REST endpoints To support traceId that is stored in X-Correlation-Id HTTP header, set this in application.properties. # list of supported traceId headers where the first one is the default label trace.http.header=X-Correlation-Id, X-Trace-Id","title":"Reserved HTTP header names"},{"location":"guides/APPENDIX-II/#transient-data-store","text":"The system uses a temp folder in \"/tmp/composable/java/temp-streams\" to hold temporary data blocks for streaming I/O. Appendix-I Home Appendix-III Application Configuration Table of Contents Actuators, HTTP client and More","title":"Transient data store"},{"location":"guides/APPENDIX-III/","text":"Actuators, HTTP client and More Actuator endpoints The following are actuator endpoints: GET /info GET /info/routes GET /info/lib GET /env GET /health GET /livenessprobe Endpoint Purpose /info Describe the application /info/routes List all private and public function route names /info/lib List libraries packed with this executable /env Show selected environment variables and application parameters /health Application health check endpoint /livenessprobe Check if application is running normally System provided REST endpoints When REST automation is turned on, the following essential REST endpoints will be provided if they are not configured in rest.yaml. The \"POST /api/event\" is used for Event-Over-HTTP protocol and the others are actuator endpoints. To override the default parameters such as timeout, tracing and authentication, you can configure them in rest.yaml. rest: - service: \"event.api.service\" methods: ['POST'] url: \"/api/event\" timeout: 60s tracing: true - service: \"info.actuator.service\" methods: ['GET'] url: \"/info\" timeout: 10s - service: \"lib.actuator.service\" methods: ['GET'] url: \"/info/lib\" timeout: 10s - service: \"routes.actuator.service\" methods: ['GET'] url: \"/info/routes\" timeout: 10s - service: \"health.actuator.service\" methods: ['GET'] url: \"/health\" timeout: 10s - service: \"liveness.actuator.service\" methods: ['GET'] url: \"/livenessprobe\" timeout: 10s - service: \"env.actuator.service\" methods: ['GET'] url: \"/env\" timeout: 10s Note : When using the rest-spring-3 library, the actuator endpoints are always available from the Spring Boot's HTTP port and they cannot be changed. Custom health services You can extend the \"/health\" endpoint by implementing and registering lambda functions to be added to the \"health check\" dependencies. mandatory.health.dependencies=cloud.connector.health, demo.health optional.health.dependencies=other.service.health Your custom health service must respond to the following requests: Info request (type=info) - it should return a map that includes service name and href (protocol, hostname and port) Health check (type=health) - it should return a text string or a Map of the health check. e.g. read/write test result. If health check fails, you can throw AppException with status code and error message. Note : The \"href\" entry in the health service's response should tell the operator about the target URL if the dependency connects to a cloud platform service such as Kafka, Redis, etc. A sample health service is available in the DemoHealth class of the composable-example project as follows: @PreLoad(route=\"demo.health\", instances=5) public class DemoHealth implements LambdaFunction { private static final String TYPE = \"type\"; private static final String INFO = \"info\"; private static final String HEALTH = \"health\"; @Override public Object handleEvent(Map<String, String> headers, Object input, int instance) { /* * The interface contract for a health check service includes both INFO and HEALTH responses. * It must return a Map. */ if (INFO.equals(headers.get(TYPE))) { Map<String, Object> about = new HashMap<>(); about.put(\"service\", \"demo.service\"); about.put(\"href\", \"http://127.0.0.1\"); return about; } if (HEALTH.equals(headers.get(TYPE))) { /* * This is a place-holder for checking a downstream service. * * Please implement your own logic to test if a downstream service is running fine. * If running, just return health status as a String or a Map. * * Otherwise, * throw new AppException(status, message) */ return Map.of(\"demo\", \"I am running fine\"); } throw new IllegalArgumentException(\"type must be info or health\"); } } AsyncHttpClient service The \"async.http.request\" function can be used as a non-blocking HTTP client. To make an HTTP request to an external REST endpoint, you can create an HTTP request object using the AsyncHttpRequest class and make an async RPC call to the \"async.http.request\" function like this: PostOffice po = new PostOffice(headers, instance); AsyncHttpRequest req = new AsyncHttpRequest(); req.setMethod(\"GET\"); req.setHeader(\"accept\", \"application/json\"); req.setUrl(\"/api/hello/world?hello world=abc\"); req.setQueryParameter(\"x1\", \"y\"); List<String> list = new ArrayList<>(); list.add(\"a\"); list.add(\"b\"); req.setQueryParameter(\"x2\", list); req.setTargetHost(\"http://127.0.0.1:8083\"); EventEnvelope request = new EventEnvelope().setTo(\"async.http.request\").setBody(req); EventEnvelope res = po.request(request, 5000).get(); // the response is a Java Future and the result is an EventEnvelope By default, your user function is running in a virtual thread. While the RPC call looks like synchronous, the po.request API will run in non-blocking mode in the same fashion as the \"async/await\" pattern. For reactive programming, you can use the \"asyncRequest\" API like this: PostOffice po = new PostOffice(headers, instance); AsyncHttpRequest req = new AsyncHttpRequest(); req.setMethod(\"GET\"); req.setHeader(\"accept\", \"application/json\"); req.setUrl(\"/api/hello/world?hello world=abc\"); req.setQueryParameter(\"x1\", \"y\"); List<String> list = new ArrayList<>(); list.add(\"a\"); list.add(\"b\"); req.setQueryParameter(\"x2\", list); req.setTargetHost(\"http://127.0.0.1:8083\"); EventEnvelope request = new EventEnvelope().setTo(\"async.http.request\").setBody(req); Future<EventEnvelope> res = po.asyncRequest(request, 5000); res.onSuccess(response -> { // do something with the result }); Send HTTP request body for HTTP PUT, POST and PATCH methods For most cases, you can just set a HashMap into the request body and specify content-type as JSON or XML. The system will perform serialization properly. Example code may look like this: AsyncHttpRequest req = new AsyncHttpRequest(); req.setMethod(\"POST\"); req.setHeader(\"accept\", \"application/json\"); req.setHeader(\"content-type\", \"application/json\"); req.setUrl(\"/api/book\"); req.setTargetHost(\"https://service_provider_host\"); req.setBody(mapOfKeyValues); // where keyValues is a HashMap Send HTTP request body as a stream For larger payload, you may use the streaming method. See sample code below: int len; byte[] buffer = new byte[4096]; FileInputStream in = new FileInputStream(myFile); EventPublisher publisher = new EventPublisher(timeoutInMIlls); while ((len = in.read(buffer, 0, buffer.length)) != -1) { publisher.publish(buffer, 0, len); } // closing the output stream would send a EOF signal to the stream publisher.publishCompletion(); // tell the HTTP client to read the input stream by setting the streamId in the AsyncHttpRequest object req.setStreamRoute(publisher.getStreamId()); Read HTTP response body stream If content length is not given, the response body would arrive as a stream. Your application should check if the HTTP response header \"stream\" exists. Its value is the input \"streamId\". You can process the input stream using the FluxConsumer class like this. Please note that the FluxConsumer is typed. If you do not know the data type for the stream content, use Object for untyped read and test the object type of the incoming messages in the content stream. String streamId = headers.get(\"stream\"); long ttl = 10000; // anticipated time in milliseconds to stream the content FluxConsumer<Map<String, Object>> fc = new FluxConsumer<>(streamId, ttl); fc.consume( data -> { // handle incoming message }, e -> { // handle exception where e is a Throwable }, () -> { // handle stream completion } ); By default, a user function is executed in a virtual thread which effectively is an \"async\" function and the PostOffice \"request\" API operates in the non-blocking \"await\" mode. Rendering a small payload of streaming content If the streaming HTTP response is certain to be a small payload (i.e. Kilobytes), you can optimize the rendering by adding the HTTP request header (X-Small-Payload-As-Bytes=true) in the AsyncHttpRequest object. AsyncHttpRequest req = new AsyncHttpRequest(); req.setMethod(\"GET\"); req.setUrl(\"/api/some/binary/content\"); req.setTargetHost(\"https://service_provider_host\"); req.setHeader(\"X-Small-Payload-As-Bytes\", \"true\"); Note that the AsyncHttpClient will insert a custom HTTP response header \"X-Content-Length\" to show the size of the payload. IMPORTANT: This optimization does not validate the size of the streaming content. Therefore, it is possible for the streaming content to trigger an \"out of memory\" exception. You must make sure the streaming content is small enough before using the \"X-Small-Payload-As-Bytes\" HTTP request header. Content length for HTTP request If you do not set the \"Content-Length\" HTTP header, the AsyncHttpClient will use the \"chunking\" method to send your payload for PUT, POST and PATCH methods. If you set the \"Content-Length\" HTTP header, it must be a correct size of the payload when rendered as a byte array. Setting an incorrect value would produce suboptimal outcome. For file upload using the streaming method, please refer to the section of \"Send HTTP request body as a stream\" above. Note that the \"Content-Length\" HTTP header will be ignored by the AsyncHttpClient so that the system can compute the correct value. Multipart file upload The \"multipart/form-data\" file upload protocol is supported for client side and server side. You can upload single file or multiple files in a single HTTP request. Client side is handled by the AsyncHttpClient composable function with the route name \"async.http.request\". Server side is processed by the handleMultiPartContent method in the HttpRouter class. Please refer to the FileUploadDemo class in the \"lambda-example\" for a REST endpoint reference implementation and the MultiPartFileUploadTest class in the unit tests for the client side handling. Using AsyncHttpClient by configuration The \"async.http.request\" service can be used as a task in a flow. The following flow configuration example illustrates using it as a task. flow: id: 'http-client-by-config' description: 'Demonstrate use of the Async HTTP client using configuration means' ttl: 10s first.task: 'http.client' tasks: - name: 'http.client' input: - 'text(/api/echo/test) -> url' - 'text(PUT) -> method' - 'text(http://127.0.0.1:${server.port}) -> host' - 'input.body -> body' - 'text(world) -> parameters.query.hello' - 'text(application/json) -> headers.content-type' - 'text(application/json) -> headers.accept' process: 'async.http.request' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Return result' execution: end The interface contract for the AsyncHttpClient is the AsyncHttpRequest object. The following table lists the parameters where parameters.query, body and cookies are optional. Parameter Usage Example method HTTP method GET host Protocol and domain name (or IP address) https://demo.platformlambda.org url URI path /api/hello/world parameters.query Query parameter key-value parameters.query.hello=world headers HTTP request headers headers.content-type=application/json body HTTP request body for PUT, POST and PATCH {\"hello\": \"world\"} cookies Cookie key-value cookies.session-id=12345 Starting a flow programmatically To start an \"event\" flow from a unit test, you may use the helper class \"FlowExecutor\" under the \"Event Script\" module. Examples of some APIs are as follows: // launch a flow asychronously public void launch(String originator, String flowId, Map<String, Object> dataset, String correlationId); // launch a flow asychronously with tracing public void launch(String originator, String traceId, String tracePath, String flowId, Map<String, Object> dataset, String correlationId); // launch a flow asychronously and tracing public void launch(PostOffice po, String flowId, Map<String, Object> dataset, String correlationId); // launch a flow with callback and tracing public void launch(PostOffice po, String flowId, Map<String, Object> dataset, String replyTo, String correlationId); // launch a flow and expect a future response public Future<EventEnvelope> request(PostOffice po, String flowId, Map<String, Object> dataset, String correlationId, long timeout); The following unit test emulates a HTTP request to the flow named \"header-test\". @Test public void internalFlowTest() throws ExecutionException, InterruptedException { final long TIMEOUT = 8000; String traceId = Utility.getInstance().getUuid(); String cid = Utility.getInstance().getUuid(); PostOffice po = new PostOffice(\"unit.test\", traceId, \"INTERNAL /flow/test\"); String flowId = \"header-test\"; Map<String, Object> headers = new HashMap<>(); Map<String, Object> dataset = new HashMap<>(); dataset.put(\"header\", headers); dataset.put(\"body\", Map.of(\"hello\", \"world\")); headers.put(\"user-agent\", \"internal-flow\"); headers.put(\"accept\", \"application/json\"); headers.put(\"x-flow-id\", flowId); FlowExecutor flowExecutor = FlowExecutor.getInstance(); EventEnvelope result = flowExecutor.request(po, flowId, dataset, cid, TIMEOUT).get(); assertInstanceOf(Map.class, result.getBody()); Map<String, Object> body = (Map<String, Object>) result.getBody(); // verify that input headers are mapped to the function's input body assertEquals(\"header-test\", body.get(\"x-flow-id\")); assertEquals(\"internal-flow\", body.get(\"user-agent\")); assertEquals(\"application/json\", body.get(\"accept\")); } The dataset must contain at least the \"body\" key-value so that input data mapping is possible in a flow. For the built-in HTTP flow adapter, the dataset would contain the following: // convert HTTP context to flow \"input\" dataset Map<String, Object> dataset = new HashMap<>(); dataset.put(\"header\", request.getHeaders()); dataset.put(\"body\", request.getBody()); dataset.put(\"cookie\", request.getCookies()); dataset.put(\"path_parameter\", request.getPathParameters()); dataset.put(\"method\", request.getMethod()); dataset.put(\"uri\", request.getUrl()); dataset.put(\"query\", request.getQueryParameters()); dataset.put(\"stream\", request.getStreamRoute()); dataset.put(\"ip\", request.getRemoteIp()); dataset.put(\"filename\", request.getFileName()); dataset.put(\"session\", request.getSessionInfo()); If you write your own Kafka flow adapter, the dataset should contain headers and body mapped with a Kafka event. For other flow adapters, you may use different set of key-values. Writing your own Flow Adapters Please browse the csv-flow-adapter and csv-flow-demo subprojects in the examples project folder for hints in writing your own flow adapters to address your specific requirements. Application log format The system supports 3 types of log formats. You can set \"log.format\" parameter in application.properties to change the log format or override it at runtime using the Java \"-D\" argument. e.g. java -Dlog.format=json -jar myapp.jar Format Description text this is the default log format json application log will be printed in JSON format with line feed and indentation compact JSON format without line feed and indentation text and json formats are for human readers and compact format is designed for log analytics system. To leverge the advantage of json log format, your application may log JSON using the parameter formatter {} with a single Map parameter like this: var message = new HashMap<>(); message.put(\"id\", id); message.put(\"status\", \"completed\"); message.put(\"notes\", \"Just a demo\"); log.info(\"{}\", message); Customize log4j configuration The log4j configuration templates are available in the main \"resources\" folder of the platform-core. If you want to adjust the \"loggers\" section in log4j, please copy the required XML files to the main \"resources\" folder in your application. File Description log4j2.xml this is the default configuration file for logging in text format log4j2-json.xml configuration file for logging in JSON format log4j2-compact.xml configuration file for logging in COMPACT format The default log4j2.xml configuration file looks like this: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Configuration status=\"INFO\"> <Appenders> <Console name=\"Console\" target=\"SYSTEM_OUT\"> <PatternLayout pattern=\"%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %logger:%line - %msg%n\" /> </Console> </Appenders> <Loggers> <Root level=\"${env:LOG_LEVEL:-INFO}\" additivity=\"false\"> <AppenderRef ref=\"Console\" /> </Root> <!-- Enable INFO logging for Telemetry --> <logger name=\"org.platformlambda.core.services.Telemetry\" level=\"INFO\" /> </Loggers> </Configuration> In the \"loggers\" section, you can expand the class list to tell log4j which classes to log and at what level. Please note that the \"AppenderRef\" must point to the same \"Appenders\" in the XML file. Handling numbers in a Map The system assumes each key of a Map object to be a text string. If you use integer as a key, it will be converted to a text string. The assumed Map class is Map<String, Object> . Numbers in a value are handled differently in two cases. Serialization of an event envelope : this is done using the MsgPack protocol for binary JSON. The serialization process is optimized for performance and payload size. As a result, a small number that is declared as Long will be serialized as an Integer (Long uses 8 bytes and Integer uses 2 or 4 bytes). Serialization of nested Map in a PoJo : this is done using the GSON library. It is optimized for type matching. Integers are treated as Long numbers. If you want to enforce Integer or Long, please design a PoJo to fit your use case. However, floating point numbers (Float and Double) are rendered without type matching. For untyped numbers, you may use the convenient type conversion methods in the platform-core's Utility class. For examples, util.str2int and util.str2long. Appendix-II Home Reserved names and headers Table of Contents","title":"Appendix-III"},{"location":"guides/APPENDIX-III/#actuators-http-client-and-more","text":"","title":"Actuators, HTTP client and More"},{"location":"guides/APPENDIX-III/#actuator-endpoints","text":"The following are actuator endpoints: GET /info GET /info/routes GET /info/lib GET /env GET /health GET /livenessprobe Endpoint Purpose /info Describe the application /info/routes List all private and public function route names /info/lib List libraries packed with this executable /env Show selected environment variables and application parameters /health Application health check endpoint /livenessprobe Check if application is running normally","title":"Actuator endpoints"},{"location":"guides/APPENDIX-III/#system-provided-rest-endpoints","text":"When REST automation is turned on, the following essential REST endpoints will be provided if they are not configured in rest.yaml. The \"POST /api/event\" is used for Event-Over-HTTP protocol and the others are actuator endpoints. To override the default parameters such as timeout, tracing and authentication, you can configure them in rest.yaml. rest: - service: \"event.api.service\" methods: ['POST'] url: \"/api/event\" timeout: 60s tracing: true - service: \"info.actuator.service\" methods: ['GET'] url: \"/info\" timeout: 10s - service: \"lib.actuator.service\" methods: ['GET'] url: \"/info/lib\" timeout: 10s - service: \"routes.actuator.service\" methods: ['GET'] url: \"/info/routes\" timeout: 10s - service: \"health.actuator.service\" methods: ['GET'] url: \"/health\" timeout: 10s - service: \"liveness.actuator.service\" methods: ['GET'] url: \"/livenessprobe\" timeout: 10s - service: \"env.actuator.service\" methods: ['GET'] url: \"/env\" timeout: 10s Note : When using the rest-spring-3 library, the actuator endpoints are always available from the Spring Boot's HTTP port and they cannot be changed.","title":"System provided REST endpoints"},{"location":"guides/APPENDIX-III/#custom-health-services","text":"You can extend the \"/health\" endpoint by implementing and registering lambda functions to be added to the \"health check\" dependencies. mandatory.health.dependencies=cloud.connector.health, demo.health optional.health.dependencies=other.service.health Your custom health service must respond to the following requests: Info request (type=info) - it should return a map that includes service name and href (protocol, hostname and port) Health check (type=health) - it should return a text string or a Map of the health check. e.g. read/write test result. If health check fails, you can throw AppException with status code and error message. Note : The \"href\" entry in the health service's response should tell the operator about the target URL if the dependency connects to a cloud platform service such as Kafka, Redis, etc. A sample health service is available in the DemoHealth class of the composable-example project as follows: @PreLoad(route=\"demo.health\", instances=5) public class DemoHealth implements LambdaFunction { private static final String TYPE = \"type\"; private static final String INFO = \"info\"; private static final String HEALTH = \"health\"; @Override public Object handleEvent(Map<String, String> headers, Object input, int instance) { /* * The interface contract for a health check service includes both INFO and HEALTH responses. * It must return a Map. */ if (INFO.equals(headers.get(TYPE))) { Map<String, Object> about = new HashMap<>(); about.put(\"service\", \"demo.service\"); about.put(\"href\", \"http://127.0.0.1\"); return about; } if (HEALTH.equals(headers.get(TYPE))) { /* * This is a place-holder for checking a downstream service. * * Please implement your own logic to test if a downstream service is running fine. * If running, just return health status as a String or a Map. * * Otherwise, * throw new AppException(status, message) */ return Map.of(\"demo\", \"I am running fine\"); } throw new IllegalArgumentException(\"type must be info or health\"); } }","title":"Custom health services"},{"location":"guides/APPENDIX-III/#asynchttpclient-service","text":"The \"async.http.request\" function can be used as a non-blocking HTTP client. To make an HTTP request to an external REST endpoint, you can create an HTTP request object using the AsyncHttpRequest class and make an async RPC call to the \"async.http.request\" function like this: PostOffice po = new PostOffice(headers, instance); AsyncHttpRequest req = new AsyncHttpRequest(); req.setMethod(\"GET\"); req.setHeader(\"accept\", \"application/json\"); req.setUrl(\"/api/hello/world?hello world=abc\"); req.setQueryParameter(\"x1\", \"y\"); List<String> list = new ArrayList<>(); list.add(\"a\"); list.add(\"b\"); req.setQueryParameter(\"x2\", list); req.setTargetHost(\"http://127.0.0.1:8083\"); EventEnvelope request = new EventEnvelope().setTo(\"async.http.request\").setBody(req); EventEnvelope res = po.request(request, 5000).get(); // the response is a Java Future and the result is an EventEnvelope By default, your user function is running in a virtual thread. While the RPC call looks like synchronous, the po.request API will run in non-blocking mode in the same fashion as the \"async/await\" pattern. For reactive programming, you can use the \"asyncRequest\" API like this: PostOffice po = new PostOffice(headers, instance); AsyncHttpRequest req = new AsyncHttpRequest(); req.setMethod(\"GET\"); req.setHeader(\"accept\", \"application/json\"); req.setUrl(\"/api/hello/world?hello world=abc\"); req.setQueryParameter(\"x1\", \"y\"); List<String> list = new ArrayList<>(); list.add(\"a\"); list.add(\"b\"); req.setQueryParameter(\"x2\", list); req.setTargetHost(\"http://127.0.0.1:8083\"); EventEnvelope request = new EventEnvelope().setTo(\"async.http.request\").setBody(req); Future<EventEnvelope> res = po.asyncRequest(request, 5000); res.onSuccess(response -> { // do something with the result });","title":"AsyncHttpClient service"},{"location":"guides/APPENDIX-III/#send-http-request-body-for-http-put-post-and-patch-methods","text":"For most cases, you can just set a HashMap into the request body and specify content-type as JSON or XML. The system will perform serialization properly. Example code may look like this: AsyncHttpRequest req = new AsyncHttpRequest(); req.setMethod(\"POST\"); req.setHeader(\"accept\", \"application/json\"); req.setHeader(\"content-type\", \"application/json\"); req.setUrl(\"/api/book\"); req.setTargetHost(\"https://service_provider_host\"); req.setBody(mapOfKeyValues); // where keyValues is a HashMap","title":"Send HTTP request body for HTTP PUT, POST and PATCH methods"},{"location":"guides/APPENDIX-III/#send-http-request-body-as-a-stream","text":"For larger payload, you may use the streaming method. See sample code below: int len; byte[] buffer = new byte[4096]; FileInputStream in = new FileInputStream(myFile); EventPublisher publisher = new EventPublisher(timeoutInMIlls); while ((len = in.read(buffer, 0, buffer.length)) != -1) { publisher.publish(buffer, 0, len); } // closing the output stream would send a EOF signal to the stream publisher.publishCompletion(); // tell the HTTP client to read the input stream by setting the streamId in the AsyncHttpRequest object req.setStreamRoute(publisher.getStreamId());","title":"Send HTTP request body as a stream"},{"location":"guides/APPENDIX-III/#read-http-response-body-stream","text":"If content length is not given, the response body would arrive as a stream. Your application should check if the HTTP response header \"stream\" exists. Its value is the input \"streamId\". You can process the input stream using the FluxConsumer class like this. Please note that the FluxConsumer is typed. If you do not know the data type for the stream content, use Object for untyped read and test the object type of the incoming messages in the content stream. String streamId = headers.get(\"stream\"); long ttl = 10000; // anticipated time in milliseconds to stream the content FluxConsumer<Map<String, Object>> fc = new FluxConsumer<>(streamId, ttl); fc.consume( data -> { // handle incoming message }, e -> { // handle exception where e is a Throwable }, () -> { // handle stream completion } ); By default, a user function is executed in a virtual thread which effectively is an \"async\" function and the PostOffice \"request\" API operates in the non-blocking \"await\" mode.","title":"Read HTTP response body stream"},{"location":"guides/APPENDIX-III/#rendering-a-small-payload-of-streaming-content","text":"If the streaming HTTP response is certain to be a small payload (i.e. Kilobytes), you can optimize the rendering by adding the HTTP request header (X-Small-Payload-As-Bytes=true) in the AsyncHttpRequest object. AsyncHttpRequest req = new AsyncHttpRequest(); req.setMethod(\"GET\"); req.setUrl(\"/api/some/binary/content\"); req.setTargetHost(\"https://service_provider_host\"); req.setHeader(\"X-Small-Payload-As-Bytes\", \"true\"); Note that the AsyncHttpClient will insert a custom HTTP response header \"X-Content-Length\" to show the size of the payload. IMPORTANT: This optimization does not validate the size of the streaming content. Therefore, it is possible for the streaming content to trigger an \"out of memory\" exception. You must make sure the streaming content is small enough before using the \"X-Small-Payload-As-Bytes\" HTTP request header.","title":"Rendering a small payload of streaming content"},{"location":"guides/APPENDIX-III/#content-length-for-http-request","text":"If you do not set the \"Content-Length\" HTTP header, the AsyncHttpClient will use the \"chunking\" method to send your payload for PUT, POST and PATCH methods. If you set the \"Content-Length\" HTTP header, it must be a correct size of the payload when rendered as a byte array. Setting an incorrect value would produce suboptimal outcome. For file upload using the streaming method, please refer to the section of \"Send HTTP request body as a stream\" above. Note that the \"Content-Length\" HTTP header will be ignored by the AsyncHttpClient so that the system can compute the correct value.","title":"Content length for HTTP request"},{"location":"guides/APPENDIX-III/#multipart-file-upload","text":"The \"multipart/form-data\" file upload protocol is supported for client side and server side. You can upload single file or multiple files in a single HTTP request. Client side is handled by the AsyncHttpClient composable function with the route name \"async.http.request\". Server side is processed by the handleMultiPartContent method in the HttpRouter class. Please refer to the FileUploadDemo class in the \"lambda-example\" for a REST endpoint reference implementation and the MultiPartFileUploadTest class in the unit tests for the client side handling.","title":"Multipart file upload"},{"location":"guides/APPENDIX-III/#using-asynchttpclient-by-configuration","text":"The \"async.http.request\" service can be used as a task in a flow. The following flow configuration example illustrates using it as a task. flow: id: 'http-client-by-config' description: 'Demonstrate use of the Async HTTP client using configuration means' ttl: 10s first.task: 'http.client' tasks: - name: 'http.client' input: - 'text(/api/echo/test) -> url' - 'text(PUT) -> method' - 'text(http://127.0.0.1:${server.port}) -> host' - 'input.body -> body' - 'text(world) -> parameters.query.hello' - 'text(application/json) -> headers.content-type' - 'text(application/json) -> headers.accept' process: 'async.http.request' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Return result' execution: end The interface contract for the AsyncHttpClient is the AsyncHttpRequest object. The following table lists the parameters where parameters.query, body and cookies are optional. Parameter Usage Example method HTTP method GET host Protocol and domain name (or IP address) https://demo.platformlambda.org url URI path /api/hello/world parameters.query Query parameter key-value parameters.query.hello=world headers HTTP request headers headers.content-type=application/json body HTTP request body for PUT, POST and PATCH {\"hello\": \"world\"} cookies Cookie key-value cookies.session-id=12345","title":"Using AsyncHttpClient by configuration"},{"location":"guides/APPENDIX-III/#starting-a-flow-programmatically","text":"To start an \"event\" flow from a unit test, you may use the helper class \"FlowExecutor\" under the \"Event Script\" module. Examples of some APIs are as follows: // launch a flow asychronously public void launch(String originator, String flowId, Map<String, Object> dataset, String correlationId); // launch a flow asychronously with tracing public void launch(String originator, String traceId, String tracePath, String flowId, Map<String, Object> dataset, String correlationId); // launch a flow asychronously and tracing public void launch(PostOffice po, String flowId, Map<String, Object> dataset, String correlationId); // launch a flow with callback and tracing public void launch(PostOffice po, String flowId, Map<String, Object> dataset, String replyTo, String correlationId); // launch a flow and expect a future response public Future<EventEnvelope> request(PostOffice po, String flowId, Map<String, Object> dataset, String correlationId, long timeout); The following unit test emulates a HTTP request to the flow named \"header-test\". @Test public void internalFlowTest() throws ExecutionException, InterruptedException { final long TIMEOUT = 8000; String traceId = Utility.getInstance().getUuid(); String cid = Utility.getInstance().getUuid(); PostOffice po = new PostOffice(\"unit.test\", traceId, \"INTERNAL /flow/test\"); String flowId = \"header-test\"; Map<String, Object> headers = new HashMap<>(); Map<String, Object> dataset = new HashMap<>(); dataset.put(\"header\", headers); dataset.put(\"body\", Map.of(\"hello\", \"world\")); headers.put(\"user-agent\", \"internal-flow\"); headers.put(\"accept\", \"application/json\"); headers.put(\"x-flow-id\", flowId); FlowExecutor flowExecutor = FlowExecutor.getInstance(); EventEnvelope result = flowExecutor.request(po, flowId, dataset, cid, TIMEOUT).get(); assertInstanceOf(Map.class, result.getBody()); Map<String, Object> body = (Map<String, Object>) result.getBody(); // verify that input headers are mapped to the function's input body assertEquals(\"header-test\", body.get(\"x-flow-id\")); assertEquals(\"internal-flow\", body.get(\"user-agent\")); assertEquals(\"application/json\", body.get(\"accept\")); } The dataset must contain at least the \"body\" key-value so that input data mapping is possible in a flow. For the built-in HTTP flow adapter, the dataset would contain the following: // convert HTTP context to flow \"input\" dataset Map<String, Object> dataset = new HashMap<>(); dataset.put(\"header\", request.getHeaders()); dataset.put(\"body\", request.getBody()); dataset.put(\"cookie\", request.getCookies()); dataset.put(\"path_parameter\", request.getPathParameters()); dataset.put(\"method\", request.getMethod()); dataset.put(\"uri\", request.getUrl()); dataset.put(\"query\", request.getQueryParameters()); dataset.put(\"stream\", request.getStreamRoute()); dataset.put(\"ip\", request.getRemoteIp()); dataset.put(\"filename\", request.getFileName()); dataset.put(\"session\", request.getSessionInfo()); If you write your own Kafka flow adapter, the dataset should contain headers and body mapped with a Kafka event. For other flow adapters, you may use different set of key-values.","title":"Starting a flow programmatically"},{"location":"guides/APPENDIX-III/#writing-your-own-flow-adapters","text":"Please browse the csv-flow-adapter and csv-flow-demo subprojects in the examples project folder for hints in writing your own flow adapters to address your specific requirements.","title":"Writing your own Flow Adapters"},{"location":"guides/APPENDIX-III/#application-log-format","text":"The system supports 3 types of log formats. You can set \"log.format\" parameter in application.properties to change the log format or override it at runtime using the Java \"-D\" argument. e.g. java -Dlog.format=json -jar myapp.jar Format Description text this is the default log format json application log will be printed in JSON format with line feed and indentation compact JSON format without line feed and indentation text and json formats are for human readers and compact format is designed for log analytics system. To leverge the advantage of json log format, your application may log JSON using the parameter formatter {} with a single Map parameter like this: var message = new HashMap<>(); message.put(\"id\", id); message.put(\"status\", \"completed\"); message.put(\"notes\", \"Just a demo\"); log.info(\"{}\", message);","title":"Application log format"},{"location":"guides/APPENDIX-III/#customize-log4j-configuration","text":"The log4j configuration templates are available in the main \"resources\" folder of the platform-core. If you want to adjust the \"loggers\" section in log4j, please copy the required XML files to the main \"resources\" folder in your application. File Description log4j2.xml this is the default configuration file for logging in text format log4j2-json.xml configuration file for logging in JSON format log4j2-compact.xml configuration file for logging in COMPACT format The default log4j2.xml configuration file looks like this: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Configuration status=\"INFO\"> <Appenders> <Console name=\"Console\" target=\"SYSTEM_OUT\"> <PatternLayout pattern=\"%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %logger:%line - %msg%n\" /> </Console> </Appenders> <Loggers> <Root level=\"${env:LOG_LEVEL:-INFO}\" additivity=\"false\"> <AppenderRef ref=\"Console\" /> </Root> <!-- Enable INFO logging for Telemetry --> <logger name=\"org.platformlambda.core.services.Telemetry\" level=\"INFO\" /> </Loggers> </Configuration> In the \"loggers\" section, you can expand the class list to tell log4j which classes to log and at what level. Please note that the \"AppenderRef\" must point to the same \"Appenders\" in the XML file.","title":"Customize log4j configuration"},{"location":"guides/APPENDIX-III/#handling-numbers-in-a-map","text":"The system assumes each key of a Map object to be a text string. If you use integer as a key, it will be converted to a text string. The assumed Map class is Map<String, Object> . Numbers in a value are handled differently in two cases. Serialization of an event envelope : this is done using the MsgPack protocol for binary JSON. The serialization process is optimized for performance and payload size. As a result, a small number that is declared as Long will be serialized as an Integer (Long uses 8 bytes and Integer uses 2 or 4 bytes). Serialization of nested Map in a PoJo : this is done using the GSON library. It is optimized for type matching. Integers are treated as Long numbers. If you want to enforce Integer or Long, please design a PoJo to fit your use case. However, floating point numbers (Float and Double) are rendered without type matching. For untyped numbers, you may use the convenient type conversion methods in the platform-core's Utility class. For examples, util.str2int and util.str2long. Appendix-II Home Reserved names and headers Table of Contents","title":"Handling numbers in a Map"},{"location":"guides/CHAPTER-1/","text":"Introduction Mercury Composable is a software development toolkit for writing composable applications. Composable application means that an application is assembled from modular software components or functions that are self-contained and pluggable. You can mix-n-match functions to form new applications. You can retire outdated functions without adverse side effect to a production system. Multiple versions of a function can exist, and you can decide how to route user requests to different versions of a function. Applications would be easier to design, develop, maintain, deploy, and scale. Composable application architecture Figure 1 - Composable application architecture As shown in Figure 1, a composable application contains the following: Flow adapters : Each flow adapter listens to requests for onwards delivery to an event manager. Event Manager : it sends events to a set of user functions for them to work together as an application. User functions : these are self-contained functions with clear input and output that are immutable. HTTP flow adapter A non-blocking HTTP flow adapter is built-in. For other external interface types, you can implement your own flow adapters. e.g. Adapters for MQ, Kafka, Serverless, File based staging area, etc. The standard HTTP flow adapter leverages the underlying REST automation system to serve user facing REST API endpoints. For example, a hypothetical \"get profile\" endpoint is created like this in the \"rest.yaml\" configuration file: - service: \"http.flow.adapter\" methods: ['GET'] url: \"/api/profile/{profile_id}\" flow: 'get-profile' timeout: 10s cors: cors_1 headers: header_1 tracing: true In this REST configuration entry, the system creates a REST API endpoint for \"GET /api/profile/{profile_id}\". When a request arrives at this endpoint, the HTTP request will be converted to an incoming event by the flow adapter that routes the event to the \"event manager\" to execute a new instance of the \"get-profile\" flow. Flow configuration example The event manager is driven by configuration instead of code. A hypothetical \"get profile\" flow is defined in a YAML file like this: flow: id: 'get-profile' description: 'Get a user profile using profile ID' ttl: 10s exception: 'v1.hello.exception' first.task: 'v1.get.profile' tasks: - input: - 'input.path_parameter.profile_id -> header.profile_id' process: 'v1.get.profile' output: - 'result -> model.profile' description: 'Retrieve user profile from database using profile_id' execution: sequential next: - 'v1.decrypt.fields' - input: - 'model.profile -> dataset' - 'text(telephone, address) -> protected_fields' process: 'v1.decrypt.fields' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Decrypt fields' execution: end - input: - 'error.code -> status' - 'error.message -> message' - 'error.stack -> stack' process: 'v1.hello.exception' output: - 'result.status -> output.status' - 'result -> output.body' description: 'Just a demo exception handler' execution: end Note that the flow configuration is referring user functions by their \"route\" names. It is because all user functions are self-contained with clearly defined input and output and the event manager would set their inputs and collect their outputs accordingly. Note that you can map selected key-values or the whole event as a business object and this decoupling promotes highly reusable user functional software. The event manager will create a \"state machine\" to manage each transaction flow because all user functions are stateless. The \"state machine\" is referenced using the namespace \"model\". Assigning a route name to a user function You can assign a route name to a Java class using the PreLoad annotation like this: @PreLoad(route=\"v1.get.profile\", instances=100) public class GetProfile implements TypedLambdaFunction<Map<String, Object>, Profile> { @Override public Profile handleEvent(Map<String, String> headers, Map<String, Object> input, int instance) { // your business logic here return result; } } Inside the \"handleEvent\" method, you can write regular Java code using your preferred coding style and framework. You can define input/output as Map or PoJo. Building the Mercury libraries from source Mercury Composable leverages the best of Java 21 virtual threading technology. Therefore, you would need to install Java JDK version 21 or higher. You also need maven version 3.9.7 or higher to build the libraries. Assuming you clone the repository into the \"sandbox\" directory, you may build the libraries like this. cd sandbox/mercury-composable mvn clean install The compiled libraries will be saved to your local \".m2\" maven repository. For production, you may publish the Mercury Composable libraries into your enterprise artifactory. We use \"maven\" build scripts. If your organization uses other build tools such as gradle, please convert them accordingly. Things to avoid with Java 21 By default, user functions are executed using Java 21 virtual threading technology. However, for performance reason, there are two things that you MUST avoid: Synchronized keyword : This will block the event loop in the Java VM, meaning that your whole application is blocked when the synchronized block executes. ThreadLocal : Java 21 virtual thread is designed to be very light weight. When you use ThreadLocal variables, the \"virtual thread\" becomes heavy weighted and the Garbage Collector may have difficulties catching up. Since Mercury provides thread management abstraction, there is no need to use the Synchronized keyword and ThreadLocal variables. The built-in \"state machine\" is a better place to keep your runtime variables for each transaction. Interestingly, the \"Thread\" and \"Future\" APIs are safe to use in a virtual thread. If you are putting legacy code inside a new user function and the legacy code runs in blocking mode, you can annotate the user function with the \"KernelThreadRunner\" class. This tells the system to turn on compatibility mode to support the blocking code. The kernel thread would isolate the blocking code from the rest of the application. However, kernel threads are limited resources. While virtual threads can support tens of thousands of cooperative concurrent execution, kernel threads are limited to 250, depending on the number of CPU cores that the target machine has. Composable application example Let's take a test drive of a composable application example in the \"examples/composable-example\" subproject. You can use your favorite IDE to run the example or execute it from a terminal using command line. To run it from the command line, you may do this: cd sandbox/mercury-composable/examples/composable-example java -jar target/composable-example-4.2.0.jar If you run the application from the IDE, you may execute the \"main\" method in the MainApp class under the \"com.accenture.demo.start\" package folder. The first step in designing a composable application is to draw an event flow diagram. This is similar to a data flow diagram where the arrows are labeled with the event objects. Note that event flow diagram is not a flow chart and thus decision box is not required. If a user function (also known as a \"task\") contains decision logic, you can draw two or more output from the task to connect to the next set of functions. For example, label the arrows as true, false or a number starting from 1. The composable-example application is a hypothetical \"profile management system\" where you can create a profile, browse or delete it. Figure 2 - Create a profile Figure 2 illustrates an event flow to create a profile. Note that the \"create profile\" can send acknowledgement to the user first. It then encrypts and saves the profile into a data store. Figure 3 - Retrieve a profile Figure 3 demonstrates the case to retrieve a profile. It retrieves an encrypted profile and then passes it to the decryption decryption function to return \"clear text\" of the profile to the user. Figure 4 - Delete a profile Figure 4 shows the case to delete a profile. It deletes a profile using the given profile ID and sends an acknowledgement to the user. The REST endpoints for the three use cases are shown in the \"rest.yaml\" configuration file under the \"main/resources\" in the example subproject. You also find the following configuration parameters in \"application.properties\": rest.server.port=8100 rest.automation=true yaml.rest.automation=classpath:/rest.yaml yaml.flow.automation=classpath:/flows.yaml The flow configuration files are shown in the \"main/resources/flows\" folder where you will find the flow configuration files for the three event flows, namely get-profile.yml, delete-profile.yml and create-profile.yml. Starting the application When the application starts, you will see extract of the application log like this: CompileFlows:142 - Loaded create-profile CompileFlows:142 - Loaded delete-profile CompileFlows:142 - Loaded get-profile CompileFlows:144 - Event scripts deployed: 3 ... ServiceQueue:91 - PRIVATE v1.get.profile with 100 instances started as virtual threads ... RoutingEntry:582 - GET /api/profile/{profile_id} -> [http.flow.adapter], timeout=10s, tracing=true, flow=get-profile ... AppStarter:378 - Modules loaded in 663 ms AppStarter:365 - Reactive HTTP server running on port-8100 It shows that the 3 flow configuration files are compiled as objects to optimize performance. The user functions are loaded into the event system and the REST endpoints are rendered from the \"rest.yaml\" file. Testing the application You can create a test user profile with this python code. Alternatively, you can also use PostMan or other means to do this. >>> import requests, json >>> d = { 'id': 100, 'name': 'Hello World', 'address': '100 World Blvd', 'telephone': '123-456-7890' } >>> h = { 'content-type': 'application/json', 'accept': 'application/json' } >>> r = requests.post('http://127.0.0.1:8100/api/profile', data=json.dumps(d), headers=h) >>> print(r.status_code) 201 >>> print(r.text) { \"profile\": { \"address\": \"***\", \"name\": \"Hello World\", \"telephone\": \"***\", \"id\": 100 }, \"type\": \"CREATE\", \"secure\": [ \"address\", \"telephone\" ] } To verify that the user profile has been created, you can point your browser to http://127.0.0.1:8100/api/profile/100 Your browser will return the following: { \"address\": \"100 World Blvd\", \"name\": \"Hello World\", \"telephone\": \"123-456-7890\", \"id\": 100 } You have successfully tested the two REST endpoints. Tracing information in the application log may look like this: { \"level\": \"INFO\", \"time\": \"2025-08-27 18:39:25.683\", \"source\": \"org.platformlambda.core.services.Telemetry.handleEvent(Telemetry.java:81)\", \"thread\": 336, \"message\": { \"trace\": { \"path\": \"GET /api/profile/100\", \"service\": \"task.executor\", \"success\": true, \"origin\": \"20250828c022812c67294a63871942c568a9e277\", \"exec_time\": 7.0, \"start\": \"2025-08-28T01:39:25.674Z\", \"from\": \"event.script.manager\", \"id\": \"9c0934a98dcf4ab1ae4b5b7b389f6d31\", \"status\": 200 }, \"annotations\": { \"execution\": \"Run 2 tasks in 7 ms\", \"tasks\": [ { \"name\": \"v1.get.profile\", \"spent\": 2.982 }, { \"name\": \"v1.decrypt.fields\", \"spent\": 1.313 } ], \"flow\": \"get-profile\" } } } { \"level\": \"INFO\", \"time\": \"2025-08-27 18:39:25.685\", \"source\": \"org.platformlambda.core.services.Telemetry.handleEvent(Telemetry.java:81)\", \"thread\": 337, \"message\": { \"trace\": { \"path\": \"GET /api/profile/100\", \"service\": \"async.http.response\", \"success\": true, \"origin\": \"20250828c022812c67294a63871942c568a9e277\", \"start\": \"2025-08-28T01:39:25.681Z\", \"exec_time\": 0.374, \"from\": \"task.executor\", \"id\": \"9c0934a98dcf4ab1ae4b5b7b389f6d31\", \"status\": 200 } } } Main application entry point Every application has an entry point. The MainApp in the example app contains the entry point like this: @MainApplication public class MainApp implements EntryPoint { public static void main(String[] args) { AutoStart.main(args); } @Override public void start(String[] args) { // your startup logic here log.info(\"Started\"); } } Since your application is event driven, the main application does not need any additional code in the above example. However, this is a good place to put application initialization code if any. There is also a \"BeforeApplication\" annotation if you want to run some start up code before the event system is started. Writing composable libraries If your software module is intended to be a composable library for other applications to use, you may use the MainApplication method to automatically initialize your library. For example, obtaining authentication and security credentials for a platform component or database that your library uses. Alternatively, you may use the \"autostart module\" method that would automatically send start command to some composable functions in your library. @PreLoad(route=\"demo.library.setup\") public class DemoLibSetup implements TypedLambdaFunction<EventEnvelope, Void> { private static final Logger log = LoggerFactory.getLogger(DemoLibSetup.class); @Override public Void handleEvent(Map<String, String> headers, EventEnvelope input, int instance) throws Exception { log.info(\"Demo library is starting\"); // put your startup business logic here return null; } } The above composable function is labeled as demo.library.setup , you would need to add this to the application.properties of the application that uses this library: # comma separated list of composable module route names modules.autostart=demo.library.setup, flow://my-startup-flow If you use application.yml, it will be a list like this: modules.autostart: - 'demo.library.setup' - 'flow://my-startup-flow' For more sophisticated startup procedure, you can use a flow to execute multiple tasks. The second item in the modules.autostart illustrates this use case. Note : autostart modules or flows should assume there is no input dataset except a header ('type = start') to indicate that the request is triggered by \"autostart\" process. Startup modules usually take input parameters from the environment variables or a secret manager. Graceful shutdown If your application has some dependencies that must be shutdown gracefully, you can use Java's native API similar to this sample code: Runtime.getRuntime().addShutdownHook(new Thread(this::stopAdmin)); Note : In the composable node.js version, there is an \"autostop\" feature to support graceful shutdown of dependencies since JavaScript does not offer similar life-cycle feature in the standard library. Dependency management As a best practice, your user functions should not have any dependencies with other user functions. The second principle of composable design is \"zero to one dependency\". If your composable function must use an external system, platform or database, you can encapsulate the dependency in a composable function. Component scan Please update the following in the application.properties (or application.yml) to include packages of your own functions: web.component.scan=your.package.name You should replace \"your.package.name\" with the real package name(s) that you use in your application. Usually this is your organization software ID or \"namespace\". \"web.component.scan\" is a comma separated list of package names. Deploy your application Composable design can be used to create microservices. You can put related functions in a bounded context with database persistence. Each composable application can be compiled and built into a single \"executable\" for deployment using mvn clean package . The executable JAR is in the target folder. Composable application is by definition cloud native. It is designed to be deployable using Kubernetes or serverless. A sample Dockerfile for your executable JAR may look like this: FROM eclipse-temurin:21.0.1_12-jdk EXPOSE 8083 WORKDIR /app COPY target/your-app-name.jar . ENTRYPOINT [\"java\",\"-jar\",\"your-app-name.jar\"] The above Dockerfile will fetch Openjdk 21 packaged in \"Ubuntu 22.04 LTS\". Event choreography by configuration The best practice for composable design is event choreography by configuration ( Event Script ) discussed above. We will examine the Event Script syntax in Chapter 4 . Generally, you only need to use a very minimal set of mercury core APIs in your user functions. e.g. use PostOffice to obtain a trackable event emitter and AsyncHttpRequest to connect to external system. For composable applications that use Event Script, Mercury core APIs (Platform and PostOffice) are only required for writing unit tests, \"custom flow adapters\", \"legacy functional wrappers\" or \"external gateways\". Orchestration by code Orchestration by code is strongly discouraged because it would result in tightly coupled code . For example, just an \"Import\" statement of another function would create tight coupling of two pieces of code, even when using reactive or event-driven programming styles. However, if there is a use case that you prefer to write orchestration logic by code, you may use the Mercury core APIs to do event-driven programming. API overview will be covered in Chapter 9 . Methodology Home Chapter-2 Methodology Table of Contents Function Execution Strategy","title":"Chapter-1"},{"location":"guides/CHAPTER-1/#introduction","text":"Mercury Composable is a software development toolkit for writing composable applications. Composable application means that an application is assembled from modular software components or functions that are self-contained and pluggable. You can mix-n-match functions to form new applications. You can retire outdated functions without adverse side effect to a production system. Multiple versions of a function can exist, and you can decide how to route user requests to different versions of a function. Applications would be easier to design, develop, maintain, deploy, and scale.","title":"Introduction"},{"location":"guides/CHAPTER-1/#composable-application-architecture","text":"Figure 1 - Composable application architecture As shown in Figure 1, a composable application contains the following: Flow adapters : Each flow adapter listens to requests for onwards delivery to an event manager. Event Manager : it sends events to a set of user functions for them to work together as an application. User functions : these are self-contained functions with clear input and output that are immutable.","title":"Composable application architecture"},{"location":"guides/CHAPTER-1/#http-flow-adapter","text":"A non-blocking HTTP flow adapter is built-in. For other external interface types, you can implement your own flow adapters. e.g. Adapters for MQ, Kafka, Serverless, File based staging area, etc. The standard HTTP flow adapter leverages the underlying REST automation system to serve user facing REST API endpoints. For example, a hypothetical \"get profile\" endpoint is created like this in the \"rest.yaml\" configuration file: - service: \"http.flow.adapter\" methods: ['GET'] url: \"/api/profile/{profile_id}\" flow: 'get-profile' timeout: 10s cors: cors_1 headers: header_1 tracing: true In this REST configuration entry, the system creates a REST API endpoint for \"GET /api/profile/{profile_id}\". When a request arrives at this endpoint, the HTTP request will be converted to an incoming event by the flow adapter that routes the event to the \"event manager\" to execute a new instance of the \"get-profile\" flow.","title":"HTTP flow adapter"},{"location":"guides/CHAPTER-1/#flow-configuration-example","text":"The event manager is driven by configuration instead of code. A hypothetical \"get profile\" flow is defined in a YAML file like this: flow: id: 'get-profile' description: 'Get a user profile using profile ID' ttl: 10s exception: 'v1.hello.exception' first.task: 'v1.get.profile' tasks: - input: - 'input.path_parameter.profile_id -> header.profile_id' process: 'v1.get.profile' output: - 'result -> model.profile' description: 'Retrieve user profile from database using profile_id' execution: sequential next: - 'v1.decrypt.fields' - input: - 'model.profile -> dataset' - 'text(telephone, address) -> protected_fields' process: 'v1.decrypt.fields' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Decrypt fields' execution: end - input: - 'error.code -> status' - 'error.message -> message' - 'error.stack -> stack' process: 'v1.hello.exception' output: - 'result.status -> output.status' - 'result -> output.body' description: 'Just a demo exception handler' execution: end Note that the flow configuration is referring user functions by their \"route\" names. It is because all user functions are self-contained with clearly defined input and output and the event manager would set their inputs and collect their outputs accordingly. Note that you can map selected key-values or the whole event as a business object and this decoupling promotes highly reusable user functional software. The event manager will create a \"state machine\" to manage each transaction flow because all user functions are stateless. The \"state machine\" is referenced using the namespace \"model\".","title":"Flow configuration example"},{"location":"guides/CHAPTER-1/#assigning-a-route-name-to-a-user-function","text":"You can assign a route name to a Java class using the PreLoad annotation like this: @PreLoad(route=\"v1.get.profile\", instances=100) public class GetProfile implements TypedLambdaFunction<Map<String, Object>, Profile> { @Override public Profile handleEvent(Map<String, String> headers, Map<String, Object> input, int instance) { // your business logic here return result; } } Inside the \"handleEvent\" method, you can write regular Java code using your preferred coding style and framework. You can define input/output as Map or PoJo.","title":"Assigning a route name to a user function"},{"location":"guides/CHAPTER-1/#building-the-mercury-libraries-from-source","text":"Mercury Composable leverages the best of Java 21 virtual threading technology. Therefore, you would need to install Java JDK version 21 or higher. You also need maven version 3.9.7 or higher to build the libraries. Assuming you clone the repository into the \"sandbox\" directory, you may build the libraries like this. cd sandbox/mercury-composable mvn clean install The compiled libraries will be saved to your local \".m2\" maven repository. For production, you may publish the Mercury Composable libraries into your enterprise artifactory. We use \"maven\" build scripts. If your organization uses other build tools such as gradle, please convert them accordingly.","title":"Building the Mercury libraries from source"},{"location":"guides/CHAPTER-1/#things-to-avoid-with-java-21","text":"By default, user functions are executed using Java 21 virtual threading technology. However, for performance reason, there are two things that you MUST avoid: Synchronized keyword : This will block the event loop in the Java VM, meaning that your whole application is blocked when the synchronized block executes. ThreadLocal : Java 21 virtual thread is designed to be very light weight. When you use ThreadLocal variables, the \"virtual thread\" becomes heavy weighted and the Garbage Collector may have difficulties catching up. Since Mercury provides thread management abstraction, there is no need to use the Synchronized keyword and ThreadLocal variables. The built-in \"state machine\" is a better place to keep your runtime variables for each transaction. Interestingly, the \"Thread\" and \"Future\" APIs are safe to use in a virtual thread. If you are putting legacy code inside a new user function and the legacy code runs in blocking mode, you can annotate the user function with the \"KernelThreadRunner\" class. This tells the system to turn on compatibility mode to support the blocking code. The kernel thread would isolate the blocking code from the rest of the application. However, kernel threads are limited resources. While virtual threads can support tens of thousands of cooperative concurrent execution, kernel threads are limited to 250, depending on the number of CPU cores that the target machine has.","title":"Things to avoid with Java 21"},{"location":"guides/CHAPTER-1/#composable-application-example","text":"Let's take a test drive of a composable application example in the \"examples/composable-example\" subproject. You can use your favorite IDE to run the example or execute it from a terminal using command line. To run it from the command line, you may do this: cd sandbox/mercury-composable/examples/composable-example java -jar target/composable-example-4.2.0.jar If you run the application from the IDE, you may execute the \"main\" method in the MainApp class under the \"com.accenture.demo.start\" package folder. The first step in designing a composable application is to draw an event flow diagram. This is similar to a data flow diagram where the arrows are labeled with the event objects. Note that event flow diagram is not a flow chart and thus decision box is not required. If a user function (also known as a \"task\") contains decision logic, you can draw two or more output from the task to connect to the next set of functions. For example, label the arrows as true, false or a number starting from 1. The composable-example application is a hypothetical \"profile management system\" where you can create a profile, browse or delete it. Figure 2 - Create a profile Figure 2 illustrates an event flow to create a profile. Note that the \"create profile\" can send acknowledgement to the user first. It then encrypts and saves the profile into a data store. Figure 3 - Retrieve a profile Figure 3 demonstrates the case to retrieve a profile. It retrieves an encrypted profile and then passes it to the decryption decryption function to return \"clear text\" of the profile to the user. Figure 4 - Delete a profile Figure 4 shows the case to delete a profile. It deletes a profile using the given profile ID and sends an acknowledgement to the user. The REST endpoints for the three use cases are shown in the \"rest.yaml\" configuration file under the \"main/resources\" in the example subproject. You also find the following configuration parameters in \"application.properties\": rest.server.port=8100 rest.automation=true yaml.rest.automation=classpath:/rest.yaml yaml.flow.automation=classpath:/flows.yaml The flow configuration files are shown in the \"main/resources/flows\" folder where you will find the flow configuration files for the three event flows, namely get-profile.yml, delete-profile.yml and create-profile.yml.","title":"Composable application example"},{"location":"guides/CHAPTER-1/#starting-the-application","text":"When the application starts, you will see extract of the application log like this: CompileFlows:142 - Loaded create-profile CompileFlows:142 - Loaded delete-profile CompileFlows:142 - Loaded get-profile CompileFlows:144 - Event scripts deployed: 3 ... ServiceQueue:91 - PRIVATE v1.get.profile with 100 instances started as virtual threads ... RoutingEntry:582 - GET /api/profile/{profile_id} -> [http.flow.adapter], timeout=10s, tracing=true, flow=get-profile ... AppStarter:378 - Modules loaded in 663 ms AppStarter:365 - Reactive HTTP server running on port-8100 It shows that the 3 flow configuration files are compiled as objects to optimize performance. The user functions are loaded into the event system and the REST endpoints are rendered from the \"rest.yaml\" file.","title":"Starting the application"},{"location":"guides/CHAPTER-1/#testing-the-application","text":"You can create a test user profile with this python code. Alternatively, you can also use PostMan or other means to do this. >>> import requests, json >>> d = { 'id': 100, 'name': 'Hello World', 'address': '100 World Blvd', 'telephone': '123-456-7890' } >>> h = { 'content-type': 'application/json', 'accept': 'application/json' } >>> r = requests.post('http://127.0.0.1:8100/api/profile', data=json.dumps(d), headers=h) >>> print(r.status_code) 201 >>> print(r.text) { \"profile\": { \"address\": \"***\", \"name\": \"Hello World\", \"telephone\": \"***\", \"id\": 100 }, \"type\": \"CREATE\", \"secure\": [ \"address\", \"telephone\" ] } To verify that the user profile has been created, you can point your browser to http://127.0.0.1:8100/api/profile/100 Your browser will return the following: { \"address\": \"100 World Blvd\", \"name\": \"Hello World\", \"telephone\": \"123-456-7890\", \"id\": 100 } You have successfully tested the two REST endpoints. Tracing information in the application log may look like this: { \"level\": \"INFO\", \"time\": \"2025-08-27 18:39:25.683\", \"source\": \"org.platformlambda.core.services.Telemetry.handleEvent(Telemetry.java:81)\", \"thread\": 336, \"message\": { \"trace\": { \"path\": \"GET /api/profile/100\", \"service\": \"task.executor\", \"success\": true, \"origin\": \"20250828c022812c67294a63871942c568a9e277\", \"exec_time\": 7.0, \"start\": \"2025-08-28T01:39:25.674Z\", \"from\": \"event.script.manager\", \"id\": \"9c0934a98dcf4ab1ae4b5b7b389f6d31\", \"status\": 200 }, \"annotations\": { \"execution\": \"Run 2 tasks in 7 ms\", \"tasks\": [ { \"name\": \"v1.get.profile\", \"spent\": 2.982 }, { \"name\": \"v1.decrypt.fields\", \"spent\": 1.313 } ], \"flow\": \"get-profile\" } } } { \"level\": \"INFO\", \"time\": \"2025-08-27 18:39:25.685\", \"source\": \"org.platformlambda.core.services.Telemetry.handleEvent(Telemetry.java:81)\", \"thread\": 337, \"message\": { \"trace\": { \"path\": \"GET /api/profile/100\", \"service\": \"async.http.response\", \"success\": true, \"origin\": \"20250828c022812c67294a63871942c568a9e277\", \"start\": \"2025-08-28T01:39:25.681Z\", \"exec_time\": 0.374, \"from\": \"task.executor\", \"id\": \"9c0934a98dcf4ab1ae4b5b7b389f6d31\", \"status\": 200 } } }","title":"Testing the application"},{"location":"guides/CHAPTER-1/#main-application-entry-point","text":"Every application has an entry point. The MainApp in the example app contains the entry point like this: @MainApplication public class MainApp implements EntryPoint { public static void main(String[] args) { AutoStart.main(args); } @Override public void start(String[] args) { // your startup logic here log.info(\"Started\"); } } Since your application is event driven, the main application does not need any additional code in the above example. However, this is a good place to put application initialization code if any. There is also a \"BeforeApplication\" annotation if you want to run some start up code before the event system is started.","title":"Main application entry point"},{"location":"guides/CHAPTER-1/#writing-composable-libraries","text":"If your software module is intended to be a composable library for other applications to use, you may use the MainApplication method to automatically initialize your library. For example, obtaining authentication and security credentials for a platform component or database that your library uses. Alternatively, you may use the \"autostart module\" method that would automatically send start command to some composable functions in your library. @PreLoad(route=\"demo.library.setup\") public class DemoLibSetup implements TypedLambdaFunction<EventEnvelope, Void> { private static final Logger log = LoggerFactory.getLogger(DemoLibSetup.class); @Override public Void handleEvent(Map<String, String> headers, EventEnvelope input, int instance) throws Exception { log.info(\"Demo library is starting\"); // put your startup business logic here return null; } } The above composable function is labeled as demo.library.setup , you would need to add this to the application.properties of the application that uses this library: # comma separated list of composable module route names modules.autostart=demo.library.setup, flow://my-startup-flow If you use application.yml, it will be a list like this: modules.autostart: - 'demo.library.setup' - 'flow://my-startup-flow' For more sophisticated startup procedure, you can use a flow to execute multiple tasks. The second item in the modules.autostart illustrates this use case. Note : autostart modules or flows should assume there is no input dataset except a header ('type = start') to indicate that the request is triggered by \"autostart\" process. Startup modules usually take input parameters from the environment variables or a secret manager.","title":"Writing composable libraries"},{"location":"guides/CHAPTER-1/#graceful-shutdown","text":"If your application has some dependencies that must be shutdown gracefully, you can use Java's native API similar to this sample code: Runtime.getRuntime().addShutdownHook(new Thread(this::stopAdmin)); Note : In the composable node.js version, there is an \"autostop\" feature to support graceful shutdown of dependencies since JavaScript does not offer similar life-cycle feature in the standard library.","title":"Graceful shutdown"},{"location":"guides/CHAPTER-1/#dependency-management","text":"As a best practice, your user functions should not have any dependencies with other user functions. The second principle of composable design is \"zero to one dependency\". If your composable function must use an external system, platform or database, you can encapsulate the dependency in a composable function.","title":"Dependency management"},{"location":"guides/CHAPTER-1/#component-scan","text":"Please update the following in the application.properties (or application.yml) to include packages of your own functions: web.component.scan=your.package.name You should replace \"your.package.name\" with the real package name(s) that you use in your application. Usually this is your organization software ID or \"namespace\". \"web.component.scan\" is a comma separated list of package names.","title":"Component scan"},{"location":"guides/CHAPTER-1/#deploy-your-application","text":"Composable design can be used to create microservices. You can put related functions in a bounded context with database persistence. Each composable application can be compiled and built into a single \"executable\" for deployment using mvn clean package . The executable JAR is in the target folder. Composable application is by definition cloud native. It is designed to be deployable using Kubernetes or serverless. A sample Dockerfile for your executable JAR may look like this: FROM eclipse-temurin:21.0.1_12-jdk EXPOSE 8083 WORKDIR /app COPY target/your-app-name.jar . ENTRYPOINT [\"java\",\"-jar\",\"your-app-name.jar\"] The above Dockerfile will fetch Openjdk 21 packaged in \"Ubuntu 22.04 LTS\".","title":"Deploy your application"},{"location":"guides/CHAPTER-1/#event-choreography-by-configuration","text":"The best practice for composable design is event choreography by configuration ( Event Script ) discussed above. We will examine the Event Script syntax in Chapter 4 . Generally, you only need to use a very minimal set of mercury core APIs in your user functions. e.g. use PostOffice to obtain a trackable event emitter and AsyncHttpRequest to connect to external system. For composable applications that use Event Script, Mercury core APIs (Platform and PostOffice) are only required for writing unit tests, \"custom flow adapters\", \"legacy functional wrappers\" or \"external gateways\".","title":"Event choreography by configuration"},{"location":"guides/CHAPTER-1/#orchestration-by-code","text":"Orchestration by code is strongly discouraged because it would result in tightly coupled code . For example, just an \"Import\" statement of another function would create tight coupling of two pieces of code, even when using reactive or event-driven programming styles. However, if there is a use case that you prefer to write orchestration logic by code, you may use the Mercury core APIs to do event-driven programming. API overview will be covered in Chapter 9 . Methodology Home Chapter-2 Methodology Table of Contents Function Execution Strategy","title":"Orchestration by code"},{"location":"guides/CHAPTER-2/","text":"Function Execution Strategies Define a function In a composable application, each function is self-contained with zero dependencies with other user functions. Only flow adapter, data adapter, notification function or gateway has a single external dependency such as a network event system, a database or an external REST resource. A \"task\" or \"function\" is a class that implements the LambdaFunction or TypedLambdaFunction interface. Within each function boundary, it may have private methods that are fully contained within the class. As discussed in Chapter-1, a function may look like this: @PreLoad(route = \"my.first.function\", instances = 10) public class MyFirstFunction implements TypedLambdaFunction<MyPoJo, AnotherPoJo> { @Override public AnotherPojo handleEvent(Map<String, String> headers, MyPoJo input, int instance) { // your business logic here return result; } } A function is an event listener with the \"handleEvent\" method. The data structures of input and output are defined by API interface contract in an event flow configuration. In the above example, the input is MyPoJo and the output is AnotherPoJo. For event choreography, input body is represented as a PoJo or a Map of key-values so that you can use the dot-bracket convention to map subset of a PoJo from one function to another if needed. In addition to the input PoJo, you may pass additional parameters to the user function as event headers. We will discuss this in Chapter 4 - Event Script Syntax . Non-blocking design While you can apply sequential, object-oriented or reactive programming styles in your functions, you should pay attention to making your function non-blocking and fast. In a virtual thread, if you use Java Future, the \".get()\" method is synchronous but it is non-blocking behind the curtain. This is like using the \"await\" keyword in other programming language. Virtual thread execution promotes performance and high concurrency. However, it would be suboptimal if you mix blocking code in a user function. It will block the whole event loop, resulting in substantial degradation of application performance. We therefore recommend your user function to be implemented in non-blocking or reactive styles. When you are using a reactive library in your function, your function can return a \"Mono\" or \"Flux\" reactive response object using the Project-Reactor Core library. For simplicity, we support only the Mono and Flux reactive response objects. If you use other types of reactive APIs, please convert them into a Mono or Flux accordingly. User function that returns a Mono object For Mono return value, a reactive user function may look like this: @PreLoad(route = \"v1.reactive.mono.function\") public class MonoUserFunction implements TypedLambdaFunction<Map<String, Object>, Mono<Map<String, Object>>> { private static final Logger log = LoggerFactory.getLogger(MonoUserFunction.class); private static final String EXCEPTION = \"exception\"; @Override public Mono<Map<String, Object>> handleEvent(Map<String, String> headers, Map<String, Object> input, int instance) { log.info(\"GOT {} {}\", headers, input); return Mono.create(callback -> { if (headers.containsKey(EXCEPTION)) { callback.error(new AppException(400, headers.get(EXCEPTION))); } else { callback.success(input); } }); } } User function that returns a Flux object For Flux return value, it may look like this: @PreLoad(route = \"v1.reactive.flux.function\") public class FluxUserFunction implements TypedLambdaFunction<Map<String, Object>, Flux<Map<String, Object>>> { private static final Logger log = LoggerFactory.getLogger(FluxUserFunction.class); private static final String EXCEPTION = \"exception\"; @Override public Flux<Map<String, Object>> handleEvent(Map<String, String> headers, Map<String, Object> input, int instance) { log.info(\"GOT {} {}\", headers, input); return Flux.create(emitter -> { if (headers.containsKey(EXCEPTION)) { emitter.error(new AppException(400, headers.get(EXCEPTION))); } else { // just generate two messages emitter.next(Map.of(\"first\", \"message\")); emitter.next(input); emitter.complete(); } }); } } Handling a Flux stream When your function returns a Flux stream object, the system will pass the stream ID of the underlying event stream to the calling function. The input arguments for the event stream ID and time-to-live parameters are provided in the event headers to your function that implements the TypedLambdaFunction or LambdaFunction. The following event headers will be provided to the calling function: x-stream-id: streamId x-ttl: ttl In the calling function, you can create a FluxConsumer to handle the incoming event stream like this: String streamId = headers.get(\"x-stream-id\"); long ttl = Utility.getInstance().str2long(headers.get(\"x-ttl\")); FluxConsumer<Map<String, Object>> fc = new FluxConsumer<>(streamId, ttl); fc.consume( data -> { // handle incoming message }, e -> { // handle exception where e is a Throwable }, () -> { // handle stream completion } ); The API signatures for FluxConsumer are as follows: // Consume the event stream when the payload is not a PoJo public void consume(Consumer<T> consumer, Consumer<Throwable> errorConsumer, Runnable completeConsumer); // Consume the event stream when the payload can be mapped as PoJo public void consume(Consumer<T> consumer, Consumer<Throwable> errorConsumer, Runnable completeConsumer, Class<T> pojoClass); // Consume the event stream when the payload can be mapped as PoJo using a custom serializer public void consume(Consumer<T> consumer, Consumer<Throwable> errorConsumer, Runnable completeConsumer, Class<T> pojoClass, CustomSerializer serializer); Serialization consideration If you use the FluxConsumer's consume method without pojoClass hint, the system will deliver Java primitive and HashMap through an event stream. If you pass PoJo, HashMap or Java primitive such as String or byte[], you do not need to do any serialization. If the objects that your function streams over a Mono or Flux channel are not supported, you must perform custom serialization. This can be achieved using the \"map\" method of the Mono or Flux class. For example, your function obtains a stream of Flux result objects from a database call. You can serialize the objects using a custom serializer like this: // \"source\" is the original Flux object Flux<Map<String, Object> serializedStream = source.map(specialPoJo -> { return myCustomSerializer.toMap(specialPoJo); }); return serializedStream; Your customSerializer should implement the org.platformlambda.core.models.CustomSerializer interface. public interface CustomSerializer { public Map<String, Object> toMap(Object obj); public <T> T toPoJo(Object obj, Class<T> toValueType); } Extensible authentication function You can add authentication function using the optional authentication tag in a service. In \"rest.yaml\", a service for a REST endpoint refers to a function in your application. An authentication function can be written using a TypedLambdaFunction that takes the input as a \"AsyncHttpRequest\". Your authentication function can return a boolean value to indicate if the request should be accepted or rejected. A typical authentication function may validate an HTTP header or cookie. e.g. forward the \"Bearer token\" from the \"Authorization\" header to your organization's OAuth 2.0 Identity Provider for validation. To approve an incoming request, your custom authentication function can return true . Optionally, you can add \"session\" key-values by returning an EventEnvelope like this: return new EventEnvelope().setHeader(\"user_id\", \"A12345\").setBody(true); The above example approves the incoming request and returns a \"session\" variable (\"user_id\": \"A12345\") to the next task. If your authentication function returns false , the user will receive a \"HTTP-401 Unauthorized\" error response. You can also control the status code and error message by throwing an AppException like this: throw new AppException(401, \"Invalid credentials\"); Alternatively, you may implement authentication as a user function in the first step of an event flow. In this case, the input to the function is defined by the \"input data mapping\" rules in the event flow configuration. The advantage of this approach is that authentication is shown as part of an event flow so that the application design intention is clear. A composable application is assembled from a collection of self-contained functions that are highly reusable. Number of workers for a function In the following annotation, the parameter \"instances\" tells the system to reserve a number of workers for the function. Workers are running on-demand to handle concurrent user requests. @PreLoad(route = \"my.first.function\", instances = 10) Note that you can use smaller number of workers to handle many concurrent users if your function finishes processing very quickly. If not, you should reserve more workers to handle the work load. Concurrency requires careful planning for optimal performance and throughput. Strategies for function execution Let's review the strategies for function execution. A function is executed when an event arrives. Strategy Advantage Disadvantage Virtual thread Higher throughput in terms of concurrent users N/A Kernel threads Higher performance in terms of operations per seconds Lower number of concurrent threads due to high context switching overheads Virtual thread By default, the system will run your function as a virtual thread because this is the most efficient execution strategy. In a virtual thread, the \"Thread\" object in the standard library will operate in non-blocking mode. This means it is safe to use the Thread.sleep() method. It will release control to the event loop when your function enters into sleep, thus freeing CPU resources for other functions. We have added the \"request\" methods in the PostOffice API to support non-blocking RPC that leverages this suspend/resume feature of virtual thread management. Future<EventEnvelope> future = po.request(requestEvent, timeout); EventEnvelope result = future.get(); // alternatively, you can do: EventEnvelope result = po.request(requestEvent, timeout).get(); Note : The PostOffice API is used when you want to do orchestration by code. If you are using Event Script, you can manage event flows using one or more configuration files. Kernel thread pool When you add the annotation \"KernelThreadRunner\" in a function declared as LambdaFunction or TypedLambdaFunction, the function will be executed using a \"kernel thread pool\" and Java will run your function in native \"preemptive multitasking\" mode. While preemptive multitasking fully utilizes the CPU, its context switching overheads increase as the number of kernel threads grow. As a rule of thumb, you should control the maximum number of kernel threads to be less than 200. The parameter kernel.thread.pool is defined with a default value of 100. You can change this value to adjust to the actual CPU power in your environment. Keep the default value for best performance unless you have tested the limit in your environment. Note : When you have more concurrent requests, your application may slow down because some functions are blocked when the number of concurrent kernel threads is reached. You should reduce the number of \"instances\" (i.e. worker pool) for a function to a small number so that your application does not exceed the maximum limit of the kernel.thread.pool parameter. Kernel threads are precious and finite resources. When your function is computational intensive or making external HTTP or database calls in a synchronous blocking manner, you may use it with a small number of worker instances. To rapidly release kernel thread resources, you should write \"asynchronous\" code. i.e. for event-driven programming, you can use send event to another function asynchronously, and you can create a callback function to listen to responses. For RPC call, you can use the asyncRequest method to make asynchronous RPC calls. However, coding for asynchronous pattern is more challenging. For example, you may want to return a \"pending\" result immediately using HTTP-202. Your code will move on to execute using a \"future\" that will execute callback methods ( onSuccess and onFailure ). Another approach is to annotate the function as an EventInterceptor so that your function can respond to the user in a \"future\" callback. For ease of programming, we recommend using virtual thread to handle synchronous RPC calls in a non-blocking manner. Solving the puzzle of multithreading performance Before the availability of virtual thread technology in Java 21, Java VM has been using kernel threads for code execution. If you have a lot of users hitting your service concurrently, multiple threads are created to serve concurrent requests. When your code serving the requests makes blocking call to other services, the kernel threads are busy while your user functions wait for responses. Kernel threads that are in the wait state is still consuming CPU time. If the blocking calls finish very quickly, this is not be an issue. However, when the blocking calls take longer to complete, a lot of outstanding kernel threads that are waiting for responses would compete for CPU resources, resulting in higher internal friction in the JVM that makes your application running slower. This is not a productive use of computer resources. This type of performance issue caused by internal friction is very difficult to avoid. While event driven and reactive programming that uses asynchronous processing and callbacks would address this artificial bottleneck, asynchronous code is harder to implement and maintain when the application complexity increases. It would be ideal if we can write sequential code that does not block. Sequential code is much easier to write and read because it communicates the intent of the code clearly. Leveraging Java 21 virtual thread technology, Mercury Composable allows the developer to write code in a sequential manner. When code in your function makes an RPC call to another service using the PostOffice's \"request\" API, it returns a Java Future object but the \"Future\" object itself is running in a virtual thread. This means when your code retrieves the RPC result using the \"get\" method, your code appears \"blocked\" while waiting for the response from the target service. Although your code appears to be \"blocked\", the virtual thread is \u201csuspended\u201d. It will wake up when the response arrives. When a virtual thread is suspended, it does not consume CPU time and the memory structure for keeping the thread in suspend mode is very small. Virtual thread technology is designed to support tens of thousands of concurrent RPC requests in a single compute machine, container or serverless instance. Mercury Composable supports mixed thread management - virtual threads and kernel threads. Functions running in different types of threads are connected loosely in events. This functional isolation and encapsulation mean that you can precisely control how your application performs for each functional logic block. Chapter-1 Home Chapter-3 Introduction Table of Contents REST Automation","title":"Chapter-2"},{"location":"guides/CHAPTER-2/#function-execution-strategies","text":"","title":"Function Execution Strategies"},{"location":"guides/CHAPTER-2/#define-a-function","text":"In a composable application, each function is self-contained with zero dependencies with other user functions. Only flow adapter, data adapter, notification function or gateway has a single external dependency such as a network event system, a database or an external REST resource. A \"task\" or \"function\" is a class that implements the LambdaFunction or TypedLambdaFunction interface. Within each function boundary, it may have private methods that are fully contained within the class. As discussed in Chapter-1, a function may look like this: @PreLoad(route = \"my.first.function\", instances = 10) public class MyFirstFunction implements TypedLambdaFunction<MyPoJo, AnotherPoJo> { @Override public AnotherPojo handleEvent(Map<String, String> headers, MyPoJo input, int instance) { // your business logic here return result; } } A function is an event listener with the \"handleEvent\" method. The data structures of input and output are defined by API interface contract in an event flow configuration. In the above example, the input is MyPoJo and the output is AnotherPoJo. For event choreography, input body is represented as a PoJo or a Map of key-values so that you can use the dot-bracket convention to map subset of a PoJo from one function to another if needed. In addition to the input PoJo, you may pass additional parameters to the user function as event headers. We will discuss this in Chapter 4 - Event Script Syntax .","title":"Define a function"},{"location":"guides/CHAPTER-2/#non-blocking-design","text":"While you can apply sequential, object-oriented or reactive programming styles in your functions, you should pay attention to making your function non-blocking and fast. In a virtual thread, if you use Java Future, the \".get()\" method is synchronous but it is non-blocking behind the curtain. This is like using the \"await\" keyword in other programming language. Virtual thread execution promotes performance and high concurrency. However, it would be suboptimal if you mix blocking code in a user function. It will block the whole event loop, resulting in substantial degradation of application performance. We therefore recommend your user function to be implemented in non-blocking or reactive styles. When you are using a reactive library in your function, your function can return a \"Mono\" or \"Flux\" reactive response object using the Project-Reactor Core library. For simplicity, we support only the Mono and Flux reactive response objects. If you use other types of reactive APIs, please convert them into a Mono or Flux accordingly.","title":"Non-blocking design"},{"location":"guides/CHAPTER-2/#user-function-that-returns-a-mono-object","text":"For Mono return value, a reactive user function may look like this: @PreLoad(route = \"v1.reactive.mono.function\") public class MonoUserFunction implements TypedLambdaFunction<Map<String, Object>, Mono<Map<String, Object>>> { private static final Logger log = LoggerFactory.getLogger(MonoUserFunction.class); private static final String EXCEPTION = \"exception\"; @Override public Mono<Map<String, Object>> handleEvent(Map<String, String> headers, Map<String, Object> input, int instance) { log.info(\"GOT {} {}\", headers, input); return Mono.create(callback -> { if (headers.containsKey(EXCEPTION)) { callback.error(new AppException(400, headers.get(EXCEPTION))); } else { callback.success(input); } }); } }","title":"User function that returns a Mono object"},{"location":"guides/CHAPTER-2/#user-function-that-returns-a-flux-object","text":"For Flux return value, it may look like this: @PreLoad(route = \"v1.reactive.flux.function\") public class FluxUserFunction implements TypedLambdaFunction<Map<String, Object>, Flux<Map<String, Object>>> { private static final Logger log = LoggerFactory.getLogger(FluxUserFunction.class); private static final String EXCEPTION = \"exception\"; @Override public Flux<Map<String, Object>> handleEvent(Map<String, String> headers, Map<String, Object> input, int instance) { log.info(\"GOT {} {}\", headers, input); return Flux.create(emitter -> { if (headers.containsKey(EXCEPTION)) { emitter.error(new AppException(400, headers.get(EXCEPTION))); } else { // just generate two messages emitter.next(Map.of(\"first\", \"message\")); emitter.next(input); emitter.complete(); } }); } }","title":"User function that returns a Flux object"},{"location":"guides/CHAPTER-2/#handling-a-flux-stream","text":"When your function returns a Flux stream object, the system will pass the stream ID of the underlying event stream to the calling function. The input arguments for the event stream ID and time-to-live parameters are provided in the event headers to your function that implements the TypedLambdaFunction or LambdaFunction. The following event headers will be provided to the calling function: x-stream-id: streamId x-ttl: ttl In the calling function, you can create a FluxConsumer to handle the incoming event stream like this: String streamId = headers.get(\"x-stream-id\"); long ttl = Utility.getInstance().str2long(headers.get(\"x-ttl\")); FluxConsumer<Map<String, Object>> fc = new FluxConsumer<>(streamId, ttl); fc.consume( data -> { // handle incoming message }, e -> { // handle exception where e is a Throwable }, () -> { // handle stream completion } ); The API signatures for FluxConsumer are as follows: // Consume the event stream when the payload is not a PoJo public void consume(Consumer<T> consumer, Consumer<Throwable> errorConsumer, Runnable completeConsumer); // Consume the event stream when the payload can be mapped as PoJo public void consume(Consumer<T> consumer, Consumer<Throwable> errorConsumer, Runnable completeConsumer, Class<T> pojoClass); // Consume the event stream when the payload can be mapped as PoJo using a custom serializer public void consume(Consumer<T> consumer, Consumer<Throwable> errorConsumer, Runnable completeConsumer, Class<T> pojoClass, CustomSerializer serializer);","title":"Handling a Flux stream"},{"location":"guides/CHAPTER-2/#serialization-consideration","text":"If you use the FluxConsumer's consume method without pojoClass hint, the system will deliver Java primitive and HashMap through an event stream. If you pass PoJo, HashMap or Java primitive such as String or byte[], you do not need to do any serialization. If the objects that your function streams over a Mono or Flux channel are not supported, you must perform custom serialization. This can be achieved using the \"map\" method of the Mono or Flux class. For example, your function obtains a stream of Flux result objects from a database call. You can serialize the objects using a custom serializer like this: // \"source\" is the original Flux object Flux<Map<String, Object> serializedStream = source.map(specialPoJo -> { return myCustomSerializer.toMap(specialPoJo); }); return serializedStream; Your customSerializer should implement the org.platformlambda.core.models.CustomSerializer interface. public interface CustomSerializer { public Map<String, Object> toMap(Object obj); public <T> T toPoJo(Object obj, Class<T> toValueType); }","title":"Serialization consideration"},{"location":"guides/CHAPTER-2/#extensible-authentication-function","text":"You can add authentication function using the optional authentication tag in a service. In \"rest.yaml\", a service for a REST endpoint refers to a function in your application. An authentication function can be written using a TypedLambdaFunction that takes the input as a \"AsyncHttpRequest\". Your authentication function can return a boolean value to indicate if the request should be accepted or rejected. A typical authentication function may validate an HTTP header or cookie. e.g. forward the \"Bearer token\" from the \"Authorization\" header to your organization's OAuth 2.0 Identity Provider for validation. To approve an incoming request, your custom authentication function can return true . Optionally, you can add \"session\" key-values by returning an EventEnvelope like this: return new EventEnvelope().setHeader(\"user_id\", \"A12345\").setBody(true); The above example approves the incoming request and returns a \"session\" variable (\"user_id\": \"A12345\") to the next task. If your authentication function returns false , the user will receive a \"HTTP-401 Unauthorized\" error response. You can also control the status code and error message by throwing an AppException like this: throw new AppException(401, \"Invalid credentials\"); Alternatively, you may implement authentication as a user function in the first step of an event flow. In this case, the input to the function is defined by the \"input data mapping\" rules in the event flow configuration. The advantage of this approach is that authentication is shown as part of an event flow so that the application design intention is clear. A composable application is assembled from a collection of self-contained functions that are highly reusable.","title":"Extensible authentication function"},{"location":"guides/CHAPTER-2/#number-of-workers-for-a-function","text":"In the following annotation, the parameter \"instances\" tells the system to reserve a number of workers for the function. Workers are running on-demand to handle concurrent user requests. @PreLoad(route = \"my.first.function\", instances = 10) Note that you can use smaller number of workers to handle many concurrent users if your function finishes processing very quickly. If not, you should reserve more workers to handle the work load. Concurrency requires careful planning for optimal performance and throughput.","title":"Number of workers for a function"},{"location":"guides/CHAPTER-2/#strategies-for-function-execution","text":"Let's review the strategies for function execution. A function is executed when an event arrives. Strategy Advantage Disadvantage Virtual thread Higher throughput in terms of concurrent users N/A Kernel threads Higher performance in terms of operations per seconds Lower number of concurrent threads due to high context switching overheads","title":"Strategies for function execution"},{"location":"guides/CHAPTER-2/#virtual-thread","text":"By default, the system will run your function as a virtual thread because this is the most efficient execution strategy. In a virtual thread, the \"Thread\" object in the standard library will operate in non-blocking mode. This means it is safe to use the Thread.sleep() method. It will release control to the event loop when your function enters into sleep, thus freeing CPU resources for other functions. We have added the \"request\" methods in the PostOffice API to support non-blocking RPC that leverages this suspend/resume feature of virtual thread management. Future<EventEnvelope> future = po.request(requestEvent, timeout); EventEnvelope result = future.get(); // alternatively, you can do: EventEnvelope result = po.request(requestEvent, timeout).get(); Note : The PostOffice API is used when you want to do orchestration by code. If you are using Event Script, you can manage event flows using one or more configuration files.","title":"Virtual thread"},{"location":"guides/CHAPTER-2/#kernel-thread-pool","text":"When you add the annotation \"KernelThreadRunner\" in a function declared as LambdaFunction or TypedLambdaFunction, the function will be executed using a \"kernel thread pool\" and Java will run your function in native \"preemptive multitasking\" mode. While preemptive multitasking fully utilizes the CPU, its context switching overheads increase as the number of kernel threads grow. As a rule of thumb, you should control the maximum number of kernel threads to be less than 200. The parameter kernel.thread.pool is defined with a default value of 100. You can change this value to adjust to the actual CPU power in your environment. Keep the default value for best performance unless you have tested the limit in your environment. Note : When you have more concurrent requests, your application may slow down because some functions are blocked when the number of concurrent kernel threads is reached. You should reduce the number of \"instances\" (i.e. worker pool) for a function to a small number so that your application does not exceed the maximum limit of the kernel.thread.pool parameter. Kernel threads are precious and finite resources. When your function is computational intensive or making external HTTP or database calls in a synchronous blocking manner, you may use it with a small number of worker instances. To rapidly release kernel thread resources, you should write \"asynchronous\" code. i.e. for event-driven programming, you can use send event to another function asynchronously, and you can create a callback function to listen to responses. For RPC call, you can use the asyncRequest method to make asynchronous RPC calls. However, coding for asynchronous pattern is more challenging. For example, you may want to return a \"pending\" result immediately using HTTP-202. Your code will move on to execute using a \"future\" that will execute callback methods ( onSuccess and onFailure ). Another approach is to annotate the function as an EventInterceptor so that your function can respond to the user in a \"future\" callback. For ease of programming, we recommend using virtual thread to handle synchronous RPC calls in a non-blocking manner.","title":"Kernel thread pool"},{"location":"guides/CHAPTER-2/#solving-the-puzzle-of-multithreading-performance","text":"Before the availability of virtual thread technology in Java 21, Java VM has been using kernel threads for code execution. If you have a lot of users hitting your service concurrently, multiple threads are created to serve concurrent requests. When your code serving the requests makes blocking call to other services, the kernel threads are busy while your user functions wait for responses. Kernel threads that are in the wait state is still consuming CPU time. If the blocking calls finish very quickly, this is not be an issue. However, when the blocking calls take longer to complete, a lot of outstanding kernel threads that are waiting for responses would compete for CPU resources, resulting in higher internal friction in the JVM that makes your application running slower. This is not a productive use of computer resources. This type of performance issue caused by internal friction is very difficult to avoid. While event driven and reactive programming that uses asynchronous processing and callbacks would address this artificial bottleneck, asynchronous code is harder to implement and maintain when the application complexity increases. It would be ideal if we can write sequential code that does not block. Sequential code is much easier to write and read because it communicates the intent of the code clearly. Leveraging Java 21 virtual thread technology, Mercury Composable allows the developer to write code in a sequential manner. When code in your function makes an RPC call to another service using the PostOffice's \"request\" API, it returns a Java Future object but the \"Future\" object itself is running in a virtual thread. This means when your code retrieves the RPC result using the \"get\" method, your code appears \"blocked\" while waiting for the response from the target service. Although your code appears to be \"blocked\", the virtual thread is \u201csuspended\u201d. It will wake up when the response arrives. When a virtual thread is suspended, it does not consume CPU time and the memory structure for keeping the thread in suspend mode is very small. Virtual thread technology is designed to support tens of thousands of concurrent RPC requests in a single compute machine, container or serverless instance. Mercury Composable supports mixed thread management - virtual threads and kernel threads. Functions running in different types of threads are connected loosely in events. This functional isolation and encapsulation mean that you can precisely control how your application performs for each functional logic block. Chapter-1 Home Chapter-3 Introduction Table of Contents REST Automation","title":"Solving the puzzle of multithreading performance"},{"location":"guides/CHAPTER-3/","text":"REST Automation The platform-core foundation library contains a built-in non-blocking HTTP server that you can use to create REST endpoints. Behind the curtain, it is using the vertx web client and server libraries. The REST automation system is not a code generator. The REST endpoints in the rest.yaml file are handled by the system directly - \"Config is the code\". We will use the \"rest.yaml\" sample configuration file in the \"lambda-example\" project to elaborate the configuration approach. The rest.yaml configuration has three sections: REST endpoint definition CORS header processing HTTP header transformation Turn on the REST automation engine REST automation is optional. To turn on REST automation, add or update the following parameters in the application.properties file (or application.yml if you like). rest.server.port=8085 rest.automation=true yaml.rest.automation=classpath:/rest.yaml When rest.automation=true , you can configure the server port using rest.server.port or server.port . REST automation can co-exist with Spring Boot. Please use rest.server.port for REST automation and server.port for Spring Boot. The yaml.rest.automation tells the system the location of the rest.yaml configuration file. Support of multiple configuration files You can configure more than one location and the system will search and merge them sequentially. The following example tells the system to merge the rest.yaml config files in the /tmp/config folder and the project's resources folder. yaml.rest.automation=file:/tmp/config/rest.yaml, classpath:/rest.yaml Duplicated REST endpoints The system will detect duplicated REST endpoint configuation. If there is a duplicated entry, it will abort the REST endpoint rendering. Your unit tests will fail because REST endpoints are not enabled. The application log may look like this: INFO - Loading config from classpath:/rest.yaml INFO - Loading config from classpath:/event-api.yaml ERROR - REST endpoint rendering aborted due to duplicated entry 'POST /api/event' in classpath:/event-api.yaml Please correct the rest.yaml configuration files and rebuild your application again. Duplicated static content, cors and headers sections When duplicated entry is detected, the subsequent one will replace the prior one. A warning will be shown in the application log like this: WARN - Duplicated 'static-content' in classpath:/duplicated-endpoint.yaml will override a prior one WARN - Duplicated 'cors' in classpath:/duplicated-endpoint.yaml will override a prior one 'cors_1' WARN - Duplicated 'headers' in classpath:/duplicated-endpoint.yaml will override a prior one 'header_1' Defining a REST endpoint The \"rest\" section of the rest.yaml configuration file may contain one or more REST endpoints. A REST endpoint may look like this: - service: [\"hello.world\"] methods: ['GET', 'PUT', 'POST', 'HEAD', 'PATCH', 'DELETE'] url: \"/api/hello/world\" timeout: 10s cors: cors_1 headers: header_1 authentication: 'v1.api.auth' tracing: true Syntax Parameter Usage Example service List of one or two route names of a service 'hello.world' ['primary.service', 'secondary.service'] methods List of one or two HTTP methods ['GET'] url URI path of the service '/api/hello/world' timeout Maximum time to wait for a REST response Default value is '30s' for 30 seconds. (\"s\" for seconds) cors Reference ID of a CORS section 'cors_1' headers Reference ID of a HEADERS transformation section 'header_1' authentication Optional . Route the HTTP request for authentication is provided. default is false tracing Enable distributed tracing when set to 'true' default is false When more than one service route name is provided, the first one is the primary service and the system will deliver its output as HTTP response. The second one is the secondary service for listening to the REST endpoint. Output from the secondary service will be ignored. When content length is not given, the system will render payload as a stream of bytes. The \"timeout\" value is the maximum time that REST endpoint will wait for a response from your function. If there is no response within the specified time interval, the user will receive an HTTP-408 timeout exception. The \"authentication\" parameter is optional. If configured, the route name given in the authentication parameter will be used. The input event will be delivered to the authentication function with the route name. In this example, it is \"v1.api.auth\". Your custom authentication function may look like this: @PreLoad(route = \"v1.api.auth\", instances = 10) public class SimpleAuthentication implements TypedLambdaFunction<AsyncHttpRequest, Object> { @Override public Object handleEvent(Map<String, String> headers, AsyncHttpRequest input, int instance) { // Your authentication logic here. The return value should be true or false. return result; } } Your authentication function can return a boolean value to indicate if the request should be accepted or rejected. If true, the system will send the HTTP request to the service. In this example, it is the \"hello.world\" function. If false, the user will receive an \"HTTP-401 Unauthorized\" exception. Optionally, you can use the authentication function to return some session information after authentication. For example, your authentication can forward the \"Authorization\" header of the incoming HTTP request to your organization's OAuth 2.0 Identity Provider for authentication. To return session information to the next function, the authentication function can return an EventEnvelope. It can set the session information as key-values in the response event headers. In the lambda-example application, there is a demo authentication function in the AuthDemo class with the \"v1.api.auth\" route name. To demonstrate passing session information, the AuthDemo class set the header \"user=demo\" in the result EventEnvelope. You can test this by visiting http://127.0.0.1:8085/api/hello/generic/1 to invoke the \"hello.generic\" function. The console will print: Telemetry:55 - trace={path=GET /api/hello/generic/1, service=v1.api.auth, success=true, origin=20230326f84dd5f298b64be4901119ce8b6c18be, exec_time=0.056, start=2023-03-26T20:08:01.702Z, from=http.request, id=aa983244cef7455cbada03c9c2132453, round_trip=1.347, status=200} HelloGeneric:56 - Got session information {user=demo} Telemetry:55 - trace={path=GET /api/hello/generic/1, service=hello.generic, success=true, origin=20230326f84dd5f298b64be4901119ce8b6c18be, start=2023-03-26T20:08:01.704Z, exec_time=0.506, from=v1.api.auth, id=aa983244cef7455cbada03c9c2132453, status=200} Telemetry:55 - trace={path=GET /api/hello/generic/1, service=async.http.response, success=true, origin=20230326f84dd5f298b64be4901119ce8b6c18be, start=2023-03-26T20:08:01.705Z, exec_time=0.431, from=hello.generic, id=aa983244cef7455cbada03c9c2132453, status=200} This illustrates that the HTTP request has been processed by the \"v1.api.auth\" function. The \"hello.generic\" function is wired to the \"/api/hello/generic/{id}\" endpoint as follows: - service: \"hello.generic\" methods: ['GET'] url: \"/api/hello/generic/{id}\" # Turn on authentication pointing to the \"v1.api.auth\" function authentication: \"v1.api.auth\" timeout: 20s cors: cors_1 headers: header_1 tracing: true The tracing parameter tells the system to turn on \"distributed tracing\". In the console log shown above, you see three lines of log from \"distributed trace\" showing that the HTTP request is processed by \"v1.api.auth\" and \"hello.generic\" before returning result to the browser using the \"async.http.response\" function. Note : The \"async.http.response\" is a built-in function to send the HTTP response to the browser. The term \"browser\" also refers to a caller from an application (\"client\"). Therefore, browser and client can be used interchangeably. The optional cors and headers sections point to the specific CORS and HEADERS sections respectively. CORS section For ease of development, you can define CORS headers using the CORS section like this. This is a convenient feature for development. For cloud native production system, it is most likely that CORS processing is done at the API gateway level. You can define different sets of CORS headers using different IDs. cors: - id: cors_1 options: - \"Access-Control-Allow-Origin: ${api.origin:*}\" - \"Access-Control-Allow-Methods: GET, DELETE, PUT, POST, PATCH, OPTIONS\" - \"Access-Control-Allow-Headers: Origin, Authorization, X-Session-Id, X-Correlation-Id, Accept, Content-Type, X-Requested-With\" - \"Access-Control-Max-Age: 86400\" headers: - \"Access-Control-Allow-Origin: ${api.origin:*}\" - \"Access-Control-Allow-Methods: GET, DELETE, PUT, POST, PATCH, OPTIONS\" - \"Access-Control-Allow-Headers: Origin, Authorization, X-Session-Id, X-Correlation-Id, Accept, Content-Type, X-Requested-With\" - \"Access-Control-Allow-Credentials: true\" HEADERS section The HEADERS section is used to do some simple transformation for HTTP request and response headers. You can add, keep or drop headers for HTTP request and response. Sample HEADERS section is shown below. headers: - id: header_1 request: # # headers to be inserted # add: [\"hello-world: nice\"] # # keep and drop are mutually exclusive where keep has precedence over drop # i.e. when keep is not empty, it will drop all headers except those to be kept # when keep is empty and drop is not, it will drop only the headers in the drop list # e.g. # keep: ['x-session-id', 'user-agent'] # drop: ['Upgrade-Insecure-Requests', 'cache-control', 'accept-encoding', 'connection'] # drop: ['Upgrade-Insecure-Requests', 'cache-control', 'accept-encoding', 'connection'] response: # # the system can filter the response headers set by a target service, # but it cannot remove any response headers set by the underlying servlet container. # However, you may override non-essential headers using the \"add\" directive. # i.e. don't touch essential headers such as content-length. # # keep: ['only_this_header_and_drop_all'] # drop: ['drop_only_these_headers', 'another_drop_header'] # # add: [\"server: mercury\"] # # You may want to add cache-control to disable browser and CDN caching. # add: [\"Cache-Control: no-cache, no-store\", \"Pragma: no-cache\", # \"Expires: Thu, 01 Jan 1970 00:00:00 GMT\"] # add: - \"Strict-Transport-Security: max-age=31536000\" - \"Cache-Control: no-cache, no-store\" - \"Pragma: no-cache\" - \"Expires: Thu, 01 Jan 1970 00:00:00 GMT\" Static content Static content (HTML/CSS/JS bundle), if any, can be placed in the \"resources/public\" folder in your application project root. It is because the default value for the \"static.html.folder\" parameter in the application configuration is \"classpath:/resources/public\". If you want to place your static content elsewhere, you may adjust this parameter. You may point it to the local file system such as \"file:/tmp/html\". For security reason, you may add the following configuration in the rest.yaml. The following example is shown in the unit test section of the platform-core library module. # # Optional static content handling for HTML/CSS/JS bundle # ------------------------------------------------------- # # no-cache-pages - tells the browser not to cache some specific pages # # The \"filter\" section is a programmatic way to protect certain static content. # # The filter can be used to inspect HTTP path, headers and query parameters. # The typical use case is to check cookies and perform browser redirection # for SSO login. Another use case is to selectively add security HTTP # response headers such as cache control and X-Frame-Options. You can also # perform HTTP to HTTPS redirection. # # Syntax for the \"no-cache-pages\", \"path\" and \"exclusion\" parameters are: # 1. Exact match - complete path # 2. Match \"startsWith\" - use a single \"*\" as the suffix # 3. Match \"endsWith\" - use a single \"*\" as the prefix # # If filter is configured, the path and service parameters are mandatory # and the exclusion parameter is optional. # # In the following example, it will intercept the home page, all contents # under \"/assets/\" and any files with extensions \".html\" and \".js\". # It will ignore all CSS files. # static-content: no-cache-pages: [\"/\", \"/index.html\"] filter: path: [\"/\", \"/assets/*\", \"*.html\", \"*.js\"] exclusion: [\"*.css\"] service: \"http.request.filter\" The sample request filter function is available in the platform-core project like this: @PreLoad(route=\"http.request.filter\", instances=100) public class GetRequestFilter implements TypedLambdaFunction<AsyncHttpRequest, EventEnvelope> { @Override public EventEnvelope handleEvent(Map<String, String> headers, AsyncHttpRequest input, int instance) { return new EventEnvelope().setHeader(\"x-filter\", \"demo\"); } } In the above http.request.filter, it adds a HTTP response header \"X-Filter\" for the unit test to validate. If you set status code in the return EventEnvelope to 302 and add a header \"Location\", the system will redirect the browser/client to the given URL in the location header. Please be careful to avoid HTTP redirection loop. Similarly, you can throw exception and the HTTP request will be rejected with the given status code and error message accordingly. Chapter-2 Home Chapter-4 Function Execution Strategies Table of Contents Event Script Syntax","title":"Chapter-3"},{"location":"guides/CHAPTER-3/#rest-automation","text":"The platform-core foundation library contains a built-in non-blocking HTTP server that you can use to create REST endpoints. Behind the curtain, it is using the vertx web client and server libraries. The REST automation system is not a code generator. The REST endpoints in the rest.yaml file are handled by the system directly - \"Config is the code\". We will use the \"rest.yaml\" sample configuration file in the \"lambda-example\" project to elaborate the configuration approach. The rest.yaml configuration has three sections: REST endpoint definition CORS header processing HTTP header transformation","title":"REST Automation"},{"location":"guides/CHAPTER-3/#turn-on-the-rest-automation-engine","text":"REST automation is optional. To turn on REST automation, add or update the following parameters in the application.properties file (or application.yml if you like). rest.server.port=8085 rest.automation=true yaml.rest.automation=classpath:/rest.yaml When rest.automation=true , you can configure the server port using rest.server.port or server.port . REST automation can co-exist with Spring Boot. Please use rest.server.port for REST automation and server.port for Spring Boot. The yaml.rest.automation tells the system the location of the rest.yaml configuration file.","title":"Turn on the REST automation engine"},{"location":"guides/CHAPTER-3/#support-of-multiple-configuration-files","text":"You can configure more than one location and the system will search and merge them sequentially. The following example tells the system to merge the rest.yaml config files in the /tmp/config folder and the project's resources folder. yaml.rest.automation=file:/tmp/config/rest.yaml, classpath:/rest.yaml","title":"Support of multiple configuration files"},{"location":"guides/CHAPTER-3/#duplicated-rest-endpoints","text":"The system will detect duplicated REST endpoint configuation. If there is a duplicated entry, it will abort the REST endpoint rendering. Your unit tests will fail because REST endpoints are not enabled. The application log may look like this: INFO - Loading config from classpath:/rest.yaml INFO - Loading config from classpath:/event-api.yaml ERROR - REST endpoint rendering aborted due to duplicated entry 'POST /api/event' in classpath:/event-api.yaml Please correct the rest.yaml configuration files and rebuild your application again.","title":"Duplicated REST endpoints"},{"location":"guides/CHAPTER-3/#duplicated-static-content-cors-and-headers-sections","text":"When duplicated entry is detected, the subsequent one will replace the prior one. A warning will be shown in the application log like this: WARN - Duplicated 'static-content' in classpath:/duplicated-endpoint.yaml will override a prior one WARN - Duplicated 'cors' in classpath:/duplicated-endpoint.yaml will override a prior one 'cors_1' WARN - Duplicated 'headers' in classpath:/duplicated-endpoint.yaml will override a prior one 'header_1'","title":"Duplicated static content, cors and headers sections"},{"location":"guides/CHAPTER-3/#defining-a-rest-endpoint","text":"The \"rest\" section of the rest.yaml configuration file may contain one or more REST endpoints. A REST endpoint may look like this: - service: [\"hello.world\"] methods: ['GET', 'PUT', 'POST', 'HEAD', 'PATCH', 'DELETE'] url: \"/api/hello/world\" timeout: 10s cors: cors_1 headers: header_1 authentication: 'v1.api.auth' tracing: true Syntax Parameter Usage Example service List of one or two route names of a service 'hello.world' ['primary.service', 'secondary.service'] methods List of one or two HTTP methods ['GET'] url URI path of the service '/api/hello/world' timeout Maximum time to wait for a REST response Default value is '30s' for 30 seconds. (\"s\" for seconds) cors Reference ID of a CORS section 'cors_1' headers Reference ID of a HEADERS transformation section 'header_1' authentication Optional . Route the HTTP request for authentication is provided. default is false tracing Enable distributed tracing when set to 'true' default is false When more than one service route name is provided, the first one is the primary service and the system will deliver its output as HTTP response. The second one is the secondary service for listening to the REST endpoint. Output from the secondary service will be ignored. When content length is not given, the system will render payload as a stream of bytes. The \"timeout\" value is the maximum time that REST endpoint will wait for a response from your function. If there is no response within the specified time interval, the user will receive an HTTP-408 timeout exception. The \"authentication\" parameter is optional. If configured, the route name given in the authentication parameter will be used. The input event will be delivered to the authentication function with the route name. In this example, it is \"v1.api.auth\". Your custom authentication function may look like this: @PreLoad(route = \"v1.api.auth\", instances = 10) public class SimpleAuthentication implements TypedLambdaFunction<AsyncHttpRequest, Object> { @Override public Object handleEvent(Map<String, String> headers, AsyncHttpRequest input, int instance) { // Your authentication logic here. The return value should be true or false. return result; } } Your authentication function can return a boolean value to indicate if the request should be accepted or rejected. If true, the system will send the HTTP request to the service. In this example, it is the \"hello.world\" function. If false, the user will receive an \"HTTP-401 Unauthorized\" exception. Optionally, you can use the authentication function to return some session information after authentication. For example, your authentication can forward the \"Authorization\" header of the incoming HTTP request to your organization's OAuth 2.0 Identity Provider for authentication. To return session information to the next function, the authentication function can return an EventEnvelope. It can set the session information as key-values in the response event headers. In the lambda-example application, there is a demo authentication function in the AuthDemo class with the \"v1.api.auth\" route name. To demonstrate passing session information, the AuthDemo class set the header \"user=demo\" in the result EventEnvelope. You can test this by visiting http://127.0.0.1:8085/api/hello/generic/1 to invoke the \"hello.generic\" function. The console will print: Telemetry:55 - trace={path=GET /api/hello/generic/1, service=v1.api.auth, success=true, origin=20230326f84dd5f298b64be4901119ce8b6c18be, exec_time=0.056, start=2023-03-26T20:08:01.702Z, from=http.request, id=aa983244cef7455cbada03c9c2132453, round_trip=1.347, status=200} HelloGeneric:56 - Got session information {user=demo} Telemetry:55 - trace={path=GET /api/hello/generic/1, service=hello.generic, success=true, origin=20230326f84dd5f298b64be4901119ce8b6c18be, start=2023-03-26T20:08:01.704Z, exec_time=0.506, from=v1.api.auth, id=aa983244cef7455cbada03c9c2132453, status=200} Telemetry:55 - trace={path=GET /api/hello/generic/1, service=async.http.response, success=true, origin=20230326f84dd5f298b64be4901119ce8b6c18be, start=2023-03-26T20:08:01.705Z, exec_time=0.431, from=hello.generic, id=aa983244cef7455cbada03c9c2132453, status=200} This illustrates that the HTTP request has been processed by the \"v1.api.auth\" function. The \"hello.generic\" function is wired to the \"/api/hello/generic/{id}\" endpoint as follows: - service: \"hello.generic\" methods: ['GET'] url: \"/api/hello/generic/{id}\" # Turn on authentication pointing to the \"v1.api.auth\" function authentication: \"v1.api.auth\" timeout: 20s cors: cors_1 headers: header_1 tracing: true The tracing parameter tells the system to turn on \"distributed tracing\". In the console log shown above, you see three lines of log from \"distributed trace\" showing that the HTTP request is processed by \"v1.api.auth\" and \"hello.generic\" before returning result to the browser using the \"async.http.response\" function. Note : The \"async.http.response\" is a built-in function to send the HTTP response to the browser. The term \"browser\" also refers to a caller from an application (\"client\"). Therefore, browser and client can be used interchangeably. The optional cors and headers sections point to the specific CORS and HEADERS sections respectively.","title":"Defining a REST endpoint"},{"location":"guides/CHAPTER-3/#cors-section","text":"For ease of development, you can define CORS headers using the CORS section like this. This is a convenient feature for development. For cloud native production system, it is most likely that CORS processing is done at the API gateway level. You can define different sets of CORS headers using different IDs. cors: - id: cors_1 options: - \"Access-Control-Allow-Origin: ${api.origin:*}\" - \"Access-Control-Allow-Methods: GET, DELETE, PUT, POST, PATCH, OPTIONS\" - \"Access-Control-Allow-Headers: Origin, Authorization, X-Session-Id, X-Correlation-Id, Accept, Content-Type, X-Requested-With\" - \"Access-Control-Max-Age: 86400\" headers: - \"Access-Control-Allow-Origin: ${api.origin:*}\" - \"Access-Control-Allow-Methods: GET, DELETE, PUT, POST, PATCH, OPTIONS\" - \"Access-Control-Allow-Headers: Origin, Authorization, X-Session-Id, X-Correlation-Id, Accept, Content-Type, X-Requested-With\" - \"Access-Control-Allow-Credentials: true\"","title":"CORS section"},{"location":"guides/CHAPTER-3/#headers-section","text":"The HEADERS section is used to do some simple transformation for HTTP request and response headers. You can add, keep or drop headers for HTTP request and response. Sample HEADERS section is shown below. headers: - id: header_1 request: # # headers to be inserted # add: [\"hello-world: nice\"] # # keep and drop are mutually exclusive where keep has precedence over drop # i.e. when keep is not empty, it will drop all headers except those to be kept # when keep is empty and drop is not, it will drop only the headers in the drop list # e.g. # keep: ['x-session-id', 'user-agent'] # drop: ['Upgrade-Insecure-Requests', 'cache-control', 'accept-encoding', 'connection'] # drop: ['Upgrade-Insecure-Requests', 'cache-control', 'accept-encoding', 'connection'] response: # # the system can filter the response headers set by a target service, # but it cannot remove any response headers set by the underlying servlet container. # However, you may override non-essential headers using the \"add\" directive. # i.e. don't touch essential headers such as content-length. # # keep: ['only_this_header_and_drop_all'] # drop: ['drop_only_these_headers', 'another_drop_header'] # # add: [\"server: mercury\"] # # You may want to add cache-control to disable browser and CDN caching. # add: [\"Cache-Control: no-cache, no-store\", \"Pragma: no-cache\", # \"Expires: Thu, 01 Jan 1970 00:00:00 GMT\"] # add: - \"Strict-Transport-Security: max-age=31536000\" - \"Cache-Control: no-cache, no-store\" - \"Pragma: no-cache\" - \"Expires: Thu, 01 Jan 1970 00:00:00 GMT\"","title":"HEADERS section"},{"location":"guides/CHAPTER-3/#static-content","text":"Static content (HTML/CSS/JS bundle), if any, can be placed in the \"resources/public\" folder in your application project root. It is because the default value for the \"static.html.folder\" parameter in the application configuration is \"classpath:/resources/public\". If you want to place your static content elsewhere, you may adjust this parameter. You may point it to the local file system such as \"file:/tmp/html\". For security reason, you may add the following configuration in the rest.yaml. The following example is shown in the unit test section of the platform-core library module. # # Optional static content handling for HTML/CSS/JS bundle # ------------------------------------------------------- # # no-cache-pages - tells the browser not to cache some specific pages # # The \"filter\" section is a programmatic way to protect certain static content. # # The filter can be used to inspect HTTP path, headers and query parameters. # The typical use case is to check cookies and perform browser redirection # for SSO login. Another use case is to selectively add security HTTP # response headers such as cache control and X-Frame-Options. You can also # perform HTTP to HTTPS redirection. # # Syntax for the \"no-cache-pages\", \"path\" and \"exclusion\" parameters are: # 1. Exact match - complete path # 2. Match \"startsWith\" - use a single \"*\" as the suffix # 3. Match \"endsWith\" - use a single \"*\" as the prefix # # If filter is configured, the path and service parameters are mandatory # and the exclusion parameter is optional. # # In the following example, it will intercept the home page, all contents # under \"/assets/\" and any files with extensions \".html\" and \".js\". # It will ignore all CSS files. # static-content: no-cache-pages: [\"/\", \"/index.html\"] filter: path: [\"/\", \"/assets/*\", \"*.html\", \"*.js\"] exclusion: [\"*.css\"] service: \"http.request.filter\" The sample request filter function is available in the platform-core project like this: @PreLoad(route=\"http.request.filter\", instances=100) public class GetRequestFilter implements TypedLambdaFunction<AsyncHttpRequest, EventEnvelope> { @Override public EventEnvelope handleEvent(Map<String, String> headers, AsyncHttpRequest input, int instance) { return new EventEnvelope().setHeader(\"x-filter\", \"demo\"); } } In the above http.request.filter, it adds a HTTP response header \"X-Filter\" for the unit test to validate. If you set status code in the return EventEnvelope to 302 and add a header \"Location\", the system will redirect the browser/client to the given URL in the location header. Please be careful to avoid HTTP redirection loop. Similarly, you can throw exception and the HTTP request will be rejected with the given status code and error message accordingly. Chapter-2 Home Chapter-4 Function Execution Strategies Table of Contents Event Script Syntax","title":"Static content"},{"location":"guides/CHAPTER-4/","text":"Event Script Syntax Event Script is a Domain Specific Language (DSL) that uses YAML to represent an end-to-end transaction flow. A transaction is a business use case, and the flow can be an API service, a batch job or a real-time transaction. Flow list This configuration file sits in the project \"resources\" project and contains a list of filenames. The default flow list is \"flows.yaml\" under the \"resources\" folder. It may look like this. flows: - 'get-profile.yml' - 'create-profile.yml' - 'delete-profile.yml' location: 'classpath:/flows/' The \"location\" parameter is optional. If present, you can tell the system to load the flow config files from another folder location. Multiple flow lists You can provide more than one flow list to your application and it can become very handy under different situations. For instance, to achieve better modularity in complex application, flows can be grouped to multiple categories based on development team's choice and these flows can be managed in multiple flow lists. Another great place to use multiple flow list is to include external libraries which contain pre-defined flow lists. The following example demonstrates that an application loads a list of flows defined in \"flows.yaml\" and additional flows defined in \"more-flows.yaml\" file of a composable library. yaml.flow.automation=classpath:/flows.yaml, classpath:/more-flows.yaml Writing new REST endpoint and function You can use the \"composable-example\" subproject as a template to write your own composable application. For each filename in the flows.yml, you should create a corresponding configuration file under the \"resources/flows\" folder. Let's write a new flow called \"greetings\". You can copy-n-paste the following into a file called \"greetings.yml\" under the \"resources/flows\" folder. flow: id: 'greetings' description: 'Simplest flow' ttl: 10s first.task: 'greeting.demo' tasks: - input: - 'input.path_parameter.user -> user' process: 'greeting.demo' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: end In the application.properties, you can specify the following parameter: yaml.flow.automation=classpath:/flows.yaml and update the \"flows.yaml\" file in the resources folder as follows: flows: - 'get-profile.yml' - 'create-profile.yml' - 'delete-profile.yml' - 'greetings.yml' Then, you can add a new REST endpoint in the \"rest.yaml\" configuration file like this. - service: \"http.flow.adapter\" methods: ['GET'] url: \"/api/greetings/{user}\" flow: 'greetings' timeout: 10s cors: cors_1 headers: header_1 The above REST endpoint takes the path parameter \"user\". The task executor will map the path parameter to the input arguments (headers and body) in your function. Now you can write your new function with the named route \"greeting.demo\". Please copy-n-paste the following into a Java class called \"Greetings\" and save in the package under \"my.organization.tasks\" in the source project. Note : The package name, \"my.organization\", is an example. Please replace it with your organization package path. @PreLoad(route=\"greeting.demo\", instances=10, isPrivate = false) public class Greetings implements TypedLambdaFunction<Map<String, Object>, Map<String, Object>> { private static final String USER = \"user\"; @Override public Map<String, Object> handleEvent(Map<String, String> headers, Map<String, Object> input, int instance) { if (input.containsKey(USER)) { String user = input.get(USER).toString(); Map<String, Object> result = new HashMap<>(); result.put(USER, user); result.put(\"message\", \"Welcome\"); result.put(\"time\", new Date()); return result; } else { throw new IllegalArgumentException(\"Missing path parameter 'user'\"); } } } For the flow-engine to find your new function, please update the key-value for \"web.component.scan\" in application.properties: web.component.scan=my.organization To test your new REST endpoint, flow configuration and function, please point your browser to http://127.0.0.1:8100/api/greetings/my_name You can replace \"my_name\" with your first name to see the response to the browser. Flow configuration syntax In your \"greetings.yml\" file above, you find the following key-values: flow.id - Each flow must have a unique flow ID. The flow ID is usually originated from a user facing endpoint through an event adapter. For example, you may write an adapter to listen to a cloud event in a serverless deployment. In The most common one is the HTTP adapter. The flow ID is originated from the \"rest.yaml\". The flow-engine will find the corresponding flow configuration and create a new flow instance to process the user request. flow.description - this describes the purpose of the flow flow.ttl - \"Time to live (TTL)\" timer for each flow. You can define the maximum time for a flow to finish processing. All events are delivered asynchronously and there is no timeout value for each event. The TTL defines the time budget for a complete end-to-end flow. Upon expiry, an unfinished flow will be aborted. You can use suffix \"s\" for seconds, \"m\" for minutes and \"h\" for hours. e.g. \"30s\" for 30 seconds. Note : When using the HTTP Flow Adapter, the flow.ttl value can be higher than the REST endpoint's timeout value. This would happen when one of your tasks in the event flow responds to the caller and the event flow continues to execute the rest of the flow. This type of task is called \"response\" task. first.task - this points to the route name of a function (aka \"task\") to which the flow engine will deliver the incoming event. The configuration file contains a list of task entries where each task is defined by \"input\", \"process\", \"output\" and \"execution\" type. In the above example, the execution type is \"end\", meaning that it is the end of a transaction and its result set will be delivered to the user. Underlying Event System The Event Script system uses platform-core as the event system where it encapsulates Java Virtual Threads and Eclipse Vertx. The integration points are intentionally minimalist. For most use cases, the user application does not need to make any API calls to the underlying event system. REST automation and HTTP flow adapter The most common transaction entry point is a REST endpoint. The event flow may look like this: REQUEST -> \"http.request\" -> \"task.executor\" -> user defined tasks -> \"async.http.response\" -> RESPONSE REST automation is part of the platform-core library. It contains a non-blocking HTTP server that converts HTTP requests and responses into events. It routes an HTTP request event to the HTTP adapter if the \"flow\" tag is provided. In the following example, the REST endpoint definition is declared in a \"rest.yaml\" configuration. It will route the URI \"/api/decision\" to the HTTP flow adapter that exposes its service route name as \"http.flow.adapter\". rest: - service: \"http.flow.adapter\" methods: ['GET'] url: \"/api/decision?decision=_\" flow: 'decision-test' timeout: 10s cors: cors_1 headers: header_1 tracing: true The \"cors\" and \"headers\" sections are optional. When specified, the REST endpoint will insert CORS headers and HTTP request headers accordingly. For REST automation syntax, please refer to Chapter 3 The HTTP flow adapter maps the HTTP request dataset and the flow ID into a standard event envelope for delivery to the flow engine. The HTTP request dataset, addressable with the \"input.\" namespace, contains the following: Key Values method HTTP method uri URI path header HTTP headers cookie HTTP cookies path_parameter Path parameters if any query HTTP query parameters if any body HTTP request body if any stream input stream route ID if any ip remote IP address filename filename if request is a multipart file upload session authenticated session key-values if any For easy matching, please use lower case for headers, cookies, query and path parameters. Regular API uses JSON and XML and they will be converted to a hash map in the event's body. For special use cases like file upload/download, your application logic may invoke a streaming API to retrieve the binary payload. Please refer to Appendix-III Task is a composable function Each task in a flow must have a corresponding composable function. You can assign a task name to the function using the Preload annotation like this. @PreLoad(route=\"greeting.demo\", instances=10) public class Greetings implements TypedLambdaFunction<Map<String, Object>, Map<String, Object>> { @Override public Map<String, Object> handleEvent(Map<String, String> headers, Map<String, Object> input, int instance) { // business logic here return someOutput; } } The \"route\" in the Preload annotation is the task name. The \"instances\" define the maximum number of \"workers\" that the function can handle concurrently. The system is designed to be reactive and the function does not consume memory and CPU resources until an event arrives. You may also define concurrency using environment variable. You can replace the \"instances\" with envInstances using standard environment variable syntax like ${SOME_ENV_VARIABLE:default_value} . PoJo serialization strategies The default serialization strategy is defined in application.properties as the snake.case.serialization parameter. Snake case serialization will be used when this parameter is set to true. Otherwise, camel case serialization will be used. You can override the default serialization strategy in 2 ways in the PreLoad annotation. Configure input / output serialization strategies Implement your own custom serializer using the CustomSerializer interface @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface PreLoad { String route(); Class<?> customSerializer() default Void.class; Class<?> inputPojoClass() default Void.class; int instances() default 1; String envInstances() default \"\"; boolean isPrivate() default true; SerializationStrategy inputStrategy() default SerializationStrategy.DEFAULT; SerializationStrategy outputStrategy() default SerializationStrategy.DEFAULT; } To instruct the composable function to deserialize input as a PoJo and serialize output PoJo, you can set the \"inputStrategy\" and/or \"outputStrategy\" parameters in the PreLoad annotation. For example, when you set the inputStrategy to SerializationStrategy.CAMEL, it will override the default snake case serialization. This is useful when your function receives input from an external source that you have no control of the serialization strategy. Similarly, when you are using default snake case serialization, you can set outputStrategy to SerializationStrategy.CAMEL in the last function in a flow so that camel case output will be delivered to an external target. If your PoJo requires special treatment and the built-in preconfigured serializer does not handle your use case, you can implement your own custom serializer. In this case, the inputStrategy and outputStrategy will be ignored. Unique task naming Composable functions are designed to be reusable. By changing some input data mapping to feed different parameters and payload, your function can behave differently. Therefore, it is quite common to use the same function (i.e. the process parameter) more than once in a single event flow. When a task is not named, the \"process\" parameter is used to name the task. Since each task must have a unique name for event routing, we cannot use the same \"process\" name more than once in an event flow. To handle this use case, you can create unique names for the same function using the name parameter like this: flow: id: 'greetings' description: 'Simplest flow' ttl: 10s first.task: 'my.first.task' tasks: - name: 'my.first.task' input: - 'input.path_parameter.user -> user' process: 'greeting.demo' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: sequential next: - 'another.task' The above event flow configuration uses \"my.first.task\" as a named route for \"greeting.demo\" by adding the \"name\" parameter to the composable function. Note : The Event Manager performs event choreography using the unique task name. Therefore, when the \"process\" name for the function is not unique, you must create unique task \"names\" for the same function to ensure correct routing. Assigning multiple route names to a single function The built-in distributed tracing system tracks the actual composable functions using the \"process\" name and not the task names. When there is a need to track the task names in distributed trace, you can tell the system to create additional instances of the same function with different route names. You can use a comma separated list as the route name like this: @PreLoad(route=\"greeting.case.1, greeting.case.2\", instances=10) public class Greetings implements TypedLambdaFunction<Map<String, Object>, Map<String, Object>> { @Override public Map<String, Object> handleEvent(Map<String, String> headers, Map<String, Object> input, int instance) { // business logic here return someResult; } } Note : The \"unique task naming\" method is more memory efficient than creating additional route names Preload overrides Once a composable function is published as a reusable library in the artifactory, its route name and number of instances are fixed using the \"PreLoad\" annotation in the function class. Without refactoring your libary, you can override its route name and instances using a preload override file like this: preload: - original: 'greeting.demo' routes: - 'greeting.case.1' - 'greeting.case.2' # the \"instances\" tag is optional instances: 20 - original: 'v1.another.reusable.function' keep-original: true routes: - 'v1.reusable.1' - 'v1.reusable.2' In the above example, the function associated with \"greeting.demo\" will be preloaded as \"greeting.case.1\" and \"greeting.case.2\". The number of maximum concurrent instances is also changed from 10 to 20. In the second example, \"v1.another.reusable.function\" is updated as \"v1.reusable.1\" and \"v1.reusable.2\" and the number of concurrent instances is not changed. The original route \"v1.another.reusable.function\" is preserved when the \"keep-original\" parameter is set to true. Assuming the above file is \"preload-override.yaml\" in the \"resources\" folder of the application source code project, you should add the following parameter in application.properties to activate this preload override feature. yaml.preload.override=classpath:/preload-override.yaml Multiple preload override config files When you publish a composable function as a library, you may want to ensure the route names of the functions are merged properly. In this case, you can bundle a library specific preload override config file. For example, your library contains a \"preload-kafka.yaml\" to override some route names, you can add it to the yaml.preload.override parameter like this: yaml.preload.override=classpath:/preload-override.yaml, classpath:/preload-kafka.yaml The system will then merge the two preload override config files. The concurrency value of a function is overwritten using the \"instances\" parameter in the first preload override file. Subsequent override of the \"instances\" parameter is ignored. i.e. the first preload override file will take precedence. Hierarchy of flows As shown in Figure 1, you can run one or more sub-flows inside a primary flow. Figure 1 - Hierarchy of flows To do this, you can use the flow protocol identifier ( flow:// ) to indicate that the task is a flow. For example, when running the following task, \"flow://my-sub-flow\" will be executed like a regular task. tasks: - input: - 'input.path_parameter.user -> header.user' - 'input.body -> body' process: 'flow://my-sub-flow' output: - 'result -> model.pojo' description: 'Execute a sub-flow' execution: sequential next: - 'my.next.function' If the sub-flow is not available, the system will throw an error stating that it is not found. Hierarchy of flows would reduce the complexity of a single flow configuration file. The \"time-to-live (TTL)\" value of the parent flow should be set to a value that covers the complete flow including the time used in the sub-flows. In the input/output data mapping sections, the configuration management system provides a parent state machine using the namespace model.parent. to be shared by the primary flow and all sub-flows that are instantiated from it. Just like a task, a subflow has \"input\" and \"output\". You can map data to the \"input\" of a subflow using the namespaces \"body\" and \"header\" where they are maps of key-values. Inside a task of the subflow, the body and header namespaces can be accessed for their key-values like this: - input: - 'input.header.user -> header.user' - 'input.body -> *' process: 'first.task.in.subflow' output: - 'result -> model.parent.subflow_result' description: 'Execute a task in a subflow' execution: end Since the parent flow and subflows has a shared state machine, passing \"body\" and \"header\" key-values to the \"input\" of a subflow is optional. You can pass key-values between the parent and subflows using the shared state machine easily. Note : The namespace model.root. is an alias of model.parent. This would reduce ambiguity if you prefer to use \"root\" referring to the parent flow that creates one or more subflows. Tasks and data mapping All tasks for a flow are defined in the \"tasks\" section. Input/Output data mapping A function is self-contained. This modularity reduces application complexity because the developer only needs interface contract details for a specific function. To handle this level of modularity, the system provides configurable input/output data mapping. Namespaces for I/O data mapping Type Keyword and/or namespace LHS / RHS Mappings Flow input dataset input. left input Flow error dataset error. left. input Flow output dataset output. right output Function input body no namespace required right input Function input or output headers header or header. both I/O Function output result set result. left output Function output status code status left output Function output pojo class name datatype left output Decision value decision right output State machine dataset model. both I/O Parent state machine dataset model.parent. both I/O Alias for parent state machine model.root. both I/O External state machine key-value ext: right I/O For state machine (model and model.parent namespaces), the system prohibits access to the whole namespace. You should only access specific key-values in the model or model.parent namespaces. The namespace model.root. or model.parent. is shared by the primary flow and all sub-flows that are instantiated from it. When your function returns a PoJo, the datatype field in the left-hand-side will contain the class name of the PoJo. This allows you to save the class name in the state machine and pass it to another task that needs to reconstruct the PoJo class. This is used when your function may return different PoJo classes for different scenarios. The error dataset is available in the input data mapping of an exception handler that attaches to a task or the generic exception handler that attaches to the flow itself. The error dataset includes the following: error.task - this is the task name of the task that throws exception error.status - the status code of the exception error.message - the error message error.stack - stack trace if any The external state machine namespace uses the namespace ext: to indicate that the key-value is external. Constants for input data mapping Type Keyword for the left-hand-side argument String text(example_value) Integer int(number) Long long(number) Float float(number) Double double(number) Boolean boolean(true or false) Map map(k1=v1, k2=v2) map(base.config.parameter) File file(text:file_path) File file(binary:file_path) File file(json:file_path) Classpath classpath(text:file_path) Classpath classpath(binary:file_path) Classpath classpath(json:file_path) For input data mapping, the \"file\" constant type is used to load some file content as an argument of a user function. You can tell the system to render the file as \"text\", \"binary\" or \"json\". Similarly, the \"classpath\" constant type refers to static file in the application source code's \"resources\" folder. When file type mapping is \"json\", the file content will be rendered as a Map or a List from a JSON string. The \"map\" constant type is used for two purposes: 1. Map of key-values The following example illustrates creation of a map of key-values. In the first entry, a map of 2 key-values is set as the input argument \"myMap\" of a user function. In the second entry, the map's values are retrieved from the key \"some.key\" in base configuration and the environment variable \"ENV_VAR_ONE\". 'map(k1=v1, k2=v2) -> myMap' 'map(k1=${some.key}, k2=${ENV_VAR_ONE}) -> myMap' Note : The comma character is used as a separator for each key-value pair. If the value contains a comma, the system cannot parse the key-values correctly. In this case, please use the 2nd method below. 2. Mapping values from application.yml The following input data mapping sets the value of \"my.key\" from the application.yml base configuration file to the input argument \"myKey\" of a user function. 'map(my.key) -> myKey' Since the system uses both application.properties and application.yml as base configuration files, you can use either configuration files depending on the data type of the value. For application.properties, \"map(my.key)\" is the same as \"text(${my.key})\". For application.yml, \"map(my.key)\" would set a primitive value (text, integer, float, boolean), a hash map of key-values or an array of values. Special content type for output data mapping Type Keyword for the right-hand-side argument File file(file_path) File file(append:file_path) For output data mapping, the \"file\" content type is used to save some data from the output of a user function to a file in the local file system. If the left-hand-side (LHS) resolved value is null, the file in the RHS will be deleted. This allows you to clean up temporary files before your flow finishes. An optional prefix \"append\" may be used to tell the system to append file content instead of overwriting it. Note : The local file system write operation is not thread-safe. If you have parallel tasks appending to the same file, the integrity of file content is not guaranteed. One way to ensure thread safety is to use singleton pattern. This can be done by setting the number of instances of the task writing to the local file system to 1. Decision value The \"decision\" keyword applies to \"right hand side\" of output data mapping statement in a decision task only (See \"Decision\" in the task section). Each flow has its own input and output Each function has its input headers, input body and output result set. Optionally, a function can return an EventEnvelope object to hold its result set in the \"body\", a \"status\" code and one or more header key-values. Since each function is stateless, a state machine (with namespace model. ) is available as a temporary memory store for transaction states that can be passed from one task to another. All variables are addressable using the standard dot-bracket convention. For example, \"hello.world\" will retrieve the value 100 from this data structure: { \"hello\": { \"world\": 100 } } and \"numbers[1]\" will retrieve the value 200 below: { \"numbers\": [100, 200] } The assignment is done using the assignment ( -> ) syntax. In the following example, the HTTP input query parameter 'amount' is passed as input body argument 'amount' to the task 'simple.decision'. The result (function \"return value\") from the task will be mapped to the special \"decision\" variable that the flow engine will evaluate. This assumes the result is a boolean or numeric value. The \"decision\" value is also saved to the state machine ( model ) for subsequent tasks to evaluate. - input: - 'input.query.amount -> amount' process: 'simple.decision' output: - 'result -> decision' - 'result -> model.decision' Environment variables You can use the standard ${ENV_VAR:default} syntax to resolve environment variables or parameters from the application.properties. Runtime model variables To use a runtime model variable value as a key or constant, you can use the {model.variable_name} syntax. For example, - input: - 'text(wonderful day) -> model.world' - 'text(world) -> model.pointer' - 'model.{model.pointer} -> value1' - 'text(new {model.pointer}) -> value2' - 'text(keep {this}/{one} unchanged) -> value3' process: 'demo.function' model.{model.pointer} is resolved as model.world , giving value1 = wonderful day and value2 = new world . The text inside a set of brackets that is not a model variable will be kept unchanged, thus value3 = keep {this}/{one} unchanged The use of string substitution is subject to event script syntax validation. Therefore, When this feature is used in the left-hand-side of an input data mapping, it can be used to substitute a constant or a segment of a key in the input. and model. namespaces. The above example shows the use of the model namespace in model.{model.pointer} -> value1 . Similarly, when used in the left-hand-side of an output data mapping, it can be used to substitute a constant or a segment of a key in the input. , model. , header. or result. namespaces. When used in the right-hand-side of an input data mapping, namespace is optional because it may map as an argument to a task. When used in the right-hand-side of an output data mapping, it can be used to substitute a model. namespace, file( output, flow output. namespace or an external state machine ext: namespace. Important : For security reason, the key inside the brackets must be a model variable. The resolved value from a model variable must be either text or number. Otherwise, it will be converted to a value of \"null\". For simplicity, nested substitution is not allowed. i.e. model.{model.{model.n}} or model.{model.list[model.n]} will be rejected. If the bracketed text is not a model variable, the brackets and the enclosed text will be kept unchanged. Handling arrays in a dataset An array of data elements is expressed as a list. { \"numbers\": [100, 200] } As discussed earlier, an array element can be retrieved using a number as index. For example, to take the second element with value 200 above, you can use this data mapping like this: - 'input.body.numbers[1] -> second_number' In the above example, it is an \"input data mapping\". It maps the second element of value 200 as the input argument \"second_number\" to a composable function. For-loop feature is supported in pipeline in an event flow. It would be convenient to use the iterator value as an index to map an input argument. We can do something like this: - 'input.body.numbers[model.n] -> second_number' where model.n is the iterator value in a for-loop. Similarly, it is possible to do output data mapping. For example, - 'result.computed -> model.list[model.n]' To address an array element, we can use a number or a \"dynamic model variable\" as an index. The model variable must resolved to a number. Note : There are some consideration when using a dynamic model variable as an index. The left-hand-side of a data mapping is a GET operation. The right-hand-side is a SET operation. If the model variable is non-numeric, the GET operation will return null and SET operation will throw exception. To avoid setting an arbitrary high index, the size of the index is limited by the parameter \"max.model.array.size\" in application.properties or application.yml Append an element to an array An empty array index in the right hand side tells the system to append an element to an array. For example, the value resolved from the left hand side \"result.item1\" and \"result.item2\" will be appended to the model.items array in the state machine. - 'result.item1 -> model.items[]' - 'result.item2 -> model.items[]' If model.items does not exist, the first element will be set as array index \"0\". Therefore, the above output data mapping statements are the same as: - 'result.item1 -> model.items[0]' - 'result.item2 -> model.items[1]' Simple type matching and conversion Event script's state machine supports simple type matching and conversion for the model namespace. This \"impedance matching\" feature allows us to accommodate minor interface contract changes without refactoring business logic of a user function. This is supported in both the left-hand-side and right-hand-side of both input and output data mappings. For the left-hand-side, the state machine's model value is matched or converted to the target data type before setting the value of the right-hand-side. The state machine values are unchanged. For the right-hand-side, the matched or converted value is applied to the state machine's model value. The syntax is model.somekey:type where \"type\" is one of the following: Type Match value as Example text text string model.someKey:text binary byte array model.someKey:binary int integer or -1 if not numeric model.someKey:int long long or -1 if not numeric model.someKey:long float float or -1 if not numeric model.someKey:float double double or -1 if not numeric model.someKey:double boolean true or false model.someKey:boolean boolean(value) true if value matches model.someKey:boolean(positive) boolean(value=true) true if value matches model.someKey:boolean(positive=true) boolean(value=false) false if value matches model.someKey:boolean(negative=false) and(model.key) boolean AND of 2 model keys model.someKey:and(model.another) or(model.key) boolean OR of 2 model keys model.someKey:or(model.another) !model.key negate of a model variable !model.someKey substring(start, end) extract a substring model.someKey:substring(0, 5) substring(start) extract a substring model.someKey:substring(5) concat(vars...) concat model variables & text model.a:concat(model.b, text(!)) b64 byte-array to Base64 text model.someKey:b64 b64 Base64 text to byte-array model.someKey:b64 uuid generated UUID-4 value model.unique_id:uuid length length of model list variable model.someList:length For Base64 type matching, it handles two symmetrical use cases. If the key-value is a text string, the system would assume it is a Base64 text string and convert it to a byte-array. If the key-value is a byte-array, the system will encode it into a Base64 text string. For uuid type matching, the system will ignore the value of the model variable in the left hand side because UUID is a generated value. When using it in the right hand side, the model variable will be updated with a generated UUID value accordingly. For simplicity of syntax, each type matching command is a single operation. For more complex operation such as multiple AND, OR and NEGATE operators, you can configure multiple steps of operation. For string concatenation, you may concat a model variable with one or more model variables and text constants. A more convenient alternative to string concatenation is the use of \"runtime model variables\". You can replace the \"concat\" method with \"runtime model variable\" method as follows: # assuming the bearer token value is in model.token - 'text(Bearer ) -> model.bearer' - 'model.bearer:concat(model.token) -> authorization' # the above is the same as - 'text(Bearer {model.token}) -> authorization' An interesting use case is a simple decision task using the built-in no-op function. For boolean with value matching, you can test if the key-value in the left-hand-side is a null value. For example, when a control file for the application is not available, your application will switch to run in dev mode. A sample task may look like this: first.task: 'no.op' tasks: - input: - 'file(binary:/tmp/interesting-config-file) -> model.is-local:boolean(null=true)' process: 'no.op' output: - 'model.is-local -> decision' execution: decision next: - 'start.in.dev.mode' - 'start.in.cloud' Another use case is type conversion for HTTP path parameter which is always a text string. If your composable function requires a path parameter to be accepted as an integer, you can do this: - input: - 'input.path_parameter.userid -> model.userid:int' - 'model.userid -> userid' The above input data mapping example illustrates the use of a model variable to convert a text parameter into an integer. Note that if the path parameter is not numeric, the converted value will be -1. Note : The system only supports \"type matching modifier\" in the model namespace because of the design principle of data immutability. The model is a state machine for a flow instance. As a temporary store, we can use it for this purpose without side effect that the user application would accidentally modify a value of the flow's input. Convenient data mapping using model variable To address the common use case of using a model variable as an intermediate value, the system supports the following formats for input data mapping and output data mapping. // 2-part data mapping format LHS -> RHS // 3-part data mapping format LHS -> model.variable -> RHS For the 2-part data mapping format, there are left-hand-side and right-hand-side where the value retrieved from the left-hand-side variable is mapped to the right-hand-side. The 3-part data mapping allows us to use a model variable as an intermediate for simple type matching. In the previous example, it uses two entries to convert a HTTP path parameter from a text string to a number and set the number as input argument. The configuration syntax can be simplified as follows: - input: - 'input.path_parameter.userid -> model.userid:int -> userid' The above 3-part data mapping entry will be expanded into two entries internally. This extra processing is done at the \"CompileFlows\" step and thus there is no impact to the task execution speed. Please note that the 3-part data mapping format is not supported when the left-hand-side is a text constant. It is because a text constant may contain any special characters including the mapping signature -> . Metadata for each flow instance For each flow instance, the state machine in the \"model\" namespace provides the following metadata that you can use in the input/output data mapping. For example, you can set this for an exception handler to log additional information. Type Keyword Comment Flow ID model.flow The ID of the event flow config Trace ID model.trace Optional traceId when tracing is turned on Correlation ID model.cid Correlation ID of the inbound request Special handling for header When function input keyword header is specified in the \"right hand side\" of an input data mapping statement, it refers to the input event envelope's headers. Therefore, it assumes the \"left hand side\" to resolve into a Map object of key-values. Otherwise, it will reject the input data mapping statement with an error like this: Invalid input mapping 'text(ok) -> header', expect: Map, Actual: String When function input namespace header. is used, the system will map the value resolved from the \"left hand side\" statement into the specific header. For example, the input data mapping statement text(ok) -> header.demo will set \"demo=ok\" into the input event envelope's headers. When function output keyword header is specified in the \"left hand side\" of an output data mapping statement, it will resolve as a Map from the function output event envelope's headers. Similarly, when function output namespace header. is used, the system will resolve the value from a specific key of the function output event envelope's headers. Function input and output To support flexible input data mapping, the input to a function must be either Map<String, Object> or PoJo . However, the output (i.e. result set) of a function can be Map, PoJo or Java primitive. Your function should implement the TypedLambdaFunction interface to configure input and output. Since a data structure is passed to your function's input argument as key-values, you may create a PoJo class to deserialize the data structure. To tell the system that your function is expecting input as a PoJo, you can use the special notation * on the right hand side. For example, the following entry tells the system to set the value in \"model.dataset\" as a PoJo input. - input: - 'model.dataset -> *' Note : If the value from the left hand side is not a map, the system will ignore the input mapping command and print out an error message in the application log. Setting function input headers When function input body is used to hold a PoJo, we may use function input headers to pass other arguments to the function without changing the data structure of a user defined PoJo. In the following example, the HTTP query parameter \"userid\" will be mapped to the function input header key \"user\" and the HTTP request body will be mapped to the function input body. - input: - 'input.query.userid -> header.user' - 'input.body -> *' process: 'my.user.function' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' Task types Decision task A decision task makes decision to select the next task to execute. It has the tag execution=decision . In the output data mapping section, it must map the corresponding result set or its key-value to the decision object. The \"next\" tag contains a list of tasks to be selected based on the decision value. If decision value is boolean, a true value will select the first task. Otherwise, the second task will be selected. If decision value is an integer, the number should start from 1 where the corresponding \"next\" task will be selected. tasks: - input: - 'input.query.decision -> decision' process: 'simple.decision' output: - 'result -> model.decision' - 'result -> decision' description: 'Simple decision test' execution: decision next: - 'decision.case.one' - 'decision.case.two' Response task A response task will provide result set as a flow output or \"response\". A response task allows the flow to respond to the user or caller immediately and then move on to the next task asynchronously. For example, telling the user that it has accepted a request and then moving on to process the request that may take longer time to run. A response task has the tag execution=response and a \"next\" task. tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.pojo' - 'result -> output.body' description: 'Pass a pojo to another task' execution: response next: - 'sequential.two' End task An end task indicates that it is the last task of the transaction processing in a flow. If the flow has not executed a response task, the end task will generate the response. Response is defined by output data mapping. This task has the tag execution=end . For example, the greeting task in the unit tests is an end task. - input: - 'input.path_parameter.user -> user' process: 'greeting.demo' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: end Sequential task Upon completion of a sequential task, the next task will be executed. This task has the tag execution=sequential . In the following example, sequential.two will be executed after sequential.one . tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.pojo' description: 'Pass a pojo to another task' execution: sequential next: - 'sequential.two' Parallel task Upon completion of a parallel task, all tasks in the \"next\" task list will be executed in parallel. This task has the tag execution=parallel . In this example, parallel.one and parallel.two will run after begin.parallel.test tasks: - input: - 'int(2) -> count' process: 'begin.parallel.test' output: [] description: 'Setup counter for two parallel tasks' execution: parallel next: - 'parallel.one' - 'parallel.two' Fork-n-join task Fork-n-join is a parallel processing pattern. A \"fork\" task will execute multiple \"next\" tasks in parallel and then wait for the result sets before running the \"join\" task. This task has the tag execution=fork . It must have a list of \"next\" tasks and a \"join\" task. It may look like this: tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.pojo' description: 'Pass a pojo to another task' execution: fork next: - 'echo.one' - 'echo.two' join: 'join.task' Dynamic fork-n-join task A special version of the fork-n-join pattern is called dynamic fork-n-join which refers to parallel processing of multiple instances of the same \"next\" task for each element in a list. For example, you have a list of 100 elements in an incoming request and each element would be processed by the same backend service. You want to process the 100 elements in parallel by multiple instances of a service wraper that connects to the backend service. The use case can be configured like this: tasks: - input: - 'input.elements -> elements' process: 'data.validation' output: - 'result.elements -> model.elements' description: 'Validate list of elements' execution: fork source: 'model.elements' next: - 'element.processor' join: 'join.task' - name: 'element.processor' input: - 'model.elements.ITEM -> item' - 'model.elements.INDEX -> index' process: 'v1.element.processor' output: [] description: 'Hello world' execution: sink To handle this special use case, you can add a source parameter in the fork task. The \"source\" parameter tells the system which model variable holds the list of elements. You should only configure a single \"next\" task. The system will spin up parallel instances of the next task to handle each element from the model variable containing the list. In the input data mapping section, there are two special suffixes .ITEM and .INDEX . The system will iterate the list of elements and spin up an instance of the \"next\" task to retrieve the element (item) and index of the element in the list. The two special suffixes are relevant only when adding to the model variable configured in the \"source\" parameter. Important : The model variables with special suffixes '.ITEM' and '.INDEX' are virtual objects for the purpose of mapping as input arguments to a task. They cannot be used as regular model variables. Dynamic fork-n-join is designed to execute the same task for a list of elements in parallel. It does not support subflow. i.e. the \"process\" tag of the \"next\" task cannot be a subflow. Sink task A sink task is a task without any next tasks. Sink tasks are used by fork-n-join and pipeline tasks as reusable modules. This task has the tag execution=sink . - input: - 'text(hello-world-two) -> key2' process: 'echo.two' output: - 'result.key2 -> model.key2' description: 'Hello world' execution: sink Special consideration for parallelism The execution types (parallel and fork-n-join) are designed for parallel processing. Usually, parallel processing would improve performance. However, spinning up a large number of concurrent sessions to a slower backend service may create performance bottleneck. In fact, a massive number of concurrent sessions to a single backend would bring down the target service. This is an unintended \"denial of service\" attack. The dynamic fork-n-join execution style should be handled with caution because it can easily spin up a large number of parallel instances of the same task. To control parallelism, you can set a smaller number of concurrent \"instances\" for the \"next\" task using the \"instances\" parameter in the \"PreLoad\" annotation of the task. For example, you have 100 elements in a list but the maximum instances of the task can be set to 20. This would reduce the concurrency to 20, thus allowing you to manage performance according to available infrastructure resources. Therefore, processing 100 elements would require 5 rounds of 20 parallel executions and this orderly execution is supported by the underlying reactive event system. Pipeline feature Pipeline is an advanced feature of Event Script. Pipeline task A pipeline is a list of tasks that will be executed orderly within the current task. When the pipeline is done, the system will execute the \"next\" task. This task has the tag execution=pipeline . tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.pojo' description: 'Pass a pojo to another task' execution: pipeline pipeline: - 'echo.one' - 'echo.two' next: - 'echo.three' Some special uses of pipelines include \"for/while-loop\" and \"continue/break\" features. Simple for-loop In the following example, the loop.statement contains a for-loop that uses a variable in the state machine to evaluate the loop. In this example, the pipeline will be executed three times before passing control to the \"next\" task. tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.pojo' description: 'Pass a pojo to another task' execution: pipeline loop: statement: 'for (model.n = 0; model.n < 3; model.n++)' pipeline: - 'echo.one' - 'echo.two' - 'echo.three' next: - 'echo.four' Simple while loop The loop.statement may use a \"while loop\" syntax like this: loop: statement: 'while (model.running)' To exit the above while loop, one of the functions in the pipeline should return a boolean \"false\" value with output \"data mapping\" to the model.running variable. For loop with break/continue decision In the following example, the system will evaluate if the model.quit variable is true. If yes, the break or continue condition will be executed. The state variable is obtained after output data mapping and any task in the pipeline can set a key-value for mapping into the state variable. tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.pojo' description: 'Pass a pojo to another task' execution: pipeline loop: statement: 'for (model.n = 0; model.n < 3; model.n++)' condition: 'if (model.quit) break' pipeline: - 'echo.one' - 'echo.two' - 'echo.three' next: - 'echo.four' Note that the \"condition\" parameter can be a single condition or a list of conditions. In the following example, the system will evaluate both the model.quit and model.jump values. loop: statement: 'for (model.n = 0; model.n < 3; model.n++)' condition: - 'if (model.quit) break' - 'if (model.jump) break' Handling exception You can define exception handler at the top level or at the task level. Exception is said to occur when a user function throws exception or returns an EventEnvelope object with a status code equals to or larger than 400. The event status uses the same numbering scheme as HTTP exception status code. Therefore, status code less than 400 is not considered an exception. Top-level exception handler Top-level exception handler is a \"catch-all\" handler. You can define it like this: flow: id: 'greetings' description: 'Simplest flow of one task' ttl: 10s exception: 'v1.my.exception.handler' In this example, the v1.my.exception.handler should point to a corresponding exception handler that you provide. The following input arguments will be delivered to your function when exception happens. Key Description status Exception status code message Error message stack Stack trace in a text string The exception handler function can be an \"end\" task to abort the transaction or a decision task to take care of the exception. For example, the exception handler can be a \"circuit-breaker\" to retry a request. Note : for efficiency, stack trace transport is limited to the first 10 lines. Task-level exception handler You can attach an exception handler to a task. One typical use is the \"circuit breaker\" pattern. In the following example, the user function \"breakable.function\" may throw an exception for some error condition. The exception will be caught by the \"v1.circuit.breaker\" function. - input: - 'input.path_parameter.accept -> accept' - 'model.attempt -> attempt' process: 'exception.simulator' output: - 'int(0) -> model.attempt' - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'This demo function will break until the \"accept\" number is reached' execution: end exception: 'resilience.handler' The configuration for the circuit breaker function may look like this: - input: - 'error.code -> status' - 'error.message -> message' - 'model.attempt -> attempt' - 'int(2) -> max_attempts' process: 'resilience.handler' output: - 'result.attempt -> model.attempt' - 'result.decision -> decision' - 'result.status -> model.status' - 'result.message -> model.message' description: 'Just a demo circuit breaker' execution: decision next: - 'breakable.function' - 'abort.request' An exception handler will be provided with the \"error\" object that contains error code, error message and a stack trace. The exception handler can inspect the error object to make decision of the next step. For circuit breaker, we can keep the number of retry attempts in the state machine under \"model.attempt\" or any key name that you prefer. In the above example, it sets an integer constant of 2 for the maximum attempts. The circuit breaker can then evaluate if the number of attempts is less than the maximum attempts. If yes, it will return a decision of \"true\" value to tell the system to route to the \"breakable.function\" again. Otherwise, it will return a decision of \"false\" value to abort the request. A more sophisticated circuit breaker may be configured with \"alternative execution paths\" depending on the error status and stack trace. In this case, the decision value can be a number from 1 to n that corresponds to the \"next\" task list. Exception handlers may be used in both queries and transactions. For a complex transaction, the exception handler may implement database rollback logic or recovery mechanism. Best practice When a task-level exception handler throws exception, it will be caught by the top-level exception handler, if any. A top-level exception handler should not throw exception. Otherwise it may go into an exception loop. Therefore, we recommend that an exception handler should return regular result set in a PoJo or a Map object. An example of task-level exception handler is shown in the \"HelloException.class\" in the \"task\" folder where it set the status code in the result set so that the system can map the status code from the result set to the next task or to the HTTP output status code. Advanced features No-operation function A convenient no-operation function with the route name no.op is available. It can be used when you want to perform some input/output data mapping without executing any business logic. Generic resilience handler function Another useful built-in function is a resilience handler with the route name resilience.handler . It is a generic resilience handler. It will retry, abort, use an alternative path or exercise a brief backoff. Figure 2 - Resilience Handler The following parameters (input data mapping) define behavior for the handler: max_attempts - when the handler has used all the attempts, it will abort. attempt - this tells the handler how many attempts it has tried status - you should map the error status code in this field message - you should map the error message in this field alternative - the optional codes and range of status codes to tell the handler to reroute delay - the delay in milliseconds before exercising retry or reroute. Minimum value is 10 ms. Delay is skipped for the first retry. This slight delay is a protection mechanism. Optional backoff behavior: cumulative - the total number of failures since last success or backoff reset if any backoff - the time of a backoff period (epoch milliseconds) if any backoff_trigger - the total number of failures that triggers a backoff backoff_seconds - the time to backoff after an abort has occurred. During this period, It will abort without updating attempt. This avoids overwhelming the target service that may result in recovery storm. Return value (output data mapping): result.attempt - the handler will clear or increment this counter result.cumulative - the handler will clear or increment this counter. Not set if \"backoff_trigger\" is not given in input. result.decision - 1, 2 or 3 where 1=retry, 2=abort, 3=reroute that corresponds to the next tasks result.status - the status code that the handler aborts the retry or reroute. Not set if retry or reroute. result.message - the reason that the handler aborts the retry or reroute. Not set if retry or reroute. result.backoff - the time of a backoff period (epoch milliseconds). Not set if not in backoff mode. Note : \"result.attempt\" should be saved in the state machine with the \"model.\" namespace. \"result.cumulative\" and \"result.backoff\" should be saved in the temporary file system or an external state machine. For more details, please refer to the event script resilience-demo.yml in the event-script-engine's test resources folder and the unit test resilienceHandlerTest() under the FlowTests class. Extract of the task configuration for the resilience handler is shown as follows. In the following example, \"my.task\" is the function that is configured with the 'resilience.handler' as an exception handler. The input data mapping tells the handler to enter into \"backoff\" period when the cumulative failure count reaches the \"backoff_trigger\" threshold of 3. After that, all requests will be aborted until the backoff period expires. - input: - 'error.code -> status' - 'error.message -> message' - 'model.attempt -> attempt' - 'int(10) -> max_attempts' - 'text(401, 403-404) -> alternative' - 'file(text:/tmp/resilience/cumulative) -> cumulative' - 'file(text:/tmp/resilience/backoff) -> backoff' - 'int(3) -> backoff_trigger' - 'int(2) -> backoff_seconds' - 'int(500) -> delay' process: 'resilience.handler' output: - 'result.status -> model.status' - 'result.message -> model.message' - 'result.attempt -> model.attempt' - 'result.decision -> decision' - 'result.backoff -> file(/tmp/resilience/backoff)' - 'result.cumulative -> file(/tmp/resilience/cumulative)' description: 'Resilience handler with alternative path and backoff features' execution: decision next: - '@retry' - 'abort.request' - 'alternative.task' You may also use this resilience handler as a template to write your own exception handler for more complex recovery use cases. Important : If you want the resilience handler to automatically retry the task that throws exception, set \"@retry\" as the first task in the \"next\" task list. When the \"backoff\" feature is enabled, you should configure the resilience handler as a gatekeeper to protect your user function. i.e. it is the \"first.task\". This allows the system to abort requests during the backoff period. If the resilience handler is used as a gatekeeper and there was no exception, it will execute the original task. However, if \"@retry\" is used as the first next task entry, it cannot resolve the original task since it has never been executed. You can add the original task to the first entry with \"|\" as a separator. e.g. @retry | my.task where \"my.task\" is the original task. This tells the system to route the request to \"my.task\" when there is no exception in the first place. The \"@retry\" keyword can only be used in the first entry of the \"next\" task list. External state machine The in-memory state machine is created for each query or transaction flow and it is temporal. For complex transactions or long running work flows, you would typically want to externalize some transaction states to a persistent store such as a distributed cache system or a high performance key-value data store. In these use cases, you can implement an external state machine function and configure it in a flow. Below is an example from a unit test. When you externalize a key-value to an external state machine, you must configure the route name (aka level-3 functional topic) of the external state machine. Note : Passing a null value to a key of an external state machine means \"removal\". external.state.machine: 'v1.ext.state.machine' tasks: - input: # A function can call an external state machine using input or output mapping. # In this example, it calls external state machine from input data mapping. - 'input.path_parameter.user -> ext:/${app.id}/user' - 'input.body -> model.body' # demonstrate saving constant to state machine and remove it using model.none - 'text(world) -> ext:hello' - 'model.none -> ext:hello' process: 'no.op' output: - 'text(application/json) -> output.header.content-type' # It calls external state machine again from output data mapping - 'input.body -> ext:/${app.id}/body' - 'input.body -> output.body' - 'text(message) -> ext:test' - 'model.none -> ext:test' description: 'Hello World' execution: end The \"external.state.machine\" parameter is optional. When present, the system will send a key-value from the current flow instance's state machine to the function implementing the external state machine. The system uses the \"ext:\" namespace to externalize a state machine's key-value. Note : The delivery of key-values to the external state machine is asynchronous. Therefore, please assume eventual consistency. You should implement a user function as the external state machine. The input interface contract to the external state machine for saving a key-value is: header.type = 'put' header.key = key body.data = value Your function should save the input key-value to a persistent store. In another flow that requires the key-value, you can add an initial task to retrieve from the persistent store and do \"output data mapping\" to save to the in-memory state machine so that your transaction flow can use the persisted key-values to continue processing. In the unit tests of the event-script-engine subproject, these two flows work together: externalize-put-key-value externalize-get-key-value IMPORTANT : Events to an external state machine are delivered asynchronously. If you want to guarantee message sequencing, please do not set the \"instances\" parameter in the PreLoad annotation. To illustrate a minimalist implementation, below is an example of an external state machine in the event-script-engine's unit test section. @PreLoad(route = \"v1.ext.state.machine\") public class ExternalStateMachine implements LambdaFunction { private static final Logger log = LoggerFactory.getLogger(ExternalStateMachine.class); private static final ManagedCache store = ManagedCache.createCache(\"state.machine\", 5000); private static final String TYPE = \"type\"; private static final String PUT = \"put\"; private static final String GET = \"get\"; private static final String REMOVE = \"remove\"; private static final String KEY = \"key\"; private static final String DATA = \"data\"; @SuppressWarnings(\"unchecked\") @Override public Object handleEvent(Map<String, String> headers, Object input, int instance) { if (!headers.containsKey(KEY)) { throw new IllegalArgumentException(\"Missing key in headers\"); } String type = headers.get(TYPE); String key = headers.get(KEY); if (PUT.equals(type) && input instanceof Map) { Map<String, Object> dataset = (Map<String, Object>) input; var data = dataset.get(DATA); if (data != null) { log.info(\"Saving {} to store\", key); store.put(key, data); return true; } } if (GET.equals(type)) { Object v = store.get(key); if (v != null) { log.info(\"Retrieve {} from store\", key); return v; } else { return null; } } if (REMOVE.equals(type)) { if (store.exists(key)) { store.remove(key); log.info(\"Removed {} from store\", key); return true; } else { return false; } } return false; } } For more sophisticated operation, you may also configure the external state machine as a \"flow\" like this: external.state.machine: 'flow://ext-state-machine' You can then define the flow for \"ext-state-machine\" like this: flow: id: 'ext-state-machine' description: 'Flow to execute an external state machine' ttl: 10s first.task: 'v1.ext.state.machine' tasks: - input: - 'input.header.key -> header.key' - 'input.header.type -> header.type' - 'input.body.data -> data' process: 'v1.ext.state.machine' output: [] description: 'Execute external state machine' execution: end Note : By definition, external state machine flow is outside the scope of the calling flow. Future task scheduling You may add a \u201cdelay\u201d tag in a task so that it will be executed later. This feature is usually used for unit tests or \"future task scheduling\". Since the system is event-driven and non-blocking, the delay is simulated by event scheduling. It does not block the processing flow. Type Value Example Fixed delay Milliseconds delay: '1000 ms' Variable delay State machine variable delay: model.delay Note that the \"ms\" suffix is optional for documentation purpose. It denotes milliseconds if present. When delay is set to a state variable that its value is not configured by a prior data mapping, the delay command will be ignored. An example task that has an artificial delay of 2 seconds: tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.ex -> exception' - 'text(hello world) -> greeting' process: 'greeting.test' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: end delay: '2000 ms' Chapter-3 Home Chapter-5 REST Automation Table of Contents Build, Test and Deploy","title":"Chapter-4"},{"location":"guides/CHAPTER-4/#event-script-syntax","text":"Event Script is a Domain Specific Language (DSL) that uses YAML to represent an end-to-end transaction flow. A transaction is a business use case, and the flow can be an API service, a batch job or a real-time transaction.","title":"Event Script Syntax"},{"location":"guides/CHAPTER-4/#flow-list","text":"This configuration file sits in the project \"resources\" project and contains a list of filenames. The default flow list is \"flows.yaml\" under the \"resources\" folder. It may look like this. flows: - 'get-profile.yml' - 'create-profile.yml' - 'delete-profile.yml' location: 'classpath:/flows/' The \"location\" parameter is optional. If present, you can tell the system to load the flow config files from another folder location.","title":"Flow list"},{"location":"guides/CHAPTER-4/#multiple-flow-lists","text":"You can provide more than one flow list to your application and it can become very handy under different situations. For instance, to achieve better modularity in complex application, flows can be grouped to multiple categories based on development team's choice and these flows can be managed in multiple flow lists. Another great place to use multiple flow list is to include external libraries which contain pre-defined flow lists. The following example demonstrates that an application loads a list of flows defined in \"flows.yaml\" and additional flows defined in \"more-flows.yaml\" file of a composable library. yaml.flow.automation=classpath:/flows.yaml, classpath:/more-flows.yaml","title":"Multiple flow lists"},{"location":"guides/CHAPTER-4/#writing-new-rest-endpoint-and-function","text":"You can use the \"composable-example\" subproject as a template to write your own composable application. For each filename in the flows.yml, you should create a corresponding configuration file under the \"resources/flows\" folder. Let's write a new flow called \"greetings\". You can copy-n-paste the following into a file called \"greetings.yml\" under the \"resources/flows\" folder. flow: id: 'greetings' description: 'Simplest flow' ttl: 10s first.task: 'greeting.demo' tasks: - input: - 'input.path_parameter.user -> user' process: 'greeting.demo' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: end In the application.properties, you can specify the following parameter: yaml.flow.automation=classpath:/flows.yaml and update the \"flows.yaml\" file in the resources folder as follows: flows: - 'get-profile.yml' - 'create-profile.yml' - 'delete-profile.yml' - 'greetings.yml' Then, you can add a new REST endpoint in the \"rest.yaml\" configuration file like this. - service: \"http.flow.adapter\" methods: ['GET'] url: \"/api/greetings/{user}\" flow: 'greetings' timeout: 10s cors: cors_1 headers: header_1 The above REST endpoint takes the path parameter \"user\". The task executor will map the path parameter to the input arguments (headers and body) in your function. Now you can write your new function with the named route \"greeting.demo\". Please copy-n-paste the following into a Java class called \"Greetings\" and save in the package under \"my.organization.tasks\" in the source project. Note : The package name, \"my.organization\", is an example. Please replace it with your organization package path. @PreLoad(route=\"greeting.demo\", instances=10, isPrivate = false) public class Greetings implements TypedLambdaFunction<Map<String, Object>, Map<String, Object>> { private static final String USER = \"user\"; @Override public Map<String, Object> handleEvent(Map<String, String> headers, Map<String, Object> input, int instance) { if (input.containsKey(USER)) { String user = input.get(USER).toString(); Map<String, Object> result = new HashMap<>(); result.put(USER, user); result.put(\"message\", \"Welcome\"); result.put(\"time\", new Date()); return result; } else { throw new IllegalArgumentException(\"Missing path parameter 'user'\"); } } } For the flow-engine to find your new function, please update the key-value for \"web.component.scan\" in application.properties: web.component.scan=my.organization To test your new REST endpoint, flow configuration and function, please point your browser to http://127.0.0.1:8100/api/greetings/my_name You can replace \"my_name\" with your first name to see the response to the browser.","title":"Writing new REST endpoint and function"},{"location":"guides/CHAPTER-4/#flow-configuration-syntax","text":"In your \"greetings.yml\" file above, you find the following key-values: flow.id - Each flow must have a unique flow ID. The flow ID is usually originated from a user facing endpoint through an event adapter. For example, you may write an adapter to listen to a cloud event in a serverless deployment. In The most common one is the HTTP adapter. The flow ID is originated from the \"rest.yaml\". The flow-engine will find the corresponding flow configuration and create a new flow instance to process the user request. flow.description - this describes the purpose of the flow flow.ttl - \"Time to live (TTL)\" timer for each flow. You can define the maximum time for a flow to finish processing. All events are delivered asynchronously and there is no timeout value for each event. The TTL defines the time budget for a complete end-to-end flow. Upon expiry, an unfinished flow will be aborted. You can use suffix \"s\" for seconds, \"m\" for minutes and \"h\" for hours. e.g. \"30s\" for 30 seconds. Note : When using the HTTP Flow Adapter, the flow.ttl value can be higher than the REST endpoint's timeout value. This would happen when one of your tasks in the event flow responds to the caller and the event flow continues to execute the rest of the flow. This type of task is called \"response\" task. first.task - this points to the route name of a function (aka \"task\") to which the flow engine will deliver the incoming event. The configuration file contains a list of task entries where each task is defined by \"input\", \"process\", \"output\" and \"execution\" type. In the above example, the execution type is \"end\", meaning that it is the end of a transaction and its result set will be delivered to the user.","title":"Flow configuration syntax"},{"location":"guides/CHAPTER-4/#underlying-event-system","text":"The Event Script system uses platform-core as the event system where it encapsulates Java Virtual Threads and Eclipse Vertx. The integration points are intentionally minimalist. For most use cases, the user application does not need to make any API calls to the underlying event system.","title":"Underlying Event System"},{"location":"guides/CHAPTER-4/#rest-automation-and-http-flow-adapter","text":"The most common transaction entry point is a REST endpoint. The event flow may look like this: REQUEST -> \"http.request\" -> \"task.executor\" -> user defined tasks -> \"async.http.response\" -> RESPONSE REST automation is part of the platform-core library. It contains a non-blocking HTTP server that converts HTTP requests and responses into events. It routes an HTTP request event to the HTTP adapter if the \"flow\" tag is provided. In the following example, the REST endpoint definition is declared in a \"rest.yaml\" configuration. It will route the URI \"/api/decision\" to the HTTP flow adapter that exposes its service route name as \"http.flow.adapter\". rest: - service: \"http.flow.adapter\" methods: ['GET'] url: \"/api/decision?decision=_\" flow: 'decision-test' timeout: 10s cors: cors_1 headers: header_1 tracing: true The \"cors\" and \"headers\" sections are optional. When specified, the REST endpoint will insert CORS headers and HTTP request headers accordingly. For REST automation syntax, please refer to Chapter 3 The HTTP flow adapter maps the HTTP request dataset and the flow ID into a standard event envelope for delivery to the flow engine. The HTTP request dataset, addressable with the \"input.\" namespace, contains the following: Key Values method HTTP method uri URI path header HTTP headers cookie HTTP cookies path_parameter Path parameters if any query HTTP query parameters if any body HTTP request body if any stream input stream route ID if any ip remote IP address filename filename if request is a multipart file upload session authenticated session key-values if any For easy matching, please use lower case for headers, cookies, query and path parameters. Regular API uses JSON and XML and they will be converted to a hash map in the event's body. For special use cases like file upload/download, your application logic may invoke a streaming API to retrieve the binary payload. Please refer to Appendix-III","title":"REST automation and HTTP flow adapter"},{"location":"guides/CHAPTER-4/#task-is-a-composable-function","text":"Each task in a flow must have a corresponding composable function. You can assign a task name to the function using the Preload annotation like this. @PreLoad(route=\"greeting.demo\", instances=10) public class Greetings implements TypedLambdaFunction<Map<String, Object>, Map<String, Object>> { @Override public Map<String, Object> handleEvent(Map<String, String> headers, Map<String, Object> input, int instance) { // business logic here return someOutput; } } The \"route\" in the Preload annotation is the task name. The \"instances\" define the maximum number of \"workers\" that the function can handle concurrently. The system is designed to be reactive and the function does not consume memory and CPU resources until an event arrives. You may also define concurrency using environment variable. You can replace the \"instances\" with envInstances using standard environment variable syntax like ${SOME_ENV_VARIABLE:default_value} .","title":"Task is a composable function"},{"location":"guides/CHAPTER-4/#pojo-serialization-strategies","text":"The default serialization strategy is defined in application.properties as the snake.case.serialization parameter. Snake case serialization will be used when this parameter is set to true. Otherwise, camel case serialization will be used. You can override the default serialization strategy in 2 ways in the PreLoad annotation. Configure input / output serialization strategies Implement your own custom serializer using the CustomSerializer interface @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface PreLoad { String route(); Class<?> customSerializer() default Void.class; Class<?> inputPojoClass() default Void.class; int instances() default 1; String envInstances() default \"\"; boolean isPrivate() default true; SerializationStrategy inputStrategy() default SerializationStrategy.DEFAULT; SerializationStrategy outputStrategy() default SerializationStrategy.DEFAULT; } To instruct the composable function to deserialize input as a PoJo and serialize output PoJo, you can set the \"inputStrategy\" and/or \"outputStrategy\" parameters in the PreLoad annotation. For example, when you set the inputStrategy to SerializationStrategy.CAMEL, it will override the default snake case serialization. This is useful when your function receives input from an external source that you have no control of the serialization strategy. Similarly, when you are using default snake case serialization, you can set outputStrategy to SerializationStrategy.CAMEL in the last function in a flow so that camel case output will be delivered to an external target. If your PoJo requires special treatment and the built-in preconfigured serializer does not handle your use case, you can implement your own custom serializer. In this case, the inputStrategy and outputStrategy will be ignored.","title":"PoJo serialization strategies"},{"location":"guides/CHAPTER-4/#unique-task-naming","text":"Composable functions are designed to be reusable. By changing some input data mapping to feed different parameters and payload, your function can behave differently. Therefore, it is quite common to use the same function (i.e. the process parameter) more than once in a single event flow. When a task is not named, the \"process\" parameter is used to name the task. Since each task must have a unique name for event routing, we cannot use the same \"process\" name more than once in an event flow. To handle this use case, you can create unique names for the same function using the name parameter like this: flow: id: 'greetings' description: 'Simplest flow' ttl: 10s first.task: 'my.first.task' tasks: - name: 'my.first.task' input: - 'input.path_parameter.user -> user' process: 'greeting.demo' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: sequential next: - 'another.task' The above event flow configuration uses \"my.first.task\" as a named route for \"greeting.demo\" by adding the \"name\" parameter to the composable function. Note : The Event Manager performs event choreography using the unique task name. Therefore, when the \"process\" name for the function is not unique, you must create unique task \"names\" for the same function to ensure correct routing.","title":"Unique task naming"},{"location":"guides/CHAPTER-4/#assigning-multiple-route-names-to-a-single-function","text":"The built-in distributed tracing system tracks the actual composable functions using the \"process\" name and not the task names. When there is a need to track the task names in distributed trace, you can tell the system to create additional instances of the same function with different route names. You can use a comma separated list as the route name like this: @PreLoad(route=\"greeting.case.1, greeting.case.2\", instances=10) public class Greetings implements TypedLambdaFunction<Map<String, Object>, Map<String, Object>> { @Override public Map<String, Object> handleEvent(Map<String, String> headers, Map<String, Object> input, int instance) { // business logic here return someResult; } } Note : The \"unique task naming\" method is more memory efficient than creating additional route names","title":"Assigning multiple route names to a single function"},{"location":"guides/CHAPTER-4/#preload-overrides","text":"Once a composable function is published as a reusable library in the artifactory, its route name and number of instances are fixed using the \"PreLoad\" annotation in the function class. Without refactoring your libary, you can override its route name and instances using a preload override file like this: preload: - original: 'greeting.demo' routes: - 'greeting.case.1' - 'greeting.case.2' # the \"instances\" tag is optional instances: 20 - original: 'v1.another.reusable.function' keep-original: true routes: - 'v1.reusable.1' - 'v1.reusable.2' In the above example, the function associated with \"greeting.demo\" will be preloaded as \"greeting.case.1\" and \"greeting.case.2\". The number of maximum concurrent instances is also changed from 10 to 20. In the second example, \"v1.another.reusable.function\" is updated as \"v1.reusable.1\" and \"v1.reusable.2\" and the number of concurrent instances is not changed. The original route \"v1.another.reusable.function\" is preserved when the \"keep-original\" parameter is set to true. Assuming the above file is \"preload-override.yaml\" in the \"resources\" folder of the application source code project, you should add the following parameter in application.properties to activate this preload override feature. yaml.preload.override=classpath:/preload-override.yaml","title":"Preload overrides"},{"location":"guides/CHAPTER-4/#multiple-preload-override-config-files","text":"When you publish a composable function as a library, you may want to ensure the route names of the functions are merged properly. In this case, you can bundle a library specific preload override config file. For example, your library contains a \"preload-kafka.yaml\" to override some route names, you can add it to the yaml.preload.override parameter like this: yaml.preload.override=classpath:/preload-override.yaml, classpath:/preload-kafka.yaml The system will then merge the two preload override config files. The concurrency value of a function is overwritten using the \"instances\" parameter in the first preload override file. Subsequent override of the \"instances\" parameter is ignored. i.e. the first preload override file will take precedence.","title":"Multiple preload override config files"},{"location":"guides/CHAPTER-4/#hierarchy-of-flows","text":"As shown in Figure 1, you can run one or more sub-flows inside a primary flow. Figure 1 - Hierarchy of flows To do this, you can use the flow protocol identifier ( flow:// ) to indicate that the task is a flow. For example, when running the following task, \"flow://my-sub-flow\" will be executed like a regular task. tasks: - input: - 'input.path_parameter.user -> header.user' - 'input.body -> body' process: 'flow://my-sub-flow' output: - 'result -> model.pojo' description: 'Execute a sub-flow' execution: sequential next: - 'my.next.function' If the sub-flow is not available, the system will throw an error stating that it is not found. Hierarchy of flows would reduce the complexity of a single flow configuration file. The \"time-to-live (TTL)\" value of the parent flow should be set to a value that covers the complete flow including the time used in the sub-flows. In the input/output data mapping sections, the configuration management system provides a parent state machine using the namespace model.parent. to be shared by the primary flow and all sub-flows that are instantiated from it. Just like a task, a subflow has \"input\" and \"output\". You can map data to the \"input\" of a subflow using the namespaces \"body\" and \"header\" where they are maps of key-values. Inside a task of the subflow, the body and header namespaces can be accessed for their key-values like this: - input: - 'input.header.user -> header.user' - 'input.body -> *' process: 'first.task.in.subflow' output: - 'result -> model.parent.subflow_result' description: 'Execute a task in a subflow' execution: end Since the parent flow and subflows has a shared state machine, passing \"body\" and \"header\" key-values to the \"input\" of a subflow is optional. You can pass key-values between the parent and subflows using the shared state machine easily. Note : The namespace model.root. is an alias of model.parent. This would reduce ambiguity if you prefer to use \"root\" referring to the parent flow that creates one or more subflows.","title":"Hierarchy of flows"},{"location":"guides/CHAPTER-4/#tasks-and-data-mapping","text":"All tasks for a flow are defined in the \"tasks\" section.","title":"Tasks and data mapping"},{"location":"guides/CHAPTER-4/#inputoutput-data-mapping","text":"A function is self-contained. This modularity reduces application complexity because the developer only needs interface contract details for a specific function. To handle this level of modularity, the system provides configurable input/output data mapping. Namespaces for I/O data mapping Type Keyword and/or namespace LHS / RHS Mappings Flow input dataset input. left input Flow error dataset error. left. input Flow output dataset output. right output Function input body no namespace required right input Function input or output headers header or header. both I/O Function output result set result. left output Function output status code status left output Function output pojo class name datatype left output Decision value decision right output State machine dataset model. both I/O Parent state machine dataset model.parent. both I/O Alias for parent state machine model.root. both I/O External state machine key-value ext: right I/O For state machine (model and model.parent namespaces), the system prohibits access to the whole namespace. You should only access specific key-values in the model or model.parent namespaces. The namespace model.root. or model.parent. is shared by the primary flow and all sub-flows that are instantiated from it. When your function returns a PoJo, the datatype field in the left-hand-side will contain the class name of the PoJo. This allows you to save the class name in the state machine and pass it to another task that needs to reconstruct the PoJo class. This is used when your function may return different PoJo classes for different scenarios. The error dataset is available in the input data mapping of an exception handler that attaches to a task or the generic exception handler that attaches to the flow itself. The error dataset includes the following: error.task - this is the task name of the task that throws exception error.status - the status code of the exception error.message - the error message error.stack - stack trace if any The external state machine namespace uses the namespace ext: to indicate that the key-value is external. Constants for input data mapping Type Keyword for the left-hand-side argument String text(example_value) Integer int(number) Long long(number) Float float(number) Double double(number) Boolean boolean(true or false) Map map(k1=v1, k2=v2) map(base.config.parameter) File file(text:file_path) File file(binary:file_path) File file(json:file_path) Classpath classpath(text:file_path) Classpath classpath(binary:file_path) Classpath classpath(json:file_path) For input data mapping, the \"file\" constant type is used to load some file content as an argument of a user function. You can tell the system to render the file as \"text\", \"binary\" or \"json\". Similarly, the \"classpath\" constant type refers to static file in the application source code's \"resources\" folder. When file type mapping is \"json\", the file content will be rendered as a Map or a List from a JSON string. The \"map\" constant type is used for two purposes: 1. Map of key-values The following example illustrates creation of a map of key-values. In the first entry, a map of 2 key-values is set as the input argument \"myMap\" of a user function. In the second entry, the map's values are retrieved from the key \"some.key\" in base configuration and the environment variable \"ENV_VAR_ONE\". 'map(k1=v1, k2=v2) -> myMap' 'map(k1=${some.key}, k2=${ENV_VAR_ONE}) -> myMap' Note : The comma character is used as a separator for each key-value pair. If the value contains a comma, the system cannot parse the key-values correctly. In this case, please use the 2nd method below. 2. Mapping values from application.yml The following input data mapping sets the value of \"my.key\" from the application.yml base configuration file to the input argument \"myKey\" of a user function. 'map(my.key) -> myKey' Since the system uses both application.properties and application.yml as base configuration files, you can use either configuration files depending on the data type of the value. For application.properties, \"map(my.key)\" is the same as \"text(${my.key})\". For application.yml, \"map(my.key)\" would set a primitive value (text, integer, float, boolean), a hash map of key-values or an array of values. Special content type for output data mapping Type Keyword for the right-hand-side argument File file(file_path) File file(append:file_path) For output data mapping, the \"file\" content type is used to save some data from the output of a user function to a file in the local file system. If the left-hand-side (LHS) resolved value is null, the file in the RHS will be deleted. This allows you to clean up temporary files before your flow finishes. An optional prefix \"append\" may be used to tell the system to append file content instead of overwriting it. Note : The local file system write operation is not thread-safe. If you have parallel tasks appending to the same file, the integrity of file content is not guaranteed. One way to ensure thread safety is to use singleton pattern. This can be done by setting the number of instances of the task writing to the local file system to 1. Decision value The \"decision\" keyword applies to \"right hand side\" of output data mapping statement in a decision task only (See \"Decision\" in the task section). Each flow has its own input and output Each function has its input headers, input body and output result set. Optionally, a function can return an EventEnvelope object to hold its result set in the \"body\", a \"status\" code and one or more header key-values. Since each function is stateless, a state machine (with namespace model. ) is available as a temporary memory store for transaction states that can be passed from one task to another. All variables are addressable using the standard dot-bracket convention. For example, \"hello.world\" will retrieve the value 100 from this data structure: { \"hello\": { \"world\": 100 } } and \"numbers[1]\" will retrieve the value 200 below: { \"numbers\": [100, 200] } The assignment is done using the assignment ( -> ) syntax. In the following example, the HTTP input query parameter 'amount' is passed as input body argument 'amount' to the task 'simple.decision'. The result (function \"return value\") from the task will be mapped to the special \"decision\" variable that the flow engine will evaluate. This assumes the result is a boolean or numeric value. The \"decision\" value is also saved to the state machine ( model ) for subsequent tasks to evaluate. - input: - 'input.query.amount -> amount' process: 'simple.decision' output: - 'result -> decision' - 'result -> model.decision'","title":"Input/Output data mapping"},{"location":"guides/CHAPTER-4/#environment-variables","text":"You can use the standard ${ENV_VAR:default} syntax to resolve environment variables or parameters from the application.properties.","title":"Environment variables"},{"location":"guides/CHAPTER-4/#runtime-model-variables","text":"To use a runtime model variable value as a key or constant, you can use the {model.variable_name} syntax. For example, - input: - 'text(wonderful day) -> model.world' - 'text(world) -> model.pointer' - 'model.{model.pointer} -> value1' - 'text(new {model.pointer}) -> value2' - 'text(keep {this}/{one} unchanged) -> value3' process: 'demo.function' model.{model.pointer} is resolved as model.world , giving value1 = wonderful day and value2 = new world . The text inside a set of brackets that is not a model variable will be kept unchanged, thus value3 = keep {this}/{one} unchanged The use of string substitution is subject to event script syntax validation. Therefore, When this feature is used in the left-hand-side of an input data mapping, it can be used to substitute a constant or a segment of a key in the input. and model. namespaces. The above example shows the use of the model namespace in model.{model.pointer} -> value1 . Similarly, when used in the left-hand-side of an output data mapping, it can be used to substitute a constant or a segment of a key in the input. , model. , header. or result. namespaces. When used in the right-hand-side of an input data mapping, namespace is optional because it may map as an argument to a task. When used in the right-hand-side of an output data mapping, it can be used to substitute a model. namespace, file( output, flow output. namespace or an external state machine ext: namespace. Important : For security reason, the key inside the brackets must be a model variable. The resolved value from a model variable must be either text or number. Otherwise, it will be converted to a value of \"null\". For simplicity, nested substitution is not allowed. i.e. model.{model.{model.n}} or model.{model.list[model.n]} will be rejected. If the bracketed text is not a model variable, the brackets and the enclosed text will be kept unchanged.","title":"Runtime model variables"},{"location":"guides/CHAPTER-4/#handling-arrays-in-a-dataset","text":"An array of data elements is expressed as a list. { \"numbers\": [100, 200] } As discussed earlier, an array element can be retrieved using a number as index. For example, to take the second element with value 200 above, you can use this data mapping like this: - 'input.body.numbers[1] -> second_number' In the above example, it is an \"input data mapping\". It maps the second element of value 200 as the input argument \"second_number\" to a composable function. For-loop feature is supported in pipeline in an event flow. It would be convenient to use the iterator value as an index to map an input argument. We can do something like this: - 'input.body.numbers[model.n] -> second_number' where model.n is the iterator value in a for-loop. Similarly, it is possible to do output data mapping. For example, - 'result.computed -> model.list[model.n]' To address an array element, we can use a number or a \"dynamic model variable\" as an index. The model variable must resolved to a number. Note : There are some consideration when using a dynamic model variable as an index. The left-hand-side of a data mapping is a GET operation. The right-hand-side is a SET operation. If the model variable is non-numeric, the GET operation will return null and SET operation will throw exception. To avoid setting an arbitrary high index, the size of the index is limited by the parameter \"max.model.array.size\" in application.properties or application.yml","title":"Handling arrays in a dataset"},{"location":"guides/CHAPTER-4/#append-an-element-to-an-array","text":"An empty array index in the right hand side tells the system to append an element to an array. For example, the value resolved from the left hand side \"result.item1\" and \"result.item2\" will be appended to the model.items array in the state machine. - 'result.item1 -> model.items[]' - 'result.item2 -> model.items[]' If model.items does not exist, the first element will be set as array index \"0\". Therefore, the above output data mapping statements are the same as: - 'result.item1 -> model.items[0]' - 'result.item2 -> model.items[1]'","title":"Append an element to an array"},{"location":"guides/CHAPTER-4/#simple-type-matching-and-conversion","text":"Event script's state machine supports simple type matching and conversion for the model namespace. This \"impedance matching\" feature allows us to accommodate minor interface contract changes without refactoring business logic of a user function. This is supported in both the left-hand-side and right-hand-side of both input and output data mappings. For the left-hand-side, the state machine's model value is matched or converted to the target data type before setting the value of the right-hand-side. The state machine values are unchanged. For the right-hand-side, the matched or converted value is applied to the state machine's model value. The syntax is model.somekey:type where \"type\" is one of the following: Type Match value as Example text text string model.someKey:text binary byte array model.someKey:binary int integer or -1 if not numeric model.someKey:int long long or -1 if not numeric model.someKey:long float float or -1 if not numeric model.someKey:float double double or -1 if not numeric model.someKey:double boolean true or false model.someKey:boolean boolean(value) true if value matches model.someKey:boolean(positive) boolean(value=true) true if value matches model.someKey:boolean(positive=true) boolean(value=false) false if value matches model.someKey:boolean(negative=false) and(model.key) boolean AND of 2 model keys model.someKey:and(model.another) or(model.key) boolean OR of 2 model keys model.someKey:or(model.another) !model.key negate of a model variable !model.someKey substring(start, end) extract a substring model.someKey:substring(0, 5) substring(start) extract a substring model.someKey:substring(5) concat(vars...) concat model variables & text model.a:concat(model.b, text(!)) b64 byte-array to Base64 text model.someKey:b64 b64 Base64 text to byte-array model.someKey:b64 uuid generated UUID-4 value model.unique_id:uuid length length of model list variable model.someList:length For Base64 type matching, it handles two symmetrical use cases. If the key-value is a text string, the system would assume it is a Base64 text string and convert it to a byte-array. If the key-value is a byte-array, the system will encode it into a Base64 text string. For uuid type matching, the system will ignore the value of the model variable in the left hand side because UUID is a generated value. When using it in the right hand side, the model variable will be updated with a generated UUID value accordingly. For simplicity of syntax, each type matching command is a single operation. For more complex operation such as multiple AND, OR and NEGATE operators, you can configure multiple steps of operation. For string concatenation, you may concat a model variable with one or more model variables and text constants. A more convenient alternative to string concatenation is the use of \"runtime model variables\". You can replace the \"concat\" method with \"runtime model variable\" method as follows: # assuming the bearer token value is in model.token - 'text(Bearer ) -> model.bearer' - 'model.bearer:concat(model.token) -> authorization' # the above is the same as - 'text(Bearer {model.token}) -> authorization' An interesting use case is a simple decision task using the built-in no-op function. For boolean with value matching, you can test if the key-value in the left-hand-side is a null value. For example, when a control file for the application is not available, your application will switch to run in dev mode. A sample task may look like this: first.task: 'no.op' tasks: - input: - 'file(binary:/tmp/interesting-config-file) -> model.is-local:boolean(null=true)' process: 'no.op' output: - 'model.is-local -> decision' execution: decision next: - 'start.in.dev.mode' - 'start.in.cloud' Another use case is type conversion for HTTP path parameter which is always a text string. If your composable function requires a path parameter to be accepted as an integer, you can do this: - input: - 'input.path_parameter.userid -> model.userid:int' - 'model.userid -> userid' The above input data mapping example illustrates the use of a model variable to convert a text parameter into an integer. Note that if the path parameter is not numeric, the converted value will be -1. Note : The system only supports \"type matching modifier\" in the model namespace because of the design principle of data immutability. The model is a state machine for a flow instance. As a temporary store, we can use it for this purpose without side effect that the user application would accidentally modify a value of the flow's input.","title":"Simple type matching and conversion"},{"location":"guides/CHAPTER-4/#convenient-data-mapping-using-model-variable","text":"To address the common use case of using a model variable as an intermediate value, the system supports the following formats for input data mapping and output data mapping. // 2-part data mapping format LHS -> RHS // 3-part data mapping format LHS -> model.variable -> RHS For the 2-part data mapping format, there are left-hand-side and right-hand-side where the value retrieved from the left-hand-side variable is mapped to the right-hand-side. The 3-part data mapping allows us to use a model variable as an intermediate for simple type matching. In the previous example, it uses two entries to convert a HTTP path parameter from a text string to a number and set the number as input argument. The configuration syntax can be simplified as follows: - input: - 'input.path_parameter.userid -> model.userid:int -> userid' The above 3-part data mapping entry will be expanded into two entries internally. This extra processing is done at the \"CompileFlows\" step and thus there is no impact to the task execution speed. Please note that the 3-part data mapping format is not supported when the left-hand-side is a text constant. It is because a text constant may contain any special characters including the mapping signature -> .","title":"Convenient data mapping using model variable"},{"location":"guides/CHAPTER-4/#metadata-for-each-flow-instance","text":"For each flow instance, the state machine in the \"model\" namespace provides the following metadata that you can use in the input/output data mapping. For example, you can set this for an exception handler to log additional information. Type Keyword Comment Flow ID model.flow The ID of the event flow config Trace ID model.trace Optional traceId when tracing is turned on Correlation ID model.cid Correlation ID of the inbound request","title":"Metadata for each flow instance"},{"location":"guides/CHAPTER-4/#special-handling-for-header","text":"When function input keyword header is specified in the \"right hand side\" of an input data mapping statement, it refers to the input event envelope's headers. Therefore, it assumes the \"left hand side\" to resolve into a Map object of key-values. Otherwise, it will reject the input data mapping statement with an error like this: Invalid input mapping 'text(ok) -> header', expect: Map, Actual: String When function input namespace header. is used, the system will map the value resolved from the \"left hand side\" statement into the specific header. For example, the input data mapping statement text(ok) -> header.demo will set \"demo=ok\" into the input event envelope's headers. When function output keyword header is specified in the \"left hand side\" of an output data mapping statement, it will resolve as a Map from the function output event envelope's headers. Similarly, when function output namespace header. is used, the system will resolve the value from a specific key of the function output event envelope's headers.","title":"Special handling for header"},{"location":"guides/CHAPTER-4/#function-input-and-output","text":"To support flexible input data mapping, the input to a function must be either Map<String, Object> or PoJo . However, the output (i.e. result set) of a function can be Map, PoJo or Java primitive. Your function should implement the TypedLambdaFunction interface to configure input and output. Since a data structure is passed to your function's input argument as key-values, you may create a PoJo class to deserialize the data structure. To tell the system that your function is expecting input as a PoJo, you can use the special notation * on the right hand side. For example, the following entry tells the system to set the value in \"model.dataset\" as a PoJo input. - input: - 'model.dataset -> *' Note : If the value from the left hand side is not a map, the system will ignore the input mapping command and print out an error message in the application log.","title":"Function input and output"},{"location":"guides/CHAPTER-4/#setting-function-input-headers","text":"When function input body is used to hold a PoJo, we may use function input headers to pass other arguments to the function without changing the data structure of a user defined PoJo. In the following example, the HTTP query parameter \"userid\" will be mapped to the function input header key \"user\" and the HTTP request body will be mapped to the function input body. - input: - 'input.query.userid -> header.user' - 'input.body -> *' process: 'my.user.function' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body'","title":"Setting function input headers"},{"location":"guides/CHAPTER-4/#task-types","text":"","title":"Task types"},{"location":"guides/CHAPTER-4/#decision-task","text":"A decision task makes decision to select the next task to execute. It has the tag execution=decision . In the output data mapping section, it must map the corresponding result set or its key-value to the decision object. The \"next\" tag contains a list of tasks to be selected based on the decision value. If decision value is boolean, a true value will select the first task. Otherwise, the second task will be selected. If decision value is an integer, the number should start from 1 where the corresponding \"next\" task will be selected. tasks: - input: - 'input.query.decision -> decision' process: 'simple.decision' output: - 'result -> model.decision' - 'result -> decision' description: 'Simple decision test' execution: decision next: - 'decision.case.one' - 'decision.case.two'","title":"Decision task"},{"location":"guides/CHAPTER-4/#response-task","text":"A response task will provide result set as a flow output or \"response\". A response task allows the flow to respond to the user or caller immediately and then move on to the next task asynchronously. For example, telling the user that it has accepted a request and then moving on to process the request that may take longer time to run. A response task has the tag execution=response and a \"next\" task. tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.pojo' - 'result -> output.body' description: 'Pass a pojo to another task' execution: response next: - 'sequential.two'","title":"Response task"},{"location":"guides/CHAPTER-4/#end-task","text":"An end task indicates that it is the last task of the transaction processing in a flow. If the flow has not executed a response task, the end task will generate the response. Response is defined by output data mapping. This task has the tag execution=end . For example, the greeting task in the unit tests is an end task. - input: - 'input.path_parameter.user -> user' process: 'greeting.demo' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: end","title":"End task"},{"location":"guides/CHAPTER-4/#sequential-task","text":"Upon completion of a sequential task, the next task will be executed. This task has the tag execution=sequential . In the following example, sequential.two will be executed after sequential.one . tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.pojo' description: 'Pass a pojo to another task' execution: sequential next: - 'sequential.two'","title":"Sequential task"},{"location":"guides/CHAPTER-4/#parallel-task","text":"Upon completion of a parallel task, all tasks in the \"next\" task list will be executed in parallel. This task has the tag execution=parallel . In this example, parallel.one and parallel.two will run after begin.parallel.test tasks: - input: - 'int(2) -> count' process: 'begin.parallel.test' output: [] description: 'Setup counter for two parallel tasks' execution: parallel next: - 'parallel.one' - 'parallel.two'","title":"Parallel task"},{"location":"guides/CHAPTER-4/#fork-n-join-task","text":"Fork-n-join is a parallel processing pattern. A \"fork\" task will execute multiple \"next\" tasks in parallel and then wait for the result sets before running the \"join\" task. This task has the tag execution=fork . It must have a list of \"next\" tasks and a \"join\" task. It may look like this: tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.pojo' description: 'Pass a pojo to another task' execution: fork next: - 'echo.one' - 'echo.two' join: 'join.task'","title":"Fork-n-join task"},{"location":"guides/CHAPTER-4/#dynamic-fork-n-join-task","text":"A special version of the fork-n-join pattern is called dynamic fork-n-join which refers to parallel processing of multiple instances of the same \"next\" task for each element in a list. For example, you have a list of 100 elements in an incoming request and each element would be processed by the same backend service. You want to process the 100 elements in parallel by multiple instances of a service wraper that connects to the backend service. The use case can be configured like this: tasks: - input: - 'input.elements -> elements' process: 'data.validation' output: - 'result.elements -> model.elements' description: 'Validate list of elements' execution: fork source: 'model.elements' next: - 'element.processor' join: 'join.task' - name: 'element.processor' input: - 'model.elements.ITEM -> item' - 'model.elements.INDEX -> index' process: 'v1.element.processor' output: [] description: 'Hello world' execution: sink To handle this special use case, you can add a source parameter in the fork task. The \"source\" parameter tells the system which model variable holds the list of elements. You should only configure a single \"next\" task. The system will spin up parallel instances of the next task to handle each element from the model variable containing the list. In the input data mapping section, there are two special suffixes .ITEM and .INDEX . The system will iterate the list of elements and spin up an instance of the \"next\" task to retrieve the element (item) and index of the element in the list. The two special suffixes are relevant only when adding to the model variable configured in the \"source\" parameter. Important : The model variables with special suffixes '.ITEM' and '.INDEX' are virtual objects for the purpose of mapping as input arguments to a task. They cannot be used as regular model variables. Dynamic fork-n-join is designed to execute the same task for a list of elements in parallel. It does not support subflow. i.e. the \"process\" tag of the \"next\" task cannot be a subflow.","title":"Dynamic fork-n-join task"},{"location":"guides/CHAPTER-4/#sink-task","text":"A sink task is a task without any next tasks. Sink tasks are used by fork-n-join and pipeline tasks as reusable modules. This task has the tag execution=sink . - input: - 'text(hello-world-two) -> key2' process: 'echo.two' output: - 'result.key2 -> model.key2' description: 'Hello world' execution: sink","title":"Sink task"},{"location":"guides/CHAPTER-4/#special-consideration-for-parallelism","text":"The execution types (parallel and fork-n-join) are designed for parallel processing. Usually, parallel processing would improve performance. However, spinning up a large number of concurrent sessions to a slower backend service may create performance bottleneck. In fact, a massive number of concurrent sessions to a single backend would bring down the target service. This is an unintended \"denial of service\" attack. The dynamic fork-n-join execution style should be handled with caution because it can easily spin up a large number of parallel instances of the same task. To control parallelism, you can set a smaller number of concurrent \"instances\" for the \"next\" task using the \"instances\" parameter in the \"PreLoad\" annotation of the task. For example, you have 100 elements in a list but the maximum instances of the task can be set to 20. This would reduce the concurrency to 20, thus allowing you to manage performance according to available infrastructure resources. Therefore, processing 100 elements would require 5 rounds of 20 parallel executions and this orderly execution is supported by the underlying reactive event system.","title":"Special consideration for parallelism"},{"location":"guides/CHAPTER-4/#pipeline-feature","text":"Pipeline is an advanced feature of Event Script.","title":"Pipeline feature"},{"location":"guides/CHAPTER-4/#pipeline-task","text":"A pipeline is a list of tasks that will be executed orderly within the current task. When the pipeline is done, the system will execute the \"next\" task. This task has the tag execution=pipeline . tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.pojo' description: 'Pass a pojo to another task' execution: pipeline pipeline: - 'echo.one' - 'echo.two' next: - 'echo.three' Some special uses of pipelines include \"for/while-loop\" and \"continue/break\" features.","title":"Pipeline task"},{"location":"guides/CHAPTER-4/#simple-for-loop","text":"In the following example, the loop.statement contains a for-loop that uses a variable in the state machine to evaluate the loop. In this example, the pipeline will be executed three times before passing control to the \"next\" task. tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.pojo' description: 'Pass a pojo to another task' execution: pipeline loop: statement: 'for (model.n = 0; model.n < 3; model.n++)' pipeline: - 'echo.one' - 'echo.two' - 'echo.three' next: - 'echo.four'","title":"Simple for-loop"},{"location":"guides/CHAPTER-4/#simple-while-loop","text":"The loop.statement may use a \"while loop\" syntax like this: loop: statement: 'while (model.running)' To exit the above while loop, one of the functions in the pipeline should return a boolean \"false\" value with output \"data mapping\" to the model.running variable.","title":"Simple while loop"},{"location":"guides/CHAPTER-4/#for-loop-with-breakcontinue-decision","text":"In the following example, the system will evaluate if the model.quit variable is true. If yes, the break or continue condition will be executed. The state variable is obtained after output data mapping and any task in the pipeline can set a key-value for mapping into the state variable. tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.seq -> sequence' process: 'sequential.one' output: - 'result -> model.pojo' description: 'Pass a pojo to another task' execution: pipeline loop: statement: 'for (model.n = 0; model.n < 3; model.n++)' condition: 'if (model.quit) break' pipeline: - 'echo.one' - 'echo.two' - 'echo.three' next: - 'echo.four' Note that the \"condition\" parameter can be a single condition or a list of conditions. In the following example, the system will evaluate both the model.quit and model.jump values. loop: statement: 'for (model.n = 0; model.n < 3; model.n++)' condition: - 'if (model.quit) break' - 'if (model.jump) break'","title":"For loop with break/continue decision"},{"location":"guides/CHAPTER-4/#handling-exception","text":"You can define exception handler at the top level or at the task level. Exception is said to occur when a user function throws exception or returns an EventEnvelope object with a status code equals to or larger than 400. The event status uses the same numbering scheme as HTTP exception status code. Therefore, status code less than 400 is not considered an exception.","title":"Handling exception"},{"location":"guides/CHAPTER-4/#top-level-exception-handler","text":"Top-level exception handler is a \"catch-all\" handler. You can define it like this: flow: id: 'greetings' description: 'Simplest flow of one task' ttl: 10s exception: 'v1.my.exception.handler' In this example, the v1.my.exception.handler should point to a corresponding exception handler that you provide. The following input arguments will be delivered to your function when exception happens. Key Description status Exception status code message Error message stack Stack trace in a text string The exception handler function can be an \"end\" task to abort the transaction or a decision task to take care of the exception. For example, the exception handler can be a \"circuit-breaker\" to retry a request. Note : for efficiency, stack trace transport is limited to the first 10 lines.","title":"Top-level exception handler"},{"location":"guides/CHAPTER-4/#task-level-exception-handler","text":"You can attach an exception handler to a task. One typical use is the \"circuit breaker\" pattern. In the following example, the user function \"breakable.function\" may throw an exception for some error condition. The exception will be caught by the \"v1.circuit.breaker\" function. - input: - 'input.path_parameter.accept -> accept' - 'model.attempt -> attempt' process: 'exception.simulator' output: - 'int(0) -> model.attempt' - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'This demo function will break until the \"accept\" number is reached' execution: end exception: 'resilience.handler' The configuration for the circuit breaker function may look like this: - input: - 'error.code -> status' - 'error.message -> message' - 'model.attempt -> attempt' - 'int(2) -> max_attempts' process: 'resilience.handler' output: - 'result.attempt -> model.attempt' - 'result.decision -> decision' - 'result.status -> model.status' - 'result.message -> model.message' description: 'Just a demo circuit breaker' execution: decision next: - 'breakable.function' - 'abort.request' An exception handler will be provided with the \"error\" object that contains error code, error message and a stack trace. The exception handler can inspect the error object to make decision of the next step. For circuit breaker, we can keep the number of retry attempts in the state machine under \"model.attempt\" or any key name that you prefer. In the above example, it sets an integer constant of 2 for the maximum attempts. The circuit breaker can then evaluate if the number of attempts is less than the maximum attempts. If yes, it will return a decision of \"true\" value to tell the system to route to the \"breakable.function\" again. Otherwise, it will return a decision of \"false\" value to abort the request. A more sophisticated circuit breaker may be configured with \"alternative execution paths\" depending on the error status and stack trace. In this case, the decision value can be a number from 1 to n that corresponds to the \"next\" task list. Exception handlers may be used in both queries and transactions. For a complex transaction, the exception handler may implement database rollback logic or recovery mechanism.","title":"Task-level exception handler"},{"location":"guides/CHAPTER-4/#best-practice","text":"When a task-level exception handler throws exception, it will be caught by the top-level exception handler, if any. A top-level exception handler should not throw exception. Otherwise it may go into an exception loop. Therefore, we recommend that an exception handler should return regular result set in a PoJo or a Map object. An example of task-level exception handler is shown in the \"HelloException.class\" in the \"task\" folder where it set the status code in the result set so that the system can map the status code from the result set to the next task or to the HTTP output status code.","title":"Best practice"},{"location":"guides/CHAPTER-4/#advanced-features","text":"","title":"Advanced features"},{"location":"guides/CHAPTER-4/#no-operation-function","text":"A convenient no-operation function with the route name no.op is available. It can be used when you want to perform some input/output data mapping without executing any business logic.","title":"No-operation function"},{"location":"guides/CHAPTER-4/#generic-resilience-handler-function","text":"Another useful built-in function is a resilience handler with the route name resilience.handler . It is a generic resilience handler. It will retry, abort, use an alternative path or exercise a brief backoff. Figure 2 - Resilience Handler The following parameters (input data mapping) define behavior for the handler: max_attempts - when the handler has used all the attempts, it will abort. attempt - this tells the handler how many attempts it has tried status - you should map the error status code in this field message - you should map the error message in this field alternative - the optional codes and range of status codes to tell the handler to reroute delay - the delay in milliseconds before exercising retry or reroute. Minimum value is 10 ms. Delay is skipped for the first retry. This slight delay is a protection mechanism. Optional backoff behavior: cumulative - the total number of failures since last success or backoff reset if any backoff - the time of a backoff period (epoch milliseconds) if any backoff_trigger - the total number of failures that triggers a backoff backoff_seconds - the time to backoff after an abort has occurred. During this period, It will abort without updating attempt. This avoids overwhelming the target service that may result in recovery storm. Return value (output data mapping): result.attempt - the handler will clear or increment this counter result.cumulative - the handler will clear or increment this counter. Not set if \"backoff_trigger\" is not given in input. result.decision - 1, 2 or 3 where 1=retry, 2=abort, 3=reroute that corresponds to the next tasks result.status - the status code that the handler aborts the retry or reroute. Not set if retry or reroute. result.message - the reason that the handler aborts the retry or reroute. Not set if retry or reroute. result.backoff - the time of a backoff period (epoch milliseconds). Not set if not in backoff mode. Note : \"result.attempt\" should be saved in the state machine with the \"model.\" namespace. \"result.cumulative\" and \"result.backoff\" should be saved in the temporary file system or an external state machine. For more details, please refer to the event script resilience-demo.yml in the event-script-engine's test resources folder and the unit test resilienceHandlerTest() under the FlowTests class. Extract of the task configuration for the resilience handler is shown as follows. In the following example, \"my.task\" is the function that is configured with the 'resilience.handler' as an exception handler. The input data mapping tells the handler to enter into \"backoff\" period when the cumulative failure count reaches the \"backoff_trigger\" threshold of 3. After that, all requests will be aborted until the backoff period expires. - input: - 'error.code -> status' - 'error.message -> message' - 'model.attempt -> attempt' - 'int(10) -> max_attempts' - 'text(401, 403-404) -> alternative' - 'file(text:/tmp/resilience/cumulative) -> cumulative' - 'file(text:/tmp/resilience/backoff) -> backoff' - 'int(3) -> backoff_trigger' - 'int(2) -> backoff_seconds' - 'int(500) -> delay' process: 'resilience.handler' output: - 'result.status -> model.status' - 'result.message -> model.message' - 'result.attempt -> model.attempt' - 'result.decision -> decision' - 'result.backoff -> file(/tmp/resilience/backoff)' - 'result.cumulative -> file(/tmp/resilience/cumulative)' description: 'Resilience handler with alternative path and backoff features' execution: decision next: - '@retry' - 'abort.request' - 'alternative.task' You may also use this resilience handler as a template to write your own exception handler for more complex recovery use cases. Important : If you want the resilience handler to automatically retry the task that throws exception, set \"@retry\" as the first task in the \"next\" task list. When the \"backoff\" feature is enabled, you should configure the resilience handler as a gatekeeper to protect your user function. i.e. it is the \"first.task\". This allows the system to abort requests during the backoff period. If the resilience handler is used as a gatekeeper and there was no exception, it will execute the original task. However, if \"@retry\" is used as the first next task entry, it cannot resolve the original task since it has never been executed. You can add the original task to the first entry with \"|\" as a separator. e.g. @retry | my.task where \"my.task\" is the original task. This tells the system to route the request to \"my.task\" when there is no exception in the first place. The \"@retry\" keyword can only be used in the first entry of the \"next\" task list.","title":"Generic resilience handler function"},{"location":"guides/CHAPTER-4/#external-state-machine","text":"The in-memory state machine is created for each query or transaction flow and it is temporal. For complex transactions or long running work flows, you would typically want to externalize some transaction states to a persistent store such as a distributed cache system or a high performance key-value data store. In these use cases, you can implement an external state machine function and configure it in a flow. Below is an example from a unit test. When you externalize a key-value to an external state machine, you must configure the route name (aka level-3 functional topic) of the external state machine. Note : Passing a null value to a key of an external state machine means \"removal\". external.state.machine: 'v1.ext.state.machine' tasks: - input: # A function can call an external state machine using input or output mapping. # In this example, it calls external state machine from input data mapping. - 'input.path_parameter.user -> ext:/${app.id}/user' - 'input.body -> model.body' # demonstrate saving constant to state machine and remove it using model.none - 'text(world) -> ext:hello' - 'model.none -> ext:hello' process: 'no.op' output: - 'text(application/json) -> output.header.content-type' # It calls external state machine again from output data mapping - 'input.body -> ext:/${app.id}/body' - 'input.body -> output.body' - 'text(message) -> ext:test' - 'model.none -> ext:test' description: 'Hello World' execution: end The \"external.state.machine\" parameter is optional. When present, the system will send a key-value from the current flow instance's state machine to the function implementing the external state machine. The system uses the \"ext:\" namespace to externalize a state machine's key-value. Note : The delivery of key-values to the external state machine is asynchronous. Therefore, please assume eventual consistency. You should implement a user function as the external state machine. The input interface contract to the external state machine for saving a key-value is: header.type = 'put' header.key = key body.data = value Your function should save the input key-value to a persistent store. In another flow that requires the key-value, you can add an initial task to retrieve from the persistent store and do \"output data mapping\" to save to the in-memory state machine so that your transaction flow can use the persisted key-values to continue processing. In the unit tests of the event-script-engine subproject, these two flows work together: externalize-put-key-value externalize-get-key-value IMPORTANT : Events to an external state machine are delivered asynchronously. If you want to guarantee message sequencing, please do not set the \"instances\" parameter in the PreLoad annotation. To illustrate a minimalist implementation, below is an example of an external state machine in the event-script-engine's unit test section. @PreLoad(route = \"v1.ext.state.machine\") public class ExternalStateMachine implements LambdaFunction { private static final Logger log = LoggerFactory.getLogger(ExternalStateMachine.class); private static final ManagedCache store = ManagedCache.createCache(\"state.machine\", 5000); private static final String TYPE = \"type\"; private static final String PUT = \"put\"; private static final String GET = \"get\"; private static final String REMOVE = \"remove\"; private static final String KEY = \"key\"; private static final String DATA = \"data\"; @SuppressWarnings(\"unchecked\") @Override public Object handleEvent(Map<String, String> headers, Object input, int instance) { if (!headers.containsKey(KEY)) { throw new IllegalArgumentException(\"Missing key in headers\"); } String type = headers.get(TYPE); String key = headers.get(KEY); if (PUT.equals(type) && input instanceof Map) { Map<String, Object> dataset = (Map<String, Object>) input; var data = dataset.get(DATA); if (data != null) { log.info(\"Saving {} to store\", key); store.put(key, data); return true; } } if (GET.equals(type)) { Object v = store.get(key); if (v != null) { log.info(\"Retrieve {} from store\", key); return v; } else { return null; } } if (REMOVE.equals(type)) { if (store.exists(key)) { store.remove(key); log.info(\"Removed {} from store\", key); return true; } else { return false; } } return false; } } For more sophisticated operation, you may also configure the external state machine as a \"flow\" like this: external.state.machine: 'flow://ext-state-machine' You can then define the flow for \"ext-state-machine\" like this: flow: id: 'ext-state-machine' description: 'Flow to execute an external state machine' ttl: 10s first.task: 'v1.ext.state.machine' tasks: - input: - 'input.header.key -> header.key' - 'input.header.type -> header.type' - 'input.body.data -> data' process: 'v1.ext.state.machine' output: [] description: 'Execute external state machine' execution: end Note : By definition, external state machine flow is outside the scope of the calling flow.","title":"External state machine"},{"location":"guides/CHAPTER-4/#future-task-scheduling","text":"You may add a \u201cdelay\u201d tag in a task so that it will be executed later. This feature is usually used for unit tests or \"future task scheduling\". Since the system is event-driven and non-blocking, the delay is simulated by event scheduling. It does not block the processing flow. Type Value Example Fixed delay Milliseconds delay: '1000 ms' Variable delay State machine variable delay: model.delay Note that the \"ms\" suffix is optional for documentation purpose. It denotes milliseconds if present. When delay is set to a state variable that its value is not configured by a prior data mapping, the delay command will be ignored. An example task that has an artificial delay of 2 seconds: tasks: - input: - 'input.path_parameter.user -> user' - 'input.query.ex -> exception' - 'text(hello world) -> greeting' process: 'greeting.test' output: - 'text(application/json) -> output.header.content-type' - 'result -> output.body' description: 'Hello World' execution: end delay: '2000 ms' Chapter-3 Home Chapter-5 REST Automation Table of Contents Build, Test and Deploy","title":"Future task scheduling"},{"location":"guides/CHAPTER-5/","text":"Build, Test and Deploy The first step in writing an application is to create an entry point for your application. Main application A minimalist main application template is shown as follows: @MainApplication public class MainApp implements EntryPoint { public static void main(String[] args) { AutoStart.main(args); } @Override public void start(String[] args) { // your startup logic here log.info(\"Started\"); } } You must have at least one \"main application\" module because it is mandatory. Note : Please adjust the parameter \"web.component.scan\" in application.properties to point to your user application package(s) in your source code project. If your application does not require additional startup logic, you may just print a greeting message. The AutoStart.main() statement in the \"main\" method is used when you want to start your application within the IDE. You can \"right-click\" the main method and select \"run\". You can also build and run the application from command line like this: cd sandbox/mercury-composable/examples/lambda-example mvn clean package java -jar target/lambda-example-4.0.16.jar The lambda-example is a sample application that you can use as a template to write your own code. Please review the pom.xml and the source directory structure. In the lambda-example project root, you will find the following directories: src/main/java src/test/java Since all functions are connected using the in-memory event bus, you can test any function by sending events from a unit test. Writing your functions Please follow the step-by-step learning guide in Chapter-1 to write your own functions. You can then configure new REST endpoints to use your new functions. In Chapter-2 , we have discussed the two function execution strategies to optimize your application to the full potential of stability, performance and throughput. HTTP forwarding In Chapter-3 , we have presented the configuration syntax for the \"rest.yaml\" REST automation definition file. Please review the sample rest.yaml file in the lambda-example project. You may notice that it has an entry for HTTP forwarding. The following entry in the sample rest.yaml file illustrates an HTTP forwarding endpoint. In HTTP forwarding, you can replace the \"service\" route name with a direct HTTP target host. You can do \"URL rewrite\" to change the URL path to the target endpoint path. In the below example, /api/v1/* will be mapped to /api/* in the target endpoint. - service: \"http://127.0.0.1:${rest.server.port}\" trust_all_cert: true methods: ['GET', 'PUT', 'POST'] url: \"/api/v1/*\" url_rewrite: ['/api/v1', '/api'] timeout: 20 cors: cors_1 headers: header_1 tracing: true Sending HTTP request event to more than one service One feature in REST automation \"rest.yaml\" configuration is that you can configure more than one function in the \"service\" section. In the following example, there are two function route names (\"hello.world\" and \"hello.copy\"). The first one \"hello.world\" is the primary service provider. The second one \"hello.copy\" will receive a copy of the incoming event. This feature allows you to write new version of a function without disruption to current functionality. Once you are happy with the new version of function, you can route the endpoint directly to the new version by updating the \"rest.yaml\" configuration file. - service: [\"hello.world\", \"hello.copy\"] Writing your first unit test Please refer to \"rpcTest\" method in the \"HelloWorldTest\" class in the lambda-example to get started. In unit test, we want to start the main application so that all the functions are ready for tests. First, we write a \"TestBase\" class to use the BeforeClass setup method to start the main application like this: public class TestBase { private static final AtomicInteger seq = new AtomicInteger(0); @BeforeClass public static void setup() { if (seq.incrementAndGet() == 1) { AutoStart.main(new String[0]); } } } The atomic integer \"seq\" is used to ensure the main application entry point is executed only once. A typical unit test may look like this: @SuppressWarnings(\"unchecked\") @Test public void rpcTest() throws InterruptedException, ExecutionException { Utility util = Utility.getInstance(); String NAME = \"hello\"; String ADDRESS = \"world\"; String TELEPHONE = \"123-456-7890\"; DemoPoJo pojo = new DemoPoJo(NAME, ADDRESS, TELEPHONE); PostOffice po = new PostOffice(\"unit.test\", \"12345\", \"POST /api/hello/world\"); EventEnvelope request = new EventEnvelope().setTo(\"hello.world\").setBody(pojo.toMap()); EventEnvelope response = po.request(request, 8000).get(); assert response != null; assertInstanceOf(Map.class, response.getBody()); MultiLevelMap map = new MultiLevelMap((Map<String, Object>) response.getBody()); assertEquals(NAME, map.getElement(\"body.name\")); assertEquals(ADDRESS, map.getElement(\"body.address\")); assertEquals(TELEPHONE, map.getElement(\"body.telephone\")); assertEquals(util.date2str(pojo.time), map.getElement(\"body.time\")); } Note that the PostOffice instance can be created with tracing information in a Unit Test. The above example tells the system that the sender is \"unit.test\", the trace ID is 12345 and the trace path is \"POST /api/hello/world\". Your second unit test Let's do a unit test for PoJo. In this second unit test, it sends a RPC request to the \"hello.pojo\" function that is designed to return a SamplePoJo object with some mock data. Please refer to \"pojoRpcTest\" method in the \"PoJoTest\" class in the lambda-example for details. The unit test verifies that the \"hello.pojo\" has correctly returned the SamplePoJo object with the pre-defined mock value. @Test public void pojoRpcTest() throws InterruptedException { Integer ID = 1; String NAME = \"Simple PoJo class\"; String ADDRESS = \"100 World Blvd, Planet Earth\"; BlockingQueue<EventEnvelope> bench = new ArrayBlockingQueue<>(1); PostOffice po = new PostOffice(\"unit.test\", \"20001\", \"GET /api/hello/pojo\"); EventEnvelope request = new EventEnvelope().setTo(\"hello.pojo\").setHeader(\"id\", \"1\"); po.asyncRequest(request, 8000).onSuccess(bench::add); EventEnvelope response = bench.poll(10, TimeUnit.SECONDS); assert response != null; assertEquals(HashMap.class, response.getBody().getClass()); SamplePoJo pojo = response.getBody(SamplePoJo.class); assertEquals(ID, pojo.getId()); assertEquals(NAME, pojo.getName()); assertEquals(ADDRESS, pojo.getAddress()); } Note that you can use the built-in serialization API to restore a PoJo like this: SamplePoJo pojo = response.getBody(SamplePoJo.class) Convenient utility classes The Utility and MultiLevelMap classes are convenient tools for unit tests. In the above example, we use the Utility class to convert a date object into a UTC timestamp. It is because date object is serialized as a UTC timestamp in an event. The MultiLevelMap supports reading an element using the convenient \"dot and bracket\" format. For example, given a map like this: { \"body\": { \"time\": \"2023-03-27T18:10:34.234Z\", \"hello\": [1, 2, 3], \"complex\": [ {\"key\": \"value1\"}, {\"key\": \"value2\"} ] } } Example Command Result 1 map.getElement(\"body.time\") 2023-03-27T18:10:34.234Z 2 map.getElement(\"body.hello[2]\") 3 3 map.getElement(\"body.complex[1].key\") value2 4 map.getElements(\"body.complex[*].key\") [ value1, value2 ] Example-4 above uses the \"getElements\" in plural form to indicate that it is retrieving a list of elements using a \"wildcard\" index. For simplicity, it does not support more than one wildcard index in the search path. Event Flow mocking framework We recommend using Event Script to write Composable application for highest level of decoupling. Event Script supports sophisticated event choreography by configuration. In Event Script, you have a event flow configuration and a few Composable functions in an application. Composable functions are self-contained with zero dependencies with other composable functions. You can invoke an event flow from an event flow adapter. The most common flow adapter is the \"HTTP flow adapter\" and it is available as a built-in module in the event-script-engine module in the system. You can associate many REST endpoints to the HTTP flow adapter. Since function routes for each composable function is defined in a event flow configuration and the same function route may be used for more than one task in the flow, the system provides a mock helper class called \"EventScriptMock\" to let your unit tests to override a task's function routes during test. In the following unit test example for a \"pipeline\" test, we created a mock function \"my.mock.function\" to override the \"no.op\" function that is associated with the first task \"echo.one\" in a pipeline. The original \"no.op\" function is an echo function. The mocked function increments a counter in addition to just echoing the input payload. In this fashion, the unit test can count the number of iteration of a pipeline to validate the looping feature of a pipeline. The unit test programmatically registers the mock function and then release it from the event loop when the test finishes. @SuppressWarnings(\"unchecked\") @Test void pipelineForLoopTest() throws InterruptedException { Platform platform = Platform.getInstance(); // The first task of the flow \"for-loop-test\" is \"echo.one\" that is using \"no.op\". // We want to override no.op with my.mock.function to demonstrate mocking a function // for a flow. var ECHO_ONE = \"echo.one\"; var MOCK_FUNCTION = \"my.mock.function\"; var iteration = new AtomicInteger(0); LambdaFunction f = (headers, body, instance) -> { var n = iteration.incrementAndGet(); log.info(\"Iteration-{} {}\", n, body); return body; }; platform.registerPrivate(MOCK_FUNCTION, f, 1); // override the function for the task \"echo.one\" to the mock function var mock = new EventScriptMock(\"for-loop-test\"); var previousRoute = mock.getFunctionRoute(ECHO_ONE); var currentRoute = mock.assignFunctionRoute(ECHO_ONE, MOCK_FUNCTION).getFunctionRoute(ECHO_ONE); assertEquals(\"no.op\", previousRoute); assertEquals(MOCK_FUNCTION, currentRoute); final BlockingQueue<EventEnvelope> bench = new ArrayBlockingQueue<>(1); final long TIMEOUT = 8000; String USER = \"test-user\"; int SEQ = 100; AsyncHttpRequest request = new AsyncHttpRequest(); request.setTargetHost(HOST).setMethod(\"GET\").setHeader(\"accept\", \"application/json\"); request.setUrl(\"/api/for-loop/\"+USER).setQueryParameter(\"seq\", SEQ); EventEmitter po = EventEmitter.getInstance(); EventEnvelope req = new EventEnvelope().setTo(HTTP_CLIENT).setBody(request); po.asyncRequest(req, TIMEOUT).onSuccess(bench::add); EventEnvelope res = bench.poll(TIMEOUT, TimeUnit.MILLISECONDS); assert res != null; assertInstanceOf(Map.class, res.getBody()); Map<String, Object> result = (Map<String, Object>) res.getBody(); assertTrue(result.containsKey(\"data\")); PoJo pojo = SimpleMapper.getInstance().getMapper().readValue(result.get(\"data\"), PoJo.class); assertEquals(SEQ, pojo.sequence); assertEquals(USER, pojo.user); assertEquals(3, result.get(\"n\")); assertEquals(3, iteration.get()); platform.release(MOCK_FUNCTION); } When the event flow finishes, you will see an \"end-of-flow\" report like this. It shows that the function route for the \"echo.one\" task has been changed to \"my.mock.function\". This end-of-flow log is useful during application development and tests so that the developer knows exactly which function has been executed. Flow for-loop-test (0afcf555fc4141f4a16393422e468dc9) completed. Run 11 tasks in 28 ms. [ sequential.one, echo.one(my.mock.function), echo.two(no.op), echo.three(no.op), echo.one(my.mock.function), echo.two(no.op), echo.three(no.op), echo.one(my.mock.function), echo.two(no.op), echo.three(no.op), echo.four(no.op) ] Inspecting the state machine using EventScriptMock The state machine of a \"flow instance\" is not accessible directly by a user task. To inspect the state machine in a unit test, you can use the setMonitorBeforeTask and setMonitorAfterTask methods. The former tells the system to send a copy of the state machine to a composable function after \"input data mapping\" but before entering a task. The latter sends a copy of the state machine to a composable function after a task is completed. The following code segment from a unit test illustrates this feature. The function \"before.task.monitor\" will get a copy of the state machine before \"my.task\" executes. Similarly, the function \"after.task.monitor\" will obtain a copy of the state machine after \"my.task\" finishes execution. var platform = Platform.getInstance(); var mock = new EventScriptMock(\"parent-greetings\"); TypedLambdaFunction<Map<String, Object>, Void> f1 = (headers, input, instance) -> { before.add(input); return null; }; platform.registerPrivate(\"before.task.monitor\", f1, 1); TypedLambdaFunction<Map<String, Object>, Void> f2 = (headers, input, instance) -> { after.add(input); return null; }; platform.registerPrivate(\"after.task.monitor\", f2, 1); mock.setMonitorBeforeTask(\"my.task\", \"before.task.monitor\") .setMonitorAfterTask(\"my.task\", \"after.task.monitor\"); Deployment The pom.xml is pre-configured to generate an executable JAR. The following is extracted from the pom.xml. The main class is AutoStart that will load the \"main application\" and use it as the entry point to run the application. <plugin> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-maven-plugin</artifactId> <configuration> <mainClass>org.platformlambda.core.system.AutoStart</mainClass> </configuration> <executions> <execution> <id>build-info</id> <goals> <goal>build-info</goal> </goals> </execution> </executions> </plugin> Composable application is designed to be deployable using Kubernetes or serverless. A sample Dockerfile for an executable JAR may look like this: FROM mcr.microsoft.com/openjdk/jdk:21-ubuntu EXPOSE 8083 WORKDIR /app COPY target/rest-spring-3-example-4.2.3.jar . ENTRYPOINT [\"java\",\"-jar\",\"rest-spring-3-example-4.2.3.jar\"] Distributed tracing The system has a built-in distributed tracing feature. You can enable tracing for any REST endpoint by adding \"tracing=true\" in the endpoint definition in the \"rest.yaml\" configuration file. You may also upload performance metrics from the distributed tracing data to your favorite telemetry system dashboard. To do that, you can implement a custom metrics function with the route name distributed.trace.forwarder . The input to the function will be a HashMap like this: trace={path=/api/upload/demo, service=hello.upload, success=true, origin=2023032731e2a5eeae8f4da09f3d9ac6b55fb0a4, exec_time=77.462, start=2023-03-27T19:38:30.061Z, from=http.request, id=12345, round_trip=132.296, status=200} The system will detect if distributed.trace.forwarder is available. If yes, it will forward performance metrics from distributed trace to your custom function. Request-response journaling Optionally, you may also implement a custom audit function named transaction.journal.recorder to monitor request-response payloads. To enable journaling, please add this to the application.properties file. journal.yaml=classpath:/journal.yaml and add the \"journal.yaml\" configuration file to the project's resources folder with content like this: journal: - \"my.test.function\" - \"another.function\" In the above example, the \"my.test.function\" and \"another.function\" will be monitored and their request-response payloads will be forwarded to your custom audit function. The input to your audit function will be a HashMap containing the performance metrics data and a \"journal\" section with the request and response payloads in clear form. IMPORTANT : journaling may contain sensitive personally identifiable data and secrets. Please check security compliance before storing them into access restricted audit data store. Performance tuning The composable framework is designed for high concurrency using virtual threads and new channel I/O (NIO). As a result, it can generate a massive volume of outgoing traffic to your system of records. If not managed properly, massive parallelism can actually degrade overall performance because it can become a form of unintended \"denial of service\" attack. Composable applications are, by definition, cloud native. You can scale your applications horizontally. In addition, composable functions in each application instance can be scaled vertically using Java virtual thread technology. Back-pressure is automated so your application usually does not need to do advanced coding to do flow control. For each composable function, you can define concurrency using the \"instances\" parameter in the \"PreLoad\" class annotation. Note that if the function makes outgoing calls to an external dependency that is slow, it can consume \"worker\" threads very quickly because the \"workers\" are waiting for a response from the external dependency. Therefore, you should increase the concurrency count to adjust for your performance requirement. If you configure parallel processing, especially when using the parallel pipeline method, it can spin up instances of composable functions in an \"uncontrolled\" manner. It would result in making too many parallel requests to an external dependency. For external system that runs using legacy technology, it can easily be overwhelmed, resulting in performance bottleneck. It can block the calling functions from a composable application and thus the outcome can be suboptimal. When configuring parallel calls, please consider end-to-end connectivity and potential bottlenecks. Since event flow instances are already running in parallel, configuring parallel requests within a single event flow would reduce performance. An orderly executed pipeline would be faster than a parallel pipeline that is \"uncontrolled\". On the other hands, fork-n-join uses a predetermined number of parallel tasks and it is easier to control parallelism. Performance tuning is an art than a science. A holistic view of end-to-end performance and careful configuration of parallelism would yield good outcome. Performance metrics The built-in telemetry system offers basic performance metrics that can be visualized with a telemetry dashboard. For more advanced performance metrics, you may refer to the \"end-of-flow\" performance report. It states the sequence of task execution and their elapsed time that includes execution time of a function, routing overheads and all system overheads. It may look like this when you configure \"log.format=json\" in application.properties. { \"level\": \"INFO\", \"time\": \"2025-08-27 18:39:25.683\", \"source\": \"org.platformlambda.core.services.Telemetry.handleEvent(Telemetry.java:81)\", \"thread\": 336, \"message\": { \"trace\": { \"path\": \"GET /api/profile/100\", \"service\": \"task.executor\", \"success\": true, \"origin\": \"20250828c022812c67294a63871942c568a9e277\", \"exec_time\": 7.0, \"start\": \"2025-08-28T01:39:25.674Z\", \"from\": \"event.script.manager\", \"id\": \"9c0934a98dcf4ab1ae4b5b7b389f6d31\", \"status\": 200 }, \"annotations\": { \"execution\": \"Run 2 tasks in 7 ms\", \"tasks\": [ { \"name\": \"v1.get.profile\", \"spent\": 2.982 }, { \"name\": \"v1.decrypt.fields\", \"spent\": 1.313 } ], \"flow\": \"get-profile\" } } } Chapter-4 Home Chapter-6 Event Script Syntax Table of Contents Spring Boot","title":"Chapter-5"},{"location":"guides/CHAPTER-5/#build-test-and-deploy","text":"The first step in writing an application is to create an entry point for your application.","title":"Build, Test and Deploy"},{"location":"guides/CHAPTER-5/#main-application","text":"A minimalist main application template is shown as follows: @MainApplication public class MainApp implements EntryPoint { public static void main(String[] args) { AutoStart.main(args); } @Override public void start(String[] args) { // your startup logic here log.info(\"Started\"); } } You must have at least one \"main application\" module because it is mandatory. Note : Please adjust the parameter \"web.component.scan\" in application.properties to point to your user application package(s) in your source code project. If your application does not require additional startup logic, you may just print a greeting message. The AutoStart.main() statement in the \"main\" method is used when you want to start your application within the IDE. You can \"right-click\" the main method and select \"run\". You can also build and run the application from command line like this: cd sandbox/mercury-composable/examples/lambda-example mvn clean package java -jar target/lambda-example-4.0.16.jar The lambda-example is a sample application that you can use as a template to write your own code. Please review the pom.xml and the source directory structure. In the lambda-example project root, you will find the following directories: src/main/java src/test/java Since all functions are connected using the in-memory event bus, you can test any function by sending events from a unit test.","title":"Main application"},{"location":"guides/CHAPTER-5/#writing-your-functions","text":"Please follow the step-by-step learning guide in Chapter-1 to write your own functions. You can then configure new REST endpoints to use your new functions. In Chapter-2 , we have discussed the two function execution strategies to optimize your application to the full potential of stability, performance and throughput.","title":"Writing your functions"},{"location":"guides/CHAPTER-5/#http-forwarding","text":"In Chapter-3 , we have presented the configuration syntax for the \"rest.yaml\" REST automation definition file. Please review the sample rest.yaml file in the lambda-example project. You may notice that it has an entry for HTTP forwarding. The following entry in the sample rest.yaml file illustrates an HTTP forwarding endpoint. In HTTP forwarding, you can replace the \"service\" route name with a direct HTTP target host. You can do \"URL rewrite\" to change the URL path to the target endpoint path. In the below example, /api/v1/* will be mapped to /api/* in the target endpoint. - service: \"http://127.0.0.1:${rest.server.port}\" trust_all_cert: true methods: ['GET', 'PUT', 'POST'] url: \"/api/v1/*\" url_rewrite: ['/api/v1', '/api'] timeout: 20 cors: cors_1 headers: header_1 tracing: true","title":"HTTP forwarding"},{"location":"guides/CHAPTER-5/#sending-http-request-event-to-more-than-one-service","text":"One feature in REST automation \"rest.yaml\" configuration is that you can configure more than one function in the \"service\" section. In the following example, there are two function route names (\"hello.world\" and \"hello.copy\"). The first one \"hello.world\" is the primary service provider. The second one \"hello.copy\" will receive a copy of the incoming event. This feature allows you to write new version of a function without disruption to current functionality. Once you are happy with the new version of function, you can route the endpoint directly to the new version by updating the \"rest.yaml\" configuration file. - service: [\"hello.world\", \"hello.copy\"]","title":"Sending HTTP request event to more than one service"},{"location":"guides/CHAPTER-5/#writing-your-first-unit-test","text":"Please refer to \"rpcTest\" method in the \"HelloWorldTest\" class in the lambda-example to get started. In unit test, we want to start the main application so that all the functions are ready for tests. First, we write a \"TestBase\" class to use the BeforeClass setup method to start the main application like this: public class TestBase { private static final AtomicInteger seq = new AtomicInteger(0); @BeforeClass public static void setup() { if (seq.incrementAndGet() == 1) { AutoStart.main(new String[0]); } } } The atomic integer \"seq\" is used to ensure the main application entry point is executed only once. A typical unit test may look like this: @SuppressWarnings(\"unchecked\") @Test public void rpcTest() throws InterruptedException, ExecutionException { Utility util = Utility.getInstance(); String NAME = \"hello\"; String ADDRESS = \"world\"; String TELEPHONE = \"123-456-7890\"; DemoPoJo pojo = new DemoPoJo(NAME, ADDRESS, TELEPHONE); PostOffice po = new PostOffice(\"unit.test\", \"12345\", \"POST /api/hello/world\"); EventEnvelope request = new EventEnvelope().setTo(\"hello.world\").setBody(pojo.toMap()); EventEnvelope response = po.request(request, 8000).get(); assert response != null; assertInstanceOf(Map.class, response.getBody()); MultiLevelMap map = new MultiLevelMap((Map<String, Object>) response.getBody()); assertEquals(NAME, map.getElement(\"body.name\")); assertEquals(ADDRESS, map.getElement(\"body.address\")); assertEquals(TELEPHONE, map.getElement(\"body.telephone\")); assertEquals(util.date2str(pojo.time), map.getElement(\"body.time\")); } Note that the PostOffice instance can be created with tracing information in a Unit Test. The above example tells the system that the sender is \"unit.test\", the trace ID is 12345 and the trace path is \"POST /api/hello/world\".","title":"Writing your first unit test"},{"location":"guides/CHAPTER-5/#your-second-unit-test","text":"Let's do a unit test for PoJo. In this second unit test, it sends a RPC request to the \"hello.pojo\" function that is designed to return a SamplePoJo object with some mock data. Please refer to \"pojoRpcTest\" method in the \"PoJoTest\" class in the lambda-example for details. The unit test verifies that the \"hello.pojo\" has correctly returned the SamplePoJo object with the pre-defined mock value. @Test public void pojoRpcTest() throws InterruptedException { Integer ID = 1; String NAME = \"Simple PoJo class\"; String ADDRESS = \"100 World Blvd, Planet Earth\"; BlockingQueue<EventEnvelope> bench = new ArrayBlockingQueue<>(1); PostOffice po = new PostOffice(\"unit.test\", \"20001\", \"GET /api/hello/pojo\"); EventEnvelope request = new EventEnvelope().setTo(\"hello.pojo\").setHeader(\"id\", \"1\"); po.asyncRequest(request, 8000).onSuccess(bench::add); EventEnvelope response = bench.poll(10, TimeUnit.SECONDS); assert response != null; assertEquals(HashMap.class, response.getBody().getClass()); SamplePoJo pojo = response.getBody(SamplePoJo.class); assertEquals(ID, pojo.getId()); assertEquals(NAME, pojo.getName()); assertEquals(ADDRESS, pojo.getAddress()); } Note that you can use the built-in serialization API to restore a PoJo like this: SamplePoJo pojo = response.getBody(SamplePoJo.class)","title":"Your second unit test"},{"location":"guides/CHAPTER-5/#convenient-utility-classes","text":"The Utility and MultiLevelMap classes are convenient tools for unit tests. In the above example, we use the Utility class to convert a date object into a UTC timestamp. It is because date object is serialized as a UTC timestamp in an event. The MultiLevelMap supports reading an element using the convenient \"dot and bracket\" format. For example, given a map like this: { \"body\": { \"time\": \"2023-03-27T18:10:34.234Z\", \"hello\": [1, 2, 3], \"complex\": [ {\"key\": \"value1\"}, {\"key\": \"value2\"} ] } } Example Command Result 1 map.getElement(\"body.time\") 2023-03-27T18:10:34.234Z 2 map.getElement(\"body.hello[2]\") 3 3 map.getElement(\"body.complex[1].key\") value2 4 map.getElements(\"body.complex[*].key\") [ value1, value2 ] Example-4 above uses the \"getElements\" in plural form to indicate that it is retrieving a list of elements using a \"wildcard\" index. For simplicity, it does not support more than one wildcard index in the search path.","title":"Convenient utility classes"},{"location":"guides/CHAPTER-5/#event-flow-mocking-framework","text":"We recommend using Event Script to write Composable application for highest level of decoupling. Event Script supports sophisticated event choreography by configuration. In Event Script, you have a event flow configuration and a few Composable functions in an application. Composable functions are self-contained with zero dependencies with other composable functions. You can invoke an event flow from an event flow adapter. The most common flow adapter is the \"HTTP flow adapter\" and it is available as a built-in module in the event-script-engine module in the system. You can associate many REST endpoints to the HTTP flow adapter. Since function routes for each composable function is defined in a event flow configuration and the same function route may be used for more than one task in the flow, the system provides a mock helper class called \"EventScriptMock\" to let your unit tests to override a task's function routes during test. In the following unit test example for a \"pipeline\" test, we created a mock function \"my.mock.function\" to override the \"no.op\" function that is associated with the first task \"echo.one\" in a pipeline. The original \"no.op\" function is an echo function. The mocked function increments a counter in addition to just echoing the input payload. In this fashion, the unit test can count the number of iteration of a pipeline to validate the looping feature of a pipeline. The unit test programmatically registers the mock function and then release it from the event loop when the test finishes. @SuppressWarnings(\"unchecked\") @Test void pipelineForLoopTest() throws InterruptedException { Platform platform = Platform.getInstance(); // The first task of the flow \"for-loop-test\" is \"echo.one\" that is using \"no.op\". // We want to override no.op with my.mock.function to demonstrate mocking a function // for a flow. var ECHO_ONE = \"echo.one\"; var MOCK_FUNCTION = \"my.mock.function\"; var iteration = new AtomicInteger(0); LambdaFunction f = (headers, body, instance) -> { var n = iteration.incrementAndGet(); log.info(\"Iteration-{} {}\", n, body); return body; }; platform.registerPrivate(MOCK_FUNCTION, f, 1); // override the function for the task \"echo.one\" to the mock function var mock = new EventScriptMock(\"for-loop-test\"); var previousRoute = mock.getFunctionRoute(ECHO_ONE); var currentRoute = mock.assignFunctionRoute(ECHO_ONE, MOCK_FUNCTION).getFunctionRoute(ECHO_ONE); assertEquals(\"no.op\", previousRoute); assertEquals(MOCK_FUNCTION, currentRoute); final BlockingQueue<EventEnvelope> bench = new ArrayBlockingQueue<>(1); final long TIMEOUT = 8000; String USER = \"test-user\"; int SEQ = 100; AsyncHttpRequest request = new AsyncHttpRequest(); request.setTargetHost(HOST).setMethod(\"GET\").setHeader(\"accept\", \"application/json\"); request.setUrl(\"/api/for-loop/\"+USER).setQueryParameter(\"seq\", SEQ); EventEmitter po = EventEmitter.getInstance(); EventEnvelope req = new EventEnvelope().setTo(HTTP_CLIENT).setBody(request); po.asyncRequest(req, TIMEOUT).onSuccess(bench::add); EventEnvelope res = bench.poll(TIMEOUT, TimeUnit.MILLISECONDS); assert res != null; assertInstanceOf(Map.class, res.getBody()); Map<String, Object> result = (Map<String, Object>) res.getBody(); assertTrue(result.containsKey(\"data\")); PoJo pojo = SimpleMapper.getInstance().getMapper().readValue(result.get(\"data\"), PoJo.class); assertEquals(SEQ, pojo.sequence); assertEquals(USER, pojo.user); assertEquals(3, result.get(\"n\")); assertEquals(3, iteration.get()); platform.release(MOCK_FUNCTION); } When the event flow finishes, you will see an \"end-of-flow\" report like this. It shows that the function route for the \"echo.one\" task has been changed to \"my.mock.function\". This end-of-flow log is useful during application development and tests so that the developer knows exactly which function has been executed. Flow for-loop-test (0afcf555fc4141f4a16393422e468dc9) completed. Run 11 tasks in 28 ms. [ sequential.one, echo.one(my.mock.function), echo.two(no.op), echo.three(no.op), echo.one(my.mock.function), echo.two(no.op), echo.three(no.op), echo.one(my.mock.function), echo.two(no.op), echo.three(no.op), echo.four(no.op) ]","title":"Event Flow mocking framework"},{"location":"guides/CHAPTER-5/#inspecting-the-state-machine-using-eventscriptmock","text":"The state machine of a \"flow instance\" is not accessible directly by a user task. To inspect the state machine in a unit test, you can use the setMonitorBeforeTask and setMonitorAfterTask methods. The former tells the system to send a copy of the state machine to a composable function after \"input data mapping\" but before entering a task. The latter sends a copy of the state machine to a composable function after a task is completed. The following code segment from a unit test illustrates this feature. The function \"before.task.monitor\" will get a copy of the state machine before \"my.task\" executes. Similarly, the function \"after.task.monitor\" will obtain a copy of the state machine after \"my.task\" finishes execution. var platform = Platform.getInstance(); var mock = new EventScriptMock(\"parent-greetings\"); TypedLambdaFunction<Map<String, Object>, Void> f1 = (headers, input, instance) -> { before.add(input); return null; }; platform.registerPrivate(\"before.task.monitor\", f1, 1); TypedLambdaFunction<Map<String, Object>, Void> f2 = (headers, input, instance) -> { after.add(input); return null; }; platform.registerPrivate(\"after.task.monitor\", f2, 1); mock.setMonitorBeforeTask(\"my.task\", \"before.task.monitor\") .setMonitorAfterTask(\"my.task\", \"after.task.monitor\");","title":"Inspecting the state machine using EventScriptMock"},{"location":"guides/CHAPTER-5/#deployment","text":"The pom.xml is pre-configured to generate an executable JAR. The following is extracted from the pom.xml. The main class is AutoStart that will load the \"main application\" and use it as the entry point to run the application. <plugin> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-maven-plugin</artifactId> <configuration> <mainClass>org.platformlambda.core.system.AutoStart</mainClass> </configuration> <executions> <execution> <id>build-info</id> <goals> <goal>build-info</goal> </goals> </execution> </executions> </plugin> Composable application is designed to be deployable using Kubernetes or serverless. A sample Dockerfile for an executable JAR may look like this: FROM mcr.microsoft.com/openjdk/jdk:21-ubuntu EXPOSE 8083 WORKDIR /app COPY target/rest-spring-3-example-4.2.3.jar . ENTRYPOINT [\"java\",\"-jar\",\"rest-spring-3-example-4.2.3.jar\"]","title":"Deployment"},{"location":"guides/CHAPTER-5/#distributed-tracing","text":"The system has a built-in distributed tracing feature. You can enable tracing for any REST endpoint by adding \"tracing=true\" in the endpoint definition in the \"rest.yaml\" configuration file. You may also upload performance metrics from the distributed tracing data to your favorite telemetry system dashboard. To do that, you can implement a custom metrics function with the route name distributed.trace.forwarder . The input to the function will be a HashMap like this: trace={path=/api/upload/demo, service=hello.upload, success=true, origin=2023032731e2a5eeae8f4da09f3d9ac6b55fb0a4, exec_time=77.462, start=2023-03-27T19:38:30.061Z, from=http.request, id=12345, round_trip=132.296, status=200} The system will detect if distributed.trace.forwarder is available. If yes, it will forward performance metrics from distributed trace to your custom function.","title":"Distributed tracing"},{"location":"guides/CHAPTER-5/#request-response-journaling","text":"Optionally, you may also implement a custom audit function named transaction.journal.recorder to monitor request-response payloads. To enable journaling, please add this to the application.properties file. journal.yaml=classpath:/journal.yaml and add the \"journal.yaml\" configuration file to the project's resources folder with content like this: journal: - \"my.test.function\" - \"another.function\" In the above example, the \"my.test.function\" and \"another.function\" will be monitored and their request-response payloads will be forwarded to your custom audit function. The input to your audit function will be a HashMap containing the performance metrics data and a \"journal\" section with the request and response payloads in clear form. IMPORTANT : journaling may contain sensitive personally identifiable data and secrets. Please check security compliance before storing them into access restricted audit data store.","title":"Request-response journaling"},{"location":"guides/CHAPTER-5/#performance-tuning","text":"The composable framework is designed for high concurrency using virtual threads and new channel I/O (NIO). As a result, it can generate a massive volume of outgoing traffic to your system of records. If not managed properly, massive parallelism can actually degrade overall performance because it can become a form of unintended \"denial of service\" attack. Composable applications are, by definition, cloud native. You can scale your applications horizontally. In addition, composable functions in each application instance can be scaled vertically using Java virtual thread technology. Back-pressure is automated so your application usually does not need to do advanced coding to do flow control. For each composable function, you can define concurrency using the \"instances\" parameter in the \"PreLoad\" class annotation. Note that if the function makes outgoing calls to an external dependency that is slow, it can consume \"worker\" threads very quickly because the \"workers\" are waiting for a response from the external dependency. Therefore, you should increase the concurrency count to adjust for your performance requirement. If you configure parallel processing, especially when using the parallel pipeline method, it can spin up instances of composable functions in an \"uncontrolled\" manner. It would result in making too many parallel requests to an external dependency. For external system that runs using legacy technology, it can easily be overwhelmed, resulting in performance bottleneck. It can block the calling functions from a composable application and thus the outcome can be suboptimal. When configuring parallel calls, please consider end-to-end connectivity and potential bottlenecks. Since event flow instances are already running in parallel, configuring parallel requests within a single event flow would reduce performance. An orderly executed pipeline would be faster than a parallel pipeline that is \"uncontrolled\". On the other hands, fork-n-join uses a predetermined number of parallel tasks and it is easier to control parallelism. Performance tuning is an art than a science. A holistic view of end-to-end performance and careful configuration of parallelism would yield good outcome.","title":"Performance tuning"},{"location":"guides/CHAPTER-5/#performance-metrics","text":"The built-in telemetry system offers basic performance metrics that can be visualized with a telemetry dashboard. For more advanced performance metrics, you may refer to the \"end-of-flow\" performance report. It states the sequence of task execution and their elapsed time that includes execution time of a function, routing overheads and all system overheads. It may look like this when you configure \"log.format=json\" in application.properties. { \"level\": \"INFO\", \"time\": \"2025-08-27 18:39:25.683\", \"source\": \"org.platformlambda.core.services.Telemetry.handleEvent(Telemetry.java:81)\", \"thread\": 336, \"message\": { \"trace\": { \"path\": \"GET /api/profile/100\", \"service\": \"task.executor\", \"success\": true, \"origin\": \"20250828c022812c67294a63871942c568a9e277\", \"exec_time\": 7.0, \"start\": \"2025-08-28T01:39:25.674Z\", \"from\": \"event.script.manager\", \"id\": \"9c0934a98dcf4ab1ae4b5b7b389f6d31\", \"status\": 200 }, \"annotations\": { \"execution\": \"Run 2 tasks in 7 ms\", \"tasks\": [ { \"name\": \"v1.get.profile\", \"spent\": 2.982 }, { \"name\": \"v1.decrypt.fields\", \"spent\": 1.313 } ], \"flow\": \"get-profile\" } } } Chapter-4 Home Chapter-6 Event Script Syntax Table of Contents Spring Boot","title":"Performance metrics"},{"location":"guides/CHAPTER-6/","text":"Spring Boot Integration While the platform-core foundation code includes a lightweight non-blocking HTTP server, you can also turn your application into an executable Spring Boot application. There are two ways to do that: Add dependency for Spring Boot version 3 and implement your Spring Boot main application Add the rest-spring-3 add-on library for a pre-configured Spring Boot experience Add platform-core to an existing Spring Boot application For option 1, the platform-core library can co-exist with Spring Boot. You can write code specific to Spring Boot and the Spring framework ecosystem. Please make sure you add the following startup code to your Spring Boot main application like this: @ComponentScan({\"org.platformlambda\", \"${web.component.scan}\"}) @SpringBootApplication public class MyMainApp extends SpringBootServletInitializer { public static void main(String[] args) { AutoStart.main(args); SpringApplication.run(MyMainApp.class, args); } } We suggest running AutoStart.main before the SpringApplication.run statement. This would allow the platform-core foundation code to load the event-listener functions into memory before Spring Boot starts. Use the rest-spring library in your application The rest-spring-3 subproject is a pre-configured Spring Boot 3 library with WebFlux as the asynchronous HTTP servlet engine. You can add it to your application and turn it into a pre-configured Spring Boot 3 application. It provides consistent behavior for XML and JSON serializaation and exception handling. The RestServer class in the rest-spring-3 library is used to bootstrap a Spring Boot application. However, Spring Boot is a sophisticated ecosystem by itself. If the simple RestServer bootstrap does not fit your use cases, please implement your own Spring Boot initializer. You can add the \"spring.boot.main\" parameter in the application.properties to point to your Spring Boot initializer main class. Note that the default value is \"org.platformlambda.rest.RestServer\" that points to the system provided Spring Boot initializer. spring.boot.main=org.platformlambda.rest.RestServer The Spring Boot initialization main class must have at least the following annotations: @ComponentScan({\"org.platformlambda\", \"${web.component.scan}\"}) @SpringBootApplication The ComponentScan must include the package \"org.platformlambda\" to allow the system to load the pre-configured serializers, actuator endpoints and exception handlers for consistent behavior as the built-in lightweight non-blocking HTTP server. If you want to disable the lightweight HTTP server, you can set rest.automation=false in application.properties. The REST automation engine and the lightweight HTTP server will be turned off. IMPORTANT : When using Event Script, you must keep rest.automation=true because the HTTP flow adapter depends on the REST automation engine for incoming HTTP requests. Note : The platform-core library assumes the application configuration files to be either application.yml or application.properties. If you use custom Spring profile, please keep the application.yml or application.properties for the platform-core. If you use default Spring profile, both platform-core and Spring Boot will use the same configuration files. You can customize your error page using the default errorPage.html by copying it from the platform-core's or rest-spring's resources folder to your source project. The default page is shown below. <!DOCTYPE html> <html lang=\"en\"> <head> <title>HTTP Error</title> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> </head> <body> <div> <h3>HTTP-${status}</h3> <div>${warning}</div><br/> <table> <tbody> <tr><td style=\"font-style: italic; width: 100px\">Type</td><td>error</td></tr> <tr><td style=\"font-style: italic; width: 100px\">Status</td><td>${status}</td></tr> <tr><td style=\"font-style: italic; width: 100px\">Message</td><td>${message}</td></tr> </tbody> </table> </div> </body> </html> This is the HTML error page that the platform-core or rest-spring library uses. You can update it with your corporate style guide. Please keep the parameters (status, message, path, warning) intact. If you want to keep REST automation's lightweight HTTP server to co-exist with Spring Boot's Tomcat or other application server, please add the following to your application.properties file: server.port=8083 rest.server.port=8085 rest.automation=true The platform-core and Spring Boot will use rest.server.port and server.port respectively. Spring Autowiring When using the rest-spring-3 module, bean and value injection may be used: @PreLoad(route = \"v1.demo.function\") public class DemoFunction implements LambdaFunction { @Autowired LegacyBean legacyBean; @Value(\"${injected.value}\") String injectedValue; @Override public Object handleEvent(Map<String, String> headers, Object input, int instance) throws Exception { // ... } } Note : This should be done judiciously - Composable functions should have very few dependencies and each injected dependency is a violation of that principle. Limitations Only applies to LambdaFunction and TypedLambdaFunction . Only applies to instances loaded with @PreLoad - Functions registered programmatically will not undergo injection. Constructor injection is not viable - these instances are constructed before the Spring context is created. The rest-spring-3-example demo application Let's review the rest-spring-3-example demo application in the \"examples/rest-spring-3-example\" project. You can use the rest-spring-3-example as a template to create a Spring Boot application. In addition to the REST automation engine that lets you create REST endpoints by configuration, you can also programmatically create REST endpoints with the following methods: Spring RestControllers with Mono/Flux Servlet 3.1 WebServlets We will examine asynchronous REST endpoint with the AsyncHelloWorld class. @RestController public class AsyncHelloWorld { private static final AtomicInteger seq = new AtomicInteger(0); @GetMapping(value = \"/api/hello/world\", produces={\"application/json\", \"application/xml\"}) public Mono<Map<String, Object>> hello(HttpServletRequest request) { String traceId = Utility.getInstance().getUuid(); PostOffice po = new PostOffice(\"hello.world.endpoint\", traceId, \"GET /api/hello/world\"); Map<String, Object> forward = new HashMap<>(); Enumeration<String> headers = request.getHeaderNames(); while (headers.hasMoreElements()) { String key = headers.nextElement(); forward.put(key, request.getHeader(key)); } // As a demo, just put the incoming HTTP headers as a payload and a parameter showing the sequence counter. // The echo service will return both. int n = seq.incrementAndGet(); EventEnvelope req = new EventEnvelope(); req.setTo(\"hello.world\").setBody(forward).setHeader(\"seq\", n); return Mono.create(callback -> { po.asyncRequest(req, 3000) .onSuccess(event -> { Map<String, Object> result = new HashMap<>(); result.put(\"status\", event.getStatus()); result.put(\"headers\", event.getHeaders()); result.put(\"body\", event.getBody()); result.put(\"execution_time\", event.getExecutionTime()); result.put(\"round_trip\", event.getRoundTrip()); callback.success(result); }) .onFailure(ex -> callback.error(new AppException(408, ex.getMessage()))); }); } } In this hello world REST endpoint, Spring Reactor runs the \"hello\" method asynchronously without waiting for a response. The example code copies the HTTP requests and sends it as the request payload to the \"hello.world\" function. The function is defined in the MainApp like this: Platform platform = Platform.getInstance(); LambdaFunction echo = (headers, input, instance) -> { Map<String, Object> result = new HashMap<>(); result.put(\"headers\", headers); result.put(\"body\", input); result.put(\"instance\", instance); result.put(\"origin\", platform.getOrigin()); return result; }; platform.register(\"hello.world\", echo, 20); When \"hello.world\" responds, its result set will be returned to the onSuccess method as a \"future response\". The \"onSuccess\" method then sends the response to the browser. The AsyncHelloConcurrent is the same as the AsyncHelloWorld except that it performs a \"fork-n-join\" operation to multiple instances of the \"hello.world\" function. Unlike \"rest.yaml\" that defines tracing by configuration, you must turn on tracing programmatically in a Spring RestController endpoint. To enable tracing, the function sets the trace ID and path in the PostOffice constructor. When you try the endpoint at http://127.0.0.1:8083/api/hello/world, it will echo your HTTP request headers. In the command terminal, you will see tracing information in the console log like this: Telemetry:67 - trace={path=GET /api/hello/world, service=hello.world, success=true, origin=20230403364f70ebeb54477f91986289dfcd7b75, exec_time=0.249, start=2023-04-03T04:42:43.445Z, from=hello.world.endpoint, id=e12e871096ba4938b871ee72ef09aa0a, round_trip=20.018, status=200} Lightweight non-blocking websocket server If you want to turn on a non-blocking websocket server, you can add the following configuration to application.properties. server.port=8083 websocket.server.port=8085 The above assumes Spring Boot runs on port 8083 and the websocket server runs on port 8085. Note : The \"websocket.server.port\" parameter is an alias of \"rest.server.port\" You can create a websocket service with a Java class like this: @WebSocketService(\"hello\") public class WsEchoDemo implements LambdaFunction { @Override public Object handleEvent(Map<String, String> headers, Object body, int instance) { // handle the incoming websocket events (type = open, close, bytes or string) } } The above creates a websocket service at the URL \"/ws/hello\" server endpoint. Please review the example code in the WsEchoDemo class in the rest-spring-3-example project for details. If you want to use Spring Boot's Tomcat websocket server, you can disable the non-blocking websocket server feature by removing the websocket.server.port configuration and any websocket service classes with the WebSocketService annotation. To try out the demo websocket server, visit http://127.0.0.1:8083 and select \"Websocket demo\". Chapter-5 Home Chapter-7 Build, Test and Deploy Table of Contents Event over HTTP","title":"Chapter-6"},{"location":"guides/CHAPTER-6/#spring-boot-integration","text":"While the platform-core foundation code includes a lightweight non-blocking HTTP server, you can also turn your application into an executable Spring Boot application. There are two ways to do that: Add dependency for Spring Boot version 3 and implement your Spring Boot main application Add the rest-spring-3 add-on library for a pre-configured Spring Boot experience","title":"Spring Boot Integration"},{"location":"guides/CHAPTER-6/#add-platform-core-to-an-existing-spring-boot-application","text":"For option 1, the platform-core library can co-exist with Spring Boot. You can write code specific to Spring Boot and the Spring framework ecosystem. Please make sure you add the following startup code to your Spring Boot main application like this: @ComponentScan({\"org.platformlambda\", \"${web.component.scan}\"}) @SpringBootApplication public class MyMainApp extends SpringBootServletInitializer { public static void main(String[] args) { AutoStart.main(args); SpringApplication.run(MyMainApp.class, args); } } We suggest running AutoStart.main before the SpringApplication.run statement. This would allow the platform-core foundation code to load the event-listener functions into memory before Spring Boot starts.","title":"Add platform-core to an existing Spring Boot application"},{"location":"guides/CHAPTER-6/#use-the-rest-spring-library-in-your-application","text":"The rest-spring-3 subproject is a pre-configured Spring Boot 3 library with WebFlux as the asynchronous HTTP servlet engine. You can add it to your application and turn it into a pre-configured Spring Boot 3 application. It provides consistent behavior for XML and JSON serializaation and exception handling. The RestServer class in the rest-spring-3 library is used to bootstrap a Spring Boot application. However, Spring Boot is a sophisticated ecosystem by itself. If the simple RestServer bootstrap does not fit your use cases, please implement your own Spring Boot initializer. You can add the \"spring.boot.main\" parameter in the application.properties to point to your Spring Boot initializer main class. Note that the default value is \"org.platformlambda.rest.RestServer\" that points to the system provided Spring Boot initializer. spring.boot.main=org.platformlambda.rest.RestServer The Spring Boot initialization main class must have at least the following annotations: @ComponentScan({\"org.platformlambda\", \"${web.component.scan}\"}) @SpringBootApplication The ComponentScan must include the package \"org.platformlambda\" to allow the system to load the pre-configured serializers, actuator endpoints and exception handlers for consistent behavior as the built-in lightweight non-blocking HTTP server. If you want to disable the lightweight HTTP server, you can set rest.automation=false in application.properties. The REST automation engine and the lightweight HTTP server will be turned off. IMPORTANT : When using Event Script, you must keep rest.automation=true because the HTTP flow adapter depends on the REST automation engine for incoming HTTP requests. Note : The platform-core library assumes the application configuration files to be either application.yml or application.properties. If you use custom Spring profile, please keep the application.yml or application.properties for the platform-core. If you use default Spring profile, both platform-core and Spring Boot will use the same configuration files. You can customize your error page using the default errorPage.html by copying it from the platform-core's or rest-spring's resources folder to your source project. The default page is shown below. <!DOCTYPE html> <html lang=\"en\"> <head> <title>HTTP Error</title> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> </head> <body> <div> <h3>HTTP-${status}</h3> <div>${warning}</div><br/> <table> <tbody> <tr><td style=\"font-style: italic; width: 100px\">Type</td><td>error</td></tr> <tr><td style=\"font-style: italic; width: 100px\">Status</td><td>${status}</td></tr> <tr><td style=\"font-style: italic; width: 100px\">Message</td><td>${message}</td></tr> </tbody> </table> </div> </body> </html> This is the HTML error page that the platform-core or rest-spring library uses. You can update it with your corporate style guide. Please keep the parameters (status, message, path, warning) intact. If you want to keep REST automation's lightweight HTTP server to co-exist with Spring Boot's Tomcat or other application server, please add the following to your application.properties file: server.port=8083 rest.server.port=8085 rest.automation=true The platform-core and Spring Boot will use rest.server.port and server.port respectively.","title":"Use the rest-spring library in your application"},{"location":"guides/CHAPTER-6/#spring-autowiring","text":"When using the rest-spring-3 module, bean and value injection may be used: @PreLoad(route = \"v1.demo.function\") public class DemoFunction implements LambdaFunction { @Autowired LegacyBean legacyBean; @Value(\"${injected.value}\") String injectedValue; @Override public Object handleEvent(Map<String, String> headers, Object input, int instance) throws Exception { // ... } } Note : This should be done judiciously - Composable functions should have very few dependencies and each injected dependency is a violation of that principle.","title":"Spring Autowiring"},{"location":"guides/CHAPTER-6/#limitations","text":"Only applies to LambdaFunction and TypedLambdaFunction . Only applies to instances loaded with @PreLoad - Functions registered programmatically will not undergo injection. Constructor injection is not viable - these instances are constructed before the Spring context is created.","title":"Limitations"},{"location":"guides/CHAPTER-6/#the-rest-spring-3-example-demo-application","text":"Let's review the rest-spring-3-example demo application in the \"examples/rest-spring-3-example\" project. You can use the rest-spring-3-example as a template to create a Spring Boot application. In addition to the REST automation engine that lets you create REST endpoints by configuration, you can also programmatically create REST endpoints with the following methods: Spring RestControllers with Mono/Flux Servlet 3.1 WebServlets We will examine asynchronous REST endpoint with the AsyncHelloWorld class. @RestController public class AsyncHelloWorld { private static final AtomicInteger seq = new AtomicInteger(0); @GetMapping(value = \"/api/hello/world\", produces={\"application/json\", \"application/xml\"}) public Mono<Map<String, Object>> hello(HttpServletRequest request) { String traceId = Utility.getInstance().getUuid(); PostOffice po = new PostOffice(\"hello.world.endpoint\", traceId, \"GET /api/hello/world\"); Map<String, Object> forward = new HashMap<>(); Enumeration<String> headers = request.getHeaderNames(); while (headers.hasMoreElements()) { String key = headers.nextElement(); forward.put(key, request.getHeader(key)); } // As a demo, just put the incoming HTTP headers as a payload and a parameter showing the sequence counter. // The echo service will return both. int n = seq.incrementAndGet(); EventEnvelope req = new EventEnvelope(); req.setTo(\"hello.world\").setBody(forward).setHeader(\"seq\", n); return Mono.create(callback -> { po.asyncRequest(req, 3000) .onSuccess(event -> { Map<String, Object> result = new HashMap<>(); result.put(\"status\", event.getStatus()); result.put(\"headers\", event.getHeaders()); result.put(\"body\", event.getBody()); result.put(\"execution_time\", event.getExecutionTime()); result.put(\"round_trip\", event.getRoundTrip()); callback.success(result); }) .onFailure(ex -> callback.error(new AppException(408, ex.getMessage()))); }); } } In this hello world REST endpoint, Spring Reactor runs the \"hello\" method asynchronously without waiting for a response. The example code copies the HTTP requests and sends it as the request payload to the \"hello.world\" function. The function is defined in the MainApp like this: Platform platform = Platform.getInstance(); LambdaFunction echo = (headers, input, instance) -> { Map<String, Object> result = new HashMap<>(); result.put(\"headers\", headers); result.put(\"body\", input); result.put(\"instance\", instance); result.put(\"origin\", platform.getOrigin()); return result; }; platform.register(\"hello.world\", echo, 20); When \"hello.world\" responds, its result set will be returned to the onSuccess method as a \"future response\". The \"onSuccess\" method then sends the response to the browser. The AsyncHelloConcurrent is the same as the AsyncHelloWorld except that it performs a \"fork-n-join\" operation to multiple instances of the \"hello.world\" function. Unlike \"rest.yaml\" that defines tracing by configuration, you must turn on tracing programmatically in a Spring RestController endpoint. To enable tracing, the function sets the trace ID and path in the PostOffice constructor. When you try the endpoint at http://127.0.0.1:8083/api/hello/world, it will echo your HTTP request headers. In the command terminal, you will see tracing information in the console log like this: Telemetry:67 - trace={path=GET /api/hello/world, service=hello.world, success=true, origin=20230403364f70ebeb54477f91986289dfcd7b75, exec_time=0.249, start=2023-04-03T04:42:43.445Z, from=hello.world.endpoint, id=e12e871096ba4938b871ee72ef09aa0a, round_trip=20.018, status=200}","title":"The rest-spring-3-example demo application"},{"location":"guides/CHAPTER-6/#lightweight-non-blocking-websocket-server","text":"If you want to turn on a non-blocking websocket server, you can add the following configuration to application.properties. server.port=8083 websocket.server.port=8085 The above assumes Spring Boot runs on port 8083 and the websocket server runs on port 8085. Note : The \"websocket.server.port\" parameter is an alias of \"rest.server.port\" You can create a websocket service with a Java class like this: @WebSocketService(\"hello\") public class WsEchoDemo implements LambdaFunction { @Override public Object handleEvent(Map<String, String> headers, Object body, int instance) { // handle the incoming websocket events (type = open, close, bytes or string) } } The above creates a websocket service at the URL \"/ws/hello\" server endpoint. Please review the example code in the WsEchoDemo class in the rest-spring-3-example project for details. If you want to use Spring Boot's Tomcat websocket server, you can disable the non-blocking websocket server feature by removing the websocket.server.port configuration and any websocket service classes with the WebSocketService annotation. To try out the demo websocket server, visit http://127.0.0.1:8083 and select \"Websocket demo\". Chapter-5 Home Chapter-7 Build, Test and Deploy Table of Contents Event over HTTP","title":"Lightweight non-blocking websocket server"},{"location":"guides/CHAPTER-7/","text":"Event over HTTP The in-memory event system allows functions to communicate with each other in the same application memory space. In composable architecture, applications are modular components in a network. Some transactions may require the services of more than one application. \"Event over HTTP\" extends the event system beyond a single application. The Event API service ( event.api.service ) is a built-in function in the system. The Event API endpoint To enable \"Event over HTTP\", you must first turn on the REST automation engine with the following parameters in the application.properties file: rest.server.port=8085 rest.automation=true and then check if the following entry is configured in the \"rest.yaml\" endpoint definition file. If not, update \"rest.yaml\" accordingly. The \"timeout\" value is set to 60 seconds to fit common use cases. - service: [ \"event.api.service\" ] methods: [ 'POST' ] url: \"/api/event\" timeout: 60s tracing: true This will expose the Event API endpoint at port 8085 and URL \"/api/event\". In kubernetes, The Event API endpoint of each application is reachable through internal DNS and there is no need to create \"ingress\" for this purpose. Test drive Event API You may now test drive the Event API service. First, build and run the lambda-example application in port 8085. cd examples/lambda-example java -jar target/lambda-example-3.1.2.jar Second, build and run the rest-spring-example application. cd examples/rest-spring-example-3 java -jar target/rest-spring-3-example-3.1.2.jar The rest-spring-3-example application will run as a Spring Boot application in port 8083 and 8086. These two applications will start independently. You may point your browser to http://127.0.0.1:8083/api/pojo/http/1 to invoke the HelloPojoEventOverHttp endpoint service that will in turn makes an Event API call to the lambda-example's \"hello.pojo\" service. You will see the following response in the browser. This means the rest-spring-example application has successfully made an event API call to the lambda-example application using the Event API endpoint. { \"id\": 1, \"name\": \"Simple PoJo class\", \"address\": \"100 World Blvd, Planet Earth\", \"date\": \"2023-03-27T23:17:19.257Z\", \"instance\": 6, \"seq\": 66, \"origin\": \"2023032791b6938a47614cf48779b1cf02fc89c4\" } To examine how the application makes the Event API call, please refer to the HelloPojoEventOverHttp class in the rest-spring-example. The class is extracted below: @RestController public class HelloPoJoEventOverHttp { @GetMapping(\"/api/pojo/http/{id}\") public Mono<SamplePoJo> getPoJo(@PathVariable(\"id\") Integer id) { AppConfigReader config = AppConfigReader.getInstance(); String remotePort = config.getProperty(\"lambda.example.port\", \"8085\"); String remoteEndpoint = \"http://127.0.0.1:\"+remotePort+\"/api/event\"; String traceId = Utility.getInstance().getUuid(); PostOffice po = new PostOffice(\"hello.pojo.endpoint\", traceId, \"GET /api/pojo/http\"); EventEnvelope req = new EventEnvelope().setTo(\"hello.pojo\").setHeader(\"id\", id); return Mono.create(callback -> { try { EventEnvelope response = po.request(req, 3000, Collections.emptyMap(), remoteEndpoint, true).get(); if (response.getBody() instanceof SamplePoJo result) { callback.success(result); } else { callback.error(new AppException(response.getStatus(), String.valueOf(response.getError()))); } } catch (ExecutionException | InterruptedException e) { callback.error(e); } }); } } The method signatures of the Event API is shown as follows: Asynchronous API (Java) // io.vertx.core.Future public Future<EventEnvelope> asyncRequest(final EventEnvelope event, long timeout, Map<String, String> headers, String eventEndpoint, boolean rpc); Sequential non-blocking API (virtual thread function) // java.util.concurrent.Future public Future<EventEnvelope> request(final EventEnvelope event, long timeout, Map<String, String> headers, String eventEndpoint, boolean rpc); Optionally, you may add security headers in the \"headers\" argument. e.g. the \"Authorization\" header. The eventEndpoint is a fully qualified URL. e.g. http://peer/api/event The \"rpc\" boolean value is set to true so that the response from the service of the peer application instance will be delivered. For drop-n-forget use case, you can set the \"rpc\" value to false. It will immediately return an HTTP-202 response. Event-over-HTTP using configuration While you can call the \"Event-over-HTTP\" APIs programmatically, it would be more convenient to automate it with a configuration. This service abstraction means that user applications do not need to know where the target services are. You can enable Event-over-HTTP configuration by adding this parameter in application.properties: # # Optional event-over-http target maps # yaml.event.over.http=classpath:/event-over-http.yaml and then create the configuration file \"event-over-http.yaml\" like this: event: http: - route: 'hello.pojo2' target: 'http://127.0.0.1:${lambda.example.port}/api/event' - route: 'event.http.test' target: 'http://127.0.0.1:${server.port}/api/event' # optional security headers headers: authorization: 'demo' - route: 'event.save.get' target: 'http://127.0.0.1:${server.port}/api/event' headers: authorization: 'demo' In the above example, there are three routes (hello.pojo2, event.http.test and event.save.get) with target URLs. If additional authentication is required for the peer's \"/api/event\" endpoint, you may add a set of security headers in each route. When you send asynchronous event or make a RPC call to \"event.save.get\" service, it will be forwarded to the peer's \"event-over-HTTP\" endpoint ( /api/event ) accordingly. If the route is a task in an event flow, the event manager will make the \"Event over HTTP\" to the target service. You may also add environment variable or base configuration references to the application.yaml file, such as \"server.port\" in this example. An example in the rest-spring-3-example subproject is shown below to illustrate this service abstraction. In this example, the remote Event-over-HTTP endpoint address is resolved from the event-over-http.yaml configuration. @RestController public class HelloPoJoEventOverHttpByConfig { @GetMapping(\"/api/pojo2/http/{id}\") public Mono<SamplePoJo> getPoJo(@PathVariable(\"id\") Integer id) { String traceId = Utility.getInstance().getUuid(); PostOffice po = new PostOffice(\"hello.pojo.endpoint\", traceId, \"GET /api/pojo2/http\"); /* * \"hello.pojo2\" resides in the lambda-example and is reachable by \"Event-over-HTTP\". * In HelloPojoEventOverHttp.java, it demonstrates the use of Event-over-HTTP API. * In this example, it illustrates the use of the \"Event-over-HTTP by configuration\" feature. * Please see application.properties and event-over-http.yaml files for more details. */ EventEnvelope req = new EventEnvelope().setTo(\"hello.pojo2\").setHeader(\"id\", id); return Mono.create(callback -> { try { EventEnvelope response = po.request(req, 3000, false).get(); if (response.getBody() instanceof SamplePoJo result) { callback.success(result); } else { callback.error(new AppException(response.getStatus(), String.valueOf(response.getError()))); } } catch (ExecutionException | InterruptedException e) { callback.error(e); } }); } } Note : The target function must declare itself as PUBLIC in the preload annotation. Otherwise, you will get a HTTP-403 exception. Advantages The Event API exposes all public functions of an application instance to the network using a single REST endpoint. The advantages of Event API includes: Convenient - you do not need to write or configure individual endpoint for each public service Efficient - events are transported in binary format from one application to another Secure - you can protect the Event API endpoint with an authentication service The following configuration adds authentication service to the Event API endpoint: - service: [ \"event.api.service\" ] methods: [ 'POST' ] url: \"/api/event\" timeout: 60s authentication: \"v1.api.auth\" tracing: true This enforces every incoming request to the Event API endpoint to be authenticated by the \"v1.api.auth\" service before passing to the Event API service. You can plug in your own authentication service such as OAuth 2.0 \"bearer token\" validation. Please refer to Chapter-3 - REST automation for details. Chapter-6 Home Chapter-8 Spring Boot Table of Contents Minimalist Service Mesh","title":"Chapter-7"},{"location":"guides/CHAPTER-7/#event-over-http","text":"The in-memory event system allows functions to communicate with each other in the same application memory space. In composable architecture, applications are modular components in a network. Some transactions may require the services of more than one application. \"Event over HTTP\" extends the event system beyond a single application. The Event API service ( event.api.service ) is a built-in function in the system.","title":"Event over HTTP"},{"location":"guides/CHAPTER-7/#the-event-api-endpoint","text":"To enable \"Event over HTTP\", you must first turn on the REST automation engine with the following parameters in the application.properties file: rest.server.port=8085 rest.automation=true and then check if the following entry is configured in the \"rest.yaml\" endpoint definition file. If not, update \"rest.yaml\" accordingly. The \"timeout\" value is set to 60 seconds to fit common use cases. - service: [ \"event.api.service\" ] methods: [ 'POST' ] url: \"/api/event\" timeout: 60s tracing: true This will expose the Event API endpoint at port 8085 and URL \"/api/event\". In kubernetes, The Event API endpoint of each application is reachable through internal DNS and there is no need to create \"ingress\" for this purpose.","title":"The Event API endpoint"},{"location":"guides/CHAPTER-7/#test-drive-event-api","text":"You may now test drive the Event API service. First, build and run the lambda-example application in port 8085. cd examples/lambda-example java -jar target/lambda-example-3.1.2.jar Second, build and run the rest-spring-example application. cd examples/rest-spring-example-3 java -jar target/rest-spring-3-example-3.1.2.jar The rest-spring-3-example application will run as a Spring Boot application in port 8083 and 8086. These two applications will start independently. You may point your browser to http://127.0.0.1:8083/api/pojo/http/1 to invoke the HelloPojoEventOverHttp endpoint service that will in turn makes an Event API call to the lambda-example's \"hello.pojo\" service. You will see the following response in the browser. This means the rest-spring-example application has successfully made an event API call to the lambda-example application using the Event API endpoint. { \"id\": 1, \"name\": \"Simple PoJo class\", \"address\": \"100 World Blvd, Planet Earth\", \"date\": \"2023-03-27T23:17:19.257Z\", \"instance\": 6, \"seq\": 66, \"origin\": \"2023032791b6938a47614cf48779b1cf02fc89c4\" } To examine how the application makes the Event API call, please refer to the HelloPojoEventOverHttp class in the rest-spring-example. The class is extracted below: @RestController public class HelloPoJoEventOverHttp { @GetMapping(\"/api/pojo/http/{id}\") public Mono<SamplePoJo> getPoJo(@PathVariable(\"id\") Integer id) { AppConfigReader config = AppConfigReader.getInstance(); String remotePort = config.getProperty(\"lambda.example.port\", \"8085\"); String remoteEndpoint = \"http://127.0.0.1:\"+remotePort+\"/api/event\"; String traceId = Utility.getInstance().getUuid(); PostOffice po = new PostOffice(\"hello.pojo.endpoint\", traceId, \"GET /api/pojo/http\"); EventEnvelope req = new EventEnvelope().setTo(\"hello.pojo\").setHeader(\"id\", id); return Mono.create(callback -> { try { EventEnvelope response = po.request(req, 3000, Collections.emptyMap(), remoteEndpoint, true).get(); if (response.getBody() instanceof SamplePoJo result) { callback.success(result); } else { callback.error(new AppException(response.getStatus(), String.valueOf(response.getError()))); } } catch (ExecutionException | InterruptedException e) { callback.error(e); } }); } } The method signatures of the Event API is shown as follows:","title":"Test drive Event API"},{"location":"guides/CHAPTER-7/#asynchronous-api-java","text":"// io.vertx.core.Future public Future<EventEnvelope> asyncRequest(final EventEnvelope event, long timeout, Map<String, String> headers, String eventEndpoint, boolean rpc);","title":"Asynchronous API (Java)"},{"location":"guides/CHAPTER-7/#sequential-non-blocking-api-virtual-thread-function","text":"// java.util.concurrent.Future public Future<EventEnvelope> request(final EventEnvelope event, long timeout, Map<String, String> headers, String eventEndpoint, boolean rpc); Optionally, you may add security headers in the \"headers\" argument. e.g. the \"Authorization\" header. The eventEndpoint is a fully qualified URL. e.g. http://peer/api/event The \"rpc\" boolean value is set to true so that the response from the service of the peer application instance will be delivered. For drop-n-forget use case, you can set the \"rpc\" value to false. It will immediately return an HTTP-202 response.","title":"Sequential non-blocking API (virtual thread function)"},{"location":"guides/CHAPTER-7/#event-over-http-using-configuration","text":"While you can call the \"Event-over-HTTP\" APIs programmatically, it would be more convenient to automate it with a configuration. This service abstraction means that user applications do not need to know where the target services are. You can enable Event-over-HTTP configuration by adding this parameter in application.properties: # # Optional event-over-http target maps # yaml.event.over.http=classpath:/event-over-http.yaml and then create the configuration file \"event-over-http.yaml\" like this: event: http: - route: 'hello.pojo2' target: 'http://127.0.0.1:${lambda.example.port}/api/event' - route: 'event.http.test' target: 'http://127.0.0.1:${server.port}/api/event' # optional security headers headers: authorization: 'demo' - route: 'event.save.get' target: 'http://127.0.0.1:${server.port}/api/event' headers: authorization: 'demo' In the above example, there are three routes (hello.pojo2, event.http.test and event.save.get) with target URLs. If additional authentication is required for the peer's \"/api/event\" endpoint, you may add a set of security headers in each route. When you send asynchronous event or make a RPC call to \"event.save.get\" service, it will be forwarded to the peer's \"event-over-HTTP\" endpoint ( /api/event ) accordingly. If the route is a task in an event flow, the event manager will make the \"Event over HTTP\" to the target service. You may also add environment variable or base configuration references to the application.yaml file, such as \"server.port\" in this example. An example in the rest-spring-3-example subproject is shown below to illustrate this service abstraction. In this example, the remote Event-over-HTTP endpoint address is resolved from the event-over-http.yaml configuration. @RestController public class HelloPoJoEventOverHttpByConfig { @GetMapping(\"/api/pojo2/http/{id}\") public Mono<SamplePoJo> getPoJo(@PathVariable(\"id\") Integer id) { String traceId = Utility.getInstance().getUuid(); PostOffice po = new PostOffice(\"hello.pojo.endpoint\", traceId, \"GET /api/pojo2/http\"); /* * \"hello.pojo2\" resides in the lambda-example and is reachable by \"Event-over-HTTP\". * In HelloPojoEventOverHttp.java, it demonstrates the use of Event-over-HTTP API. * In this example, it illustrates the use of the \"Event-over-HTTP by configuration\" feature. * Please see application.properties and event-over-http.yaml files for more details. */ EventEnvelope req = new EventEnvelope().setTo(\"hello.pojo2\").setHeader(\"id\", id); return Mono.create(callback -> { try { EventEnvelope response = po.request(req, 3000, false).get(); if (response.getBody() instanceof SamplePoJo result) { callback.success(result); } else { callback.error(new AppException(response.getStatus(), String.valueOf(response.getError()))); } } catch (ExecutionException | InterruptedException e) { callback.error(e); } }); } } Note : The target function must declare itself as PUBLIC in the preload annotation. Otherwise, you will get a HTTP-403 exception.","title":"Event-over-HTTP using configuration"},{"location":"guides/CHAPTER-7/#advantages","text":"The Event API exposes all public functions of an application instance to the network using a single REST endpoint. The advantages of Event API includes: Convenient - you do not need to write or configure individual endpoint for each public service Efficient - events are transported in binary format from one application to another Secure - you can protect the Event API endpoint with an authentication service The following configuration adds authentication service to the Event API endpoint: - service: [ \"event.api.service\" ] methods: [ 'POST' ] url: \"/api/event\" timeout: 60s authentication: \"v1.api.auth\" tracing: true This enforces every incoming request to the Event API endpoint to be authenticated by the \"v1.api.auth\" service before passing to the Event API service. You can plug in your own authentication service such as OAuth 2.0 \"bearer token\" validation. Please refer to Chapter-3 - REST automation for details. Chapter-6 Home Chapter-8 Spring Boot Table of Contents Minimalist Service Mesh","title":"Advantages"},{"location":"guides/CHAPTER-8/","text":"Minimalist Service Mesh Service mesh is a dedicated infrastructure layer to facilitate inter-container communication using \"sidecar\" and \"control plane\". Service mesh systems require additional administrative containers (PODs) for \"control plane\" and \"service discovery.\" The additional infrastructure requirements vary among products. Using kafka as a minimalist service mesh We will discuss using Kafka as a minimalist service mesh. Important : Always design your application system in an event-driven manner to decouple application services from each other. You should avoid using service mesh to tightly couple application services together because it would lead to the creation of a \"Distributed Monolith\". Minimalist service mesh is designed as a service discovery mechanism so that you can detect the presence of other application instances. You can use it for operation control mechanism and \"leader selection\" in writing resilient distributed application. Typically, a service mesh system uses a \"side-car\" to sit next to the application container in the same POD to provide service discovery and network proxy services. Instead of using a side-car proxy, the system maintains a distributed routing table in each application instance. When a function requests the service of another function which is not in the same memory space, the \"cloud.connector\" module will bridge the event to the peer application through a network event system like Kafka. As shown in the following table, if \"service.1\" and \"service.2\" are in the same memory space of an application, they will communicate using the in-memory event bus. If they are in different applications and the applications are configured with Kafka, the two functions will communicate via the \"cloud.connector\" service. In-memory event bus Network event stream \"service.1\" -> \"service.2\" \"service.1\" -> \"cloud.connector\" -> \"service.2\" The system supports Kafka out of the box. For example, to select kafka, you can configure application.properties like this: cloud.connector=kafka The \"cloud.connector\" parameter can be set to \"none\" or \"kafka\". The default parameter of \"cloud.connector\" is \"none\". This means the application is not using any network event system \"connector\", thus running independently. Let's set up a minimalist service mesh with Kafka to see how it works. Set up a standalone Kafka server for development You need a Kafka cluster as the network event stream system. For development and testing, you can build and run a standalone Kafka server like this. Note that the mvn clean package command is optional because the executable JAR should be available after the mvn clean install command in Chapter-1 . cd connectors/adapters/kafka/kafka-standalone mvn clean package java -jar target/kafka-standalone-3.1.2.jar The standalone Kafka server will start at port 9092. You may adjust the \"server.properties\" in the standalone-kafka project when necessary. When the kafka server is started, it will create a temporary directory \"/tmp/kafka-logs\". Note : The kafka server is designed for development purpose only. The kafka message log store will be cleared when the server is restarted. Prepare the kafka-presence application The \"kafka-presence\" is a \"presence monitor\" application. It is a minimalist \"control plane\" in service mesh terminology. What is a presence monitor? A presence monitor is the control plane that assigns unique \"topic\" for each user application instance. It monitors the \"presence\" of each application. If an application fails or stops, the presence monitor will advertise the event to the rest of the system so that each application container will update its corresponding distributed routing table, thus bypassing the failed application and its services. If an application has more than one container instance deployed, they will work together to share load evenly. You will start the presence monitor like this: cd connectors/adapters/kafka/kafka-presence java -jar target/kafka-presence-3.1.2.jar By default, the kafka-connector will run at port 8080. Partial start-up log is shown below: AppStarter:344 - Modules loaded in 2,370 ms AppStarter:334 - Websocket server running on port-8080 ServiceLifeCycle:73 - service.monitor, partition 0 ready HouseKeeper:72 - Registered monitor (me) 2023032896b12f9de149459f9c8b71ad8b6b49fa The presence monitor will use the topic \"service.monitor\" to connect to the Kafka server and register itself as a presence monitor. Presence monitor is resilient. You can run more than one instance to back up each other. If you are not using Docker or Kubernetes, you need to change the \"server.port\" parameter of the second instance to 8081 so that the two application instances can run in the same laptop. Launch the rest-spring-3-example and lambda-example with kafka Let's run the rest-spring-3-example and lambda-example applications with Kafka connector turned on. For demo purpose, the rest-spring-3-example and lambda-example are pre-configured with \"kafka-connector\". If you do not need these libraries, please remove them from the pom.xml built script. Since kafka-connector is pre-configured, we can start the two demo applications like this: cd examples/rest-spring-3-example java -Dcloud.connector=kafka -Dmandatory.health.dependencies=cloud.connector.health -jar target/rest-spring-3-example-3.1.2.jar cd examples/lambda-example java -Dcloud.connector=kafka -Dmandatory.health.dependencies=cloud.connector.health -jar target/lambda-example-3.1.2.jar The above command uses the \"-D\" parameters to configure the \"cloud.connector\" and \"mandatory.health.dependencies\". The parameter mandatory.health.dependencies=cloud.connector.health tells the system to turn on the health check endpoint for the application. For the rest-spring-3-example, the start-up log may look like this: AppStarter:344 - Modules loaded in 2,825 ms PresenceConnector:155 - Connected pc.abb4a4de.in, 127.0.0.1:8080, /ws/presence/202303282583899cf43a49b98f0522492b9ca178 EventConsumer:160 - Subscribed multiplex.0001.0 ServiceLifeCycle:73 - multiplex.0001, partition 0 ready This means that the rest-spring-3-example has successfully connected to the presence monitor at port 8080. It has subscribed to the topic \"multiplex.0001\" partition 0. For the lambda-example, the log may look like this: AppStarter:344 - Modules loaded in 2,742 m PresenceConnector:155 - Connected pc.991a2be0.in, 127.0.0.1:8080, /ws/presence/2023032808d82ebe2c0d4e5aa9ca96b3813bdd25 EventConsumer:160 - Subscribed multiplex.0001.1 ServiceLifeCycle:73 - multiplex.0001, partition 1 ready ServiceRegistry:242 - Peer 202303282583899cf43a49b98f0522492b9ca178 joins (rest-spring-3-example 4.1.8) ServiceRegistry:383 - hello.world (rest-spring-3-example, WEB.202303282583899cf43a49b98f0522492b9ca178) registered You notice that the lambda-example has discovered the rest-spring-3-example through Kafka and added the \"hello.world\" to the distributed routing table. At this point, the rest-spring-3-example will find the lambda-example application as well: ServiceRegistry:242 - Peer 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25 joins (lambda-example 4.1.8) ServiceRegistry:383 - hello.world (lambda-example, APP.2023032808d82ebe2c0d4e5aa9ca96b3813bdd25) registered ServiceRegistry:383 - hello.pojo (lambda-example, APP.2023032808d82ebe2c0d4e5aa9ca96b3813bdd25) registered This is real-time service discovery coordinated by the \"kafka-presence\" monitor application. Now you have created a minimalist event-driven service mesh. Send an event request from rest-spring-3-example to lambda-example In Chapter-7 , you have sent a request from the rest-spring-3-example to the lambda-example using \"Event over HTTP\" without a service mesh. In this section, you can make the same request using service mesh. Please point your browser to http://127.0.0.1:8083/api/pojo/mesh/1 You will see the following response in your browser. { \"id\": 1, \"name\": \"Simple PoJo class\", \"address\": \"100 World Blvd, Planet Earth\", \"date\": \"2023-03-28T17:53:41.696Z\", \"instance\": 1, \"seq\": 1, \"origin\": \"2023032808d82ebe2c0d4e5aa9ca96b3813bdd25\" } Presence monitor info endpoint You can check the service mesh status from the presence monitor's \"/info\" endpoint. You can visit http://127.0.0.1:8080/info and it will show something like this: { \"app\": { \"name\": \"kafka-presence\", \"description\": \"Presence Monitor\", \"version\": \"4.1.8\" }, \"personality\": \"RESOURCES\", \"more\": { \"total\": { \"topics\": 2, \"virtual_topics\": 2, \"connections\": 2 }, \"topics\": [ \"multiplex.0001 (32)\", \"service.monitor (11)\" ], \"virtual_topics\": [ \"multiplex.0001-000 -> 202303282583899cf43a49b98f0522492b9ca178, rest-spring-3-example v4.1.8\", \"multiplex.0001-001 -> 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25, lambda-example v4.1.8\" ], \"connections\": [ { \"elapsed\": \"25 minutes 12 seconds\", \"created\": \"2023-03-28T17:43:13Z\", \"origin\": \"2023032808d82ebe2c0d4e5aa9ca96b3813bdd25\", \"name\": \"lambda-example\", \"topic\": \"multiplex.0001-001\", \"monitor\": \"2023032896b12f9de149459f9c8b71ad8b6b49fa\", \"type\": \"APP\", \"updated\": \"2023-03-28T18:08:25Z\", \"version\": \"4.1.8\", \"seq\": 65, \"group\": 1 }, { \"elapsed\": \"29 minutes 42 seconds\", \"created\": \"2023-03-28T17:38:47Z\", \"origin\": \"202303282583899cf43a49b98f0522492b9ca178\", \"name\": \"rest-spring-3-example\", \"topic\": \"multiplex.0001-000\", \"monitor\": \"2023032896b12f9de149459f9c8b71ad8b6b49fa\", \"type\": \"WEB\", \"updated\": \"2023-03-28T18:08:29Z\", \"version\": \"4.1.8\", \"seq\": 75, \"group\": 1 } ], \"monitors\": [ \"2023032896b12f9de149459f9c8b71ad8b6b49fa - 2023-03-28T18:08:46Z\" ] }, \"java\": { \"version\": \"18.0.2.1+1\", }, \"origin\": \"2023032896b12f9de149459f9c8b71ad8b6b49fa\", \"time\": { \"current\": \"2023-03-28T18:08:47.613Z\", \"start\": \"2023-03-28T17:31:23.611Z\" } } In this example, it shows that there are two user applications (rest-spring-3-example and lambda-example) connected. Presence monitor health endpoint The presence monitor has a \"/health\" endpoint. You can visit http://127.0.0.1:8080/health and it will show something like this: { \"dependency\": [ { \"route\": \"cloud.connector.health\", \"status_code\": 200, \"service\": \"kafka\", \"topics\": \"on-demand\", \"href\": \"127.0.0.1:9092\", \"message\": \"Loopback test took 3 ms; System contains 2 topics\", \"required\": true } ], \"origin\": \"2023032896b12f9de149459f9c8b71ad8b6b49fa\", \"name\": \"kafka-presence\", \"status\": \"UP\" } User application health endpoint Similarly, you can check the health status of the rest-spring-3-example application with http://127.0.0.1:8083/health { \"dependency\": [ { \"route\": \"cloud.connector.health\", \"status_code\": 200, \"service\": \"kafka\", \"topics\": \"on-demand\", \"href\": \"127.0.0.1:9092\", \"message\": \"Loopback test took 4 ms\", \"required\": true } ], \"origin\": \"202303282583899cf43a49b98f0522492b9ca178\", \"name\": \"rest-spring-example\", \"status\": \"UP\" } It looks similar to the health status of the presence monitor. However, only the presence monitor shows the total number of topics because it handles topic issuance to each user application instance. Actuator endpoints Additional actuator endpoints includes: library endpoint (\"/info/lib\") - you can check the packaged libraries for each application distributed routing table (\"/info/routes\") - this will display the distributed routing table for public functions environment (\"/env\") - it shows all functions (public and private) with number of workers. livenessproble (\"/livenessprobe\") - this should display \"OK\" to indicate the application is running Stop an application You can press \"control-C\" to stop an application. Let's stop the lambda-example application. Once you stopped lamdba-example from the command line, the rest-spring-3-example will detect it: ServiceRegistry:278 - Peer 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25 left (lambda-example 4.1.8) ServiceRegistry:401 - hello.world 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25 unregistered ServiceRegistry:401 - hello.pojo 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25 unregistered The rest-spring-3-example will update its distributed routing table automatically. You will also find log messages in the kafka-presence application like this: MonitorService:120 - Member 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25 left TopicController:250 - multiplex.0001-001 released by 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25, lambda-example, 4.1.8 When an application instance stops, the presence monitor will detect the event, remove it from the registry and release the topic associated with the disconnected application instance. The presence monitor is using the \"presence\" feature in websocket, thus we call it \"presence\" monitor. Chapter-7 Home CHAPTER-9 Event over HTTP Table of Contents API Overview","title":"Chapter-8"},{"location":"guides/CHAPTER-8/#minimalist-service-mesh","text":"Service mesh is a dedicated infrastructure layer to facilitate inter-container communication using \"sidecar\" and \"control plane\". Service mesh systems require additional administrative containers (PODs) for \"control plane\" and \"service discovery.\" The additional infrastructure requirements vary among products.","title":"Minimalist Service Mesh"},{"location":"guides/CHAPTER-8/#using-kafka-as-a-minimalist-service-mesh","text":"We will discuss using Kafka as a minimalist service mesh. Important : Always design your application system in an event-driven manner to decouple application services from each other. You should avoid using service mesh to tightly couple application services together because it would lead to the creation of a \"Distributed Monolith\". Minimalist service mesh is designed as a service discovery mechanism so that you can detect the presence of other application instances. You can use it for operation control mechanism and \"leader selection\" in writing resilient distributed application. Typically, a service mesh system uses a \"side-car\" to sit next to the application container in the same POD to provide service discovery and network proxy services. Instead of using a side-car proxy, the system maintains a distributed routing table in each application instance. When a function requests the service of another function which is not in the same memory space, the \"cloud.connector\" module will bridge the event to the peer application through a network event system like Kafka. As shown in the following table, if \"service.1\" and \"service.2\" are in the same memory space of an application, they will communicate using the in-memory event bus. If they are in different applications and the applications are configured with Kafka, the two functions will communicate via the \"cloud.connector\" service. In-memory event bus Network event stream \"service.1\" -> \"service.2\" \"service.1\" -> \"cloud.connector\" -> \"service.2\" The system supports Kafka out of the box. For example, to select kafka, you can configure application.properties like this: cloud.connector=kafka The \"cloud.connector\" parameter can be set to \"none\" or \"kafka\". The default parameter of \"cloud.connector\" is \"none\". This means the application is not using any network event system \"connector\", thus running independently. Let's set up a minimalist service mesh with Kafka to see how it works.","title":"Using kafka as a minimalist service mesh"},{"location":"guides/CHAPTER-8/#set-up-a-standalone-kafka-server-for-development","text":"You need a Kafka cluster as the network event stream system. For development and testing, you can build and run a standalone Kafka server like this. Note that the mvn clean package command is optional because the executable JAR should be available after the mvn clean install command in Chapter-1 . cd connectors/adapters/kafka/kafka-standalone mvn clean package java -jar target/kafka-standalone-3.1.2.jar The standalone Kafka server will start at port 9092. You may adjust the \"server.properties\" in the standalone-kafka project when necessary. When the kafka server is started, it will create a temporary directory \"/tmp/kafka-logs\". Note : The kafka server is designed for development purpose only. The kafka message log store will be cleared when the server is restarted.","title":"Set up a standalone Kafka server for development"},{"location":"guides/CHAPTER-8/#prepare-the-kafka-presence-application","text":"The \"kafka-presence\" is a \"presence monitor\" application. It is a minimalist \"control plane\" in service mesh terminology. What is a presence monitor? A presence monitor is the control plane that assigns unique \"topic\" for each user application instance. It monitors the \"presence\" of each application. If an application fails or stops, the presence monitor will advertise the event to the rest of the system so that each application container will update its corresponding distributed routing table, thus bypassing the failed application and its services. If an application has more than one container instance deployed, they will work together to share load evenly. You will start the presence monitor like this: cd connectors/adapters/kafka/kafka-presence java -jar target/kafka-presence-3.1.2.jar By default, the kafka-connector will run at port 8080. Partial start-up log is shown below: AppStarter:344 - Modules loaded in 2,370 ms AppStarter:334 - Websocket server running on port-8080 ServiceLifeCycle:73 - service.monitor, partition 0 ready HouseKeeper:72 - Registered monitor (me) 2023032896b12f9de149459f9c8b71ad8b6b49fa The presence monitor will use the topic \"service.monitor\" to connect to the Kafka server and register itself as a presence monitor. Presence monitor is resilient. You can run more than one instance to back up each other. If you are not using Docker or Kubernetes, you need to change the \"server.port\" parameter of the second instance to 8081 so that the two application instances can run in the same laptop.","title":"Prepare the kafka-presence application"},{"location":"guides/CHAPTER-8/#launch-the-rest-spring-3-example-and-lambda-example-with-kafka","text":"Let's run the rest-spring-3-example and lambda-example applications with Kafka connector turned on. For demo purpose, the rest-spring-3-example and lambda-example are pre-configured with \"kafka-connector\". If you do not need these libraries, please remove them from the pom.xml built script. Since kafka-connector is pre-configured, we can start the two demo applications like this: cd examples/rest-spring-3-example java -Dcloud.connector=kafka -Dmandatory.health.dependencies=cloud.connector.health -jar target/rest-spring-3-example-3.1.2.jar cd examples/lambda-example java -Dcloud.connector=kafka -Dmandatory.health.dependencies=cloud.connector.health -jar target/lambda-example-3.1.2.jar The above command uses the \"-D\" parameters to configure the \"cloud.connector\" and \"mandatory.health.dependencies\". The parameter mandatory.health.dependencies=cloud.connector.health tells the system to turn on the health check endpoint for the application. For the rest-spring-3-example, the start-up log may look like this: AppStarter:344 - Modules loaded in 2,825 ms PresenceConnector:155 - Connected pc.abb4a4de.in, 127.0.0.1:8080, /ws/presence/202303282583899cf43a49b98f0522492b9ca178 EventConsumer:160 - Subscribed multiplex.0001.0 ServiceLifeCycle:73 - multiplex.0001, partition 0 ready This means that the rest-spring-3-example has successfully connected to the presence monitor at port 8080. It has subscribed to the topic \"multiplex.0001\" partition 0. For the lambda-example, the log may look like this: AppStarter:344 - Modules loaded in 2,742 m PresenceConnector:155 - Connected pc.991a2be0.in, 127.0.0.1:8080, /ws/presence/2023032808d82ebe2c0d4e5aa9ca96b3813bdd25 EventConsumer:160 - Subscribed multiplex.0001.1 ServiceLifeCycle:73 - multiplex.0001, partition 1 ready ServiceRegistry:242 - Peer 202303282583899cf43a49b98f0522492b9ca178 joins (rest-spring-3-example 4.1.8) ServiceRegistry:383 - hello.world (rest-spring-3-example, WEB.202303282583899cf43a49b98f0522492b9ca178) registered You notice that the lambda-example has discovered the rest-spring-3-example through Kafka and added the \"hello.world\" to the distributed routing table. At this point, the rest-spring-3-example will find the lambda-example application as well: ServiceRegistry:242 - Peer 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25 joins (lambda-example 4.1.8) ServiceRegistry:383 - hello.world (lambda-example, APP.2023032808d82ebe2c0d4e5aa9ca96b3813bdd25) registered ServiceRegistry:383 - hello.pojo (lambda-example, APP.2023032808d82ebe2c0d4e5aa9ca96b3813bdd25) registered This is real-time service discovery coordinated by the \"kafka-presence\" monitor application. Now you have created a minimalist event-driven service mesh.","title":"Launch the rest-spring-3-example and lambda-example with kafka"},{"location":"guides/CHAPTER-8/#send-an-event-request-from-rest-spring-3-example-to-lambda-example","text":"In Chapter-7 , you have sent a request from the rest-spring-3-example to the lambda-example using \"Event over HTTP\" without a service mesh. In this section, you can make the same request using service mesh. Please point your browser to http://127.0.0.1:8083/api/pojo/mesh/1 You will see the following response in your browser. { \"id\": 1, \"name\": \"Simple PoJo class\", \"address\": \"100 World Blvd, Planet Earth\", \"date\": \"2023-03-28T17:53:41.696Z\", \"instance\": 1, \"seq\": 1, \"origin\": \"2023032808d82ebe2c0d4e5aa9ca96b3813bdd25\" }","title":"Send an event request from rest-spring-3-example to lambda-example"},{"location":"guides/CHAPTER-8/#presence-monitor-info-endpoint","text":"You can check the service mesh status from the presence monitor's \"/info\" endpoint. You can visit http://127.0.0.1:8080/info and it will show something like this: { \"app\": { \"name\": \"kafka-presence\", \"description\": \"Presence Monitor\", \"version\": \"4.1.8\" }, \"personality\": \"RESOURCES\", \"more\": { \"total\": { \"topics\": 2, \"virtual_topics\": 2, \"connections\": 2 }, \"topics\": [ \"multiplex.0001 (32)\", \"service.monitor (11)\" ], \"virtual_topics\": [ \"multiplex.0001-000 -> 202303282583899cf43a49b98f0522492b9ca178, rest-spring-3-example v4.1.8\", \"multiplex.0001-001 -> 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25, lambda-example v4.1.8\" ], \"connections\": [ { \"elapsed\": \"25 minutes 12 seconds\", \"created\": \"2023-03-28T17:43:13Z\", \"origin\": \"2023032808d82ebe2c0d4e5aa9ca96b3813bdd25\", \"name\": \"lambda-example\", \"topic\": \"multiplex.0001-001\", \"monitor\": \"2023032896b12f9de149459f9c8b71ad8b6b49fa\", \"type\": \"APP\", \"updated\": \"2023-03-28T18:08:25Z\", \"version\": \"4.1.8\", \"seq\": 65, \"group\": 1 }, { \"elapsed\": \"29 minutes 42 seconds\", \"created\": \"2023-03-28T17:38:47Z\", \"origin\": \"202303282583899cf43a49b98f0522492b9ca178\", \"name\": \"rest-spring-3-example\", \"topic\": \"multiplex.0001-000\", \"monitor\": \"2023032896b12f9de149459f9c8b71ad8b6b49fa\", \"type\": \"WEB\", \"updated\": \"2023-03-28T18:08:29Z\", \"version\": \"4.1.8\", \"seq\": 75, \"group\": 1 } ], \"monitors\": [ \"2023032896b12f9de149459f9c8b71ad8b6b49fa - 2023-03-28T18:08:46Z\" ] }, \"java\": { \"version\": \"18.0.2.1+1\", }, \"origin\": \"2023032896b12f9de149459f9c8b71ad8b6b49fa\", \"time\": { \"current\": \"2023-03-28T18:08:47.613Z\", \"start\": \"2023-03-28T17:31:23.611Z\" } } In this example, it shows that there are two user applications (rest-spring-3-example and lambda-example) connected.","title":"Presence monitor info endpoint"},{"location":"guides/CHAPTER-8/#presence-monitor-health-endpoint","text":"The presence monitor has a \"/health\" endpoint. You can visit http://127.0.0.1:8080/health and it will show something like this: { \"dependency\": [ { \"route\": \"cloud.connector.health\", \"status_code\": 200, \"service\": \"kafka\", \"topics\": \"on-demand\", \"href\": \"127.0.0.1:9092\", \"message\": \"Loopback test took 3 ms; System contains 2 topics\", \"required\": true } ], \"origin\": \"2023032896b12f9de149459f9c8b71ad8b6b49fa\", \"name\": \"kafka-presence\", \"status\": \"UP\" }","title":"Presence monitor health endpoint"},{"location":"guides/CHAPTER-8/#user-application-health-endpoint","text":"Similarly, you can check the health status of the rest-spring-3-example application with http://127.0.0.1:8083/health { \"dependency\": [ { \"route\": \"cloud.connector.health\", \"status_code\": 200, \"service\": \"kafka\", \"topics\": \"on-demand\", \"href\": \"127.0.0.1:9092\", \"message\": \"Loopback test took 4 ms\", \"required\": true } ], \"origin\": \"202303282583899cf43a49b98f0522492b9ca178\", \"name\": \"rest-spring-example\", \"status\": \"UP\" } It looks similar to the health status of the presence monitor. However, only the presence monitor shows the total number of topics because it handles topic issuance to each user application instance.","title":"User application health endpoint"},{"location":"guides/CHAPTER-8/#actuator-endpoints","text":"Additional actuator endpoints includes: library endpoint (\"/info/lib\") - you can check the packaged libraries for each application distributed routing table (\"/info/routes\") - this will display the distributed routing table for public functions environment (\"/env\") - it shows all functions (public and private) with number of workers. livenessproble (\"/livenessprobe\") - this should display \"OK\" to indicate the application is running","title":"Actuator endpoints"},{"location":"guides/CHAPTER-8/#stop-an-application","text":"You can press \"control-C\" to stop an application. Let's stop the lambda-example application. Once you stopped lamdba-example from the command line, the rest-spring-3-example will detect it: ServiceRegistry:278 - Peer 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25 left (lambda-example 4.1.8) ServiceRegistry:401 - hello.world 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25 unregistered ServiceRegistry:401 - hello.pojo 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25 unregistered The rest-spring-3-example will update its distributed routing table automatically. You will also find log messages in the kafka-presence application like this: MonitorService:120 - Member 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25 left TopicController:250 - multiplex.0001-001 released by 2023032808d82ebe2c0d4e5aa9ca96b3813bdd25, lambda-example, 4.1.8 When an application instance stops, the presence monitor will detect the event, remove it from the registry and release the topic associated with the disconnected application instance. The presence monitor is using the \"presence\" feature in websocket, thus we call it \"presence\" monitor. Chapter-7 Home CHAPTER-9 Event over HTTP Table of Contents API Overview","title":"Stop an application"},{"location":"guides/CHAPTER-9/","text":"API Overview Main Application Each application has an entry point. You may implement an entry point in a main application like this: @MainApplication public class MainApp implements EntryPoint { public static void main(String[] args) { AutoStart.main(args); } @Override public void start(String[] args) { // your startup logic here log.info(\"Started\"); } } In your main application, you must implement the EntryPoint interface to override the \"start\" method. Typically, a main application is used to initiate some application start up procedure. In some case when your application does not need any start up logic, you can just print a message to indicate that your application has started. You may want to keep the static \"main\" method which can be used to run your application inside an IDE. The pom.xml build script is designed to run the AutoStart class that will execute your main application's start method. In some case, your application may have more than one main application module. You can decide the sequence of execution using the \"sequence\" parameter in the MainApplication annotation. The module with the smallest sequence number will run first. Duplicated sequence numbers are allowed. Normal startup sequence must be between 1 and 999. Note : It is the \"start\" method of each EntryPoint implementation that follows the execution sequence of the MainApplication annotation. The optional \"main\" method is used only to kick off the application bootstrap and it must include only the following statement: public static void main(String[] args) { AutoStart.main(args); } Therefore, even when the default sequence of the MainApplication annotation is 10 and you invoke the \"main\" method from an IDE, the \"start\" method of each MainApplication modules will execute orderly. Setup before the Main Application Sometimes, it may be required to set up some environment configuration before your main application starts. You can implement a BeforeApplication module. Its syntax is similar to the MainApplication . @BeforeApplication public class EnvSetup implements EntryPoint { @Override public void start(String[] args) { // your environment setup logic here log.info(\"initialized\"); } } The BeforeApplication logic will run before your MainApplication module(s). This is useful when you want to do special handling of environment variables. For example, decrypt an environment variable secret, construct an X.509 certificate, and save it in the \"/tmp\" folder before your main application starts. Note : Sequence 0 is reserved by the EssentialServiceLoader and 2 reserved by Event Script. Your user functions should use a number between 3 and 999. Event envelope Mercury is an event engine that encapsulates Eclipse Vertx and Java 21 virtual thread technology. A composable application is a collection of functions that communicate with each other in events. Each event is transported by an event envelope. Let's examine the envelope. There are 3 elements in an event envelope: Element Type Purpose 1 metadata Includes unique ID, target function name, reply address correlation ID, status, exception, trace ID and path 2 headers User defined key-value pairs 3 body Event payload (primitive, hash map or PoJo) Note : Headers and body are optional, but you should provide at least one of them. PoJo transport Your function can implement the TypedLambdaFunction interface if you want to use PoJo as input and output and the system will restore PoJo payload accordingly. However, if you use the EventEnvelope as an input in the TypedLambdaFunction, PoJo payload is mapped as a HashMap in the event's body. The original class name of the PoJo payload is saved in the event's type attribute. You can compare and restore the PoJo like this: if (SamplePoJo.class.getName().equals(input.getType())) { SamplePoJo pojo = input.getBody(SamplePoJo.class); // do something with your input PoJo } List of Pojo transport When sending events programmatically, you can send a list of PoJo to a user function. However, the list of pojo will be converted as a list of maps as input to the target function. Since type information is lost at runtime, you may add the inputPojoClass parameter in the PreLoad annotation of the target function. The system will then render the list of pojo as input to the target function. This applies to both untyped LambdaFunction and TypedLambdaFunction . In untyped LambdaFunction, the input is an object. In TypedLambdaFunction, you should configure the input as list of pojo and add the \"inputPojoClass\" hint in the PreLoad annotation. For example, the following unit test illustrates this: @PreLoad(route = \"input.list.of.pojo.java\", inputPojoClass = PoJo.class) public class InputAsListOfPoJo implements TypedLambdaFunction<List<PoJo>, Object> { @Override public Object handleEvent(Map<String, String> headers, List<PoJo> input, int instance) throws Exception { List<String> names = new ArrayList<>(); // prove that the list of pojo is correctly deserialized for (PoJo o: input) { if (o != null) { names.add(o.getName()); } else { names.add(\"null\"); } } Map<String, Object> result = new HashMap<>(); result.put(\"names\", names); return result; } } Note : List of PoJo as input to a composable function is not supported by input data mapping of Event Script. This is only allowed when sending events programmatically for certain use cases. PoJo deserialization hints The PoJo class definition in the TypedLambdaFunction has precedence over the \"inputPojoClass\" hint in the PreLoad annotation. For PoJo transport, the \"inputPojoClass\" parameter in the PreLoad annotation will only be used when the untyped \"LambdaFunction\" is declared. This is for backward compatibility with legacy version 2 and 3 where pojo transport restores pojo as input to user function written as untyped LambdaFunction. The list of pojo is handled differently as a deserialization hint in both the untyped LambdaFunction and the TypedLambdaFunction use cases as discussed earlier. Custom exception using AppException To reject an incoming request, you can throw an AppException like this: // example-1 throw new AppException(400, \"My custom error message\"); // example-2 throw new AppException(400, \"My custom error message\", ex); Example-1 - a simple exception with status code (400) and an error message Example-2 - includes a nested exception As a best practice, we recommend using error codes that are compatible with HTTP status codes. Defining a user function in Java You can write a function in Java like this: @PreLoad(route = \"hello.simple\", instances = 10) public class SimpleDemoEndpoint implements TypedLambdaFunction<AsyncHttpRequest, Object> { @Override public Object handleEvent(Map<String, String> headers, AsyncHttpRequest input, int instance) { // business logic here return result; } } The PreLoad annotation tells the system to preload the function into memory and register it into the event loop. You must provide a \"route name\" and configure the number of concurrent workers (\"instances\"). Route name is used by the event loop to find your function in memory. A route name must use lower letters and numbers, and it must have at least one dot as a word separator. e.g. \"hello.simple\" is a proper route name but \"HelloSimple\" is not. You can implement your function using the LambdaFunction or TypedLambdaFunction. The latter allows you to define the input and output classes. The system will map the event body into the input argument and the event headers into the headers argument. The instance argument informs your function which worker is serving the current request. Optionally, you can specify an envInstances parameter. This tells the system to use a parameter from the application.properties (or application.yml) to configure the number of workers for the function. When the parameter defined in \"envInstances\" is not found, the \"instances\" parameter is used as the default value. You can override special services such as no.op , reslience.handler , and simple.exception.handler using the following properties: worker.instances.no.op worker.instances.resilience.handler worker.instances.simple.exception.handler Inspect event metadata There are some reserved metadata such as route name (\"my_route\"), trace ID (\"my_trace_id\") and trace path (\"my_trace_path\") in the \"headers\" argument. They do not exist in the incoming event envelope. Instead, the system automatically insert them as read-only metadata. They are used to create a trackable instance of the PostOffice. e.g. var po = PostOffice.trackable(headers, instance); To inspect all metadata, you can declare the input as \"EventEnvelope\" in a TypedLambdaFunction. The system will map the whole event envelope into the \"input\" argument. You can retrieve the replyTo address and other useful items. Note : The \"replyTo\" address is optional. It is only required when the caller is making an RPC call. If the caller sends an asynchronous request, the \"replyTo\" value is null. Platform API You can obtain a singleton instance of the Platform object to do the following: Register a function We recommend using the PreLoad annotation in a class to declare the function route name, number of worker instances and whether the function is public or private. In some use cases where you want to create and destroy functions on demand, you can register them programmatically. In the following example, it registers \"my.function\" using the MyFunction class as a public function and \"another.function\" with the AnotherFunction class as a private function. Platform platform = Platform.getInstance(); // register a public function platform.register(\"my.function\", new MyFunction(), 10); // register a private function platform.registerPrivate(\"another.function\", new AnotherFunction(), 20); What is a public function? A public function is visible by any application instances in the same network. When a function is declared as \"public\", the function is reachable through the Event-over-HTTP REST endpoint or a service mesh. A private function is invisible outside the memory space of the application instance that it resides. This allows application to encapsulate business logic according to domain boundary. You can assemble closely related functions as a composable application that can be deployed independently. Release a function In some use cases, you want to release a function on-demand when it is no longer required. platform.release(\"another.function\"); The above API will unload the function from memory and release it from the \"event loop\". Check if a function is available You can check if a function with the named route has been deployed. if (platform.hasRoute(\"another.function\")) { // do something } Wait for a function to be ready Functions are registered asynchronously. For functions registered using the PreLoad annotation, they are available to your application when the MainApplication starts. For functions that are registered on-demand, you can wait for the function to get ready like this: platform.waitForProvider(\"cloud.connector\", 10) .onSuccess(ready -> { // business logic when \"cloud.connector\" is ready }); Note that the \"onFailure\" method is not required. The onSuccess will return true or false. In the above example, your application waits for up to 10 seconds. If the function (i.e. the \"provider\") is available, the API will invoke the \"onSuccess\" method immediately. Obtain the unique application instance ID When an application instance starts, a unique ID is generated. We call this the \"Origin ID\". var originId = po.getOrigin(); When running the application in a minimalist service mesh using Kafka or similar network event stream system, the origin ID is used to uniquely identify the application instance. The origin ID is automatically appended to the \"replyTo\" address when making a RPC call over a network event stream so that the system can send the response event back to the \"originator\" or \"calling\" application instance. Set application personality An application may have one of the following personality: REST - the deployed application is user facing APP - the deployed application serves business logic RESOURCES - this is a resource-tier service. e.g. database service, MQ gateway, legacy service proxy, utility, etc. You can change the application personality like this: // the default value is \"APP\" ServerPersonality.getInstance().setType(ServerPersonality.Type.REST); The personality setting is for documentation purpose only. It does not affect the behavior of your application. It will appear in the application \"/info\" endpoint. PostOffice API You can obtain an instance of the PostOffice from the input \"headers\" and \"instance\" parameters in the input arguments of your function. var po = PostOffice.trackable(headers, instance); The PostOffice is the event manager that you can use to send asynchronous events or to make RPC requests. The constructor uses the READ only metadata in the \"headers\" argument in the \"handleEvent\" method of your function. For end-to-end traceability, please use the PostOffice instance to make requests to a composable library. It maintains the same traceId and tracePath in the traceability graph. If your handleEvent method calls another method in your class, you should pass this PostOffice instance so that any event calls from the other method can propagate the tracing information. For Unit Tests, since a test does not start with the handleEvent of a LambdaFunction, you can use the following to create a PostOffice with your own traceId. The \"myRoute\" is the caller's route name. In this case, you can set it to \"unit.test\". public PostOffice(String myRoute, String myTraceId, String myTracePath); Send an asynchronous event to a function You can send an asynchronous event like this. // example-1 po.send(\"another.function\", \"test message\"); // example-2 po.send(\"another.function\", new Kv(\"some_key\", \"some_value\"), new kv(\"another_key\", \"another_value\")); // example-3 po.send(\"another.function\", somePoJo, new Kv(\"some_key\", \"some_value\")); // example-4 EventEnvelope event = new EventEnvelope().setTo(\"another.function\") .setHeader(\"some_key\", \"some_value\").setBody(somePoJo); po.send(event) // example-5 po.sendLater(event, new Date(System.currentTimeMillis() + 5000)); Example-1 sends the text string \"test message\" to the target service named \"another.function\". Example-2 sends two key-values as \"headers\" parameters to the same service. Example-3 sends a PoJo and a key-value pair to the same service. Example-4 is the same as example-3. It is using an EventEnvelope to construct the request. Example-5 schedules an event to be sent 5 seconds later. The first 3 APIs are convenient methods and the system will automatically create an EventEnvelope to hold the target route name, key-values and/or event payload. Make an asynchronous RPC call You can make RPC call like this: // example-1 EventEnvelope request = new EventEnvelope().setTo(\"another.function\") .setHeader(\"some_key\", \"some_value\").setBody(somePoJo); Future<EventEnvelope> response = po.asyncRequest(request, 5000); response.onSuccess(result -> { // result is the response event }); response.onFailure(e -> { // handle timeout exception }); // example-2 Future<EventEnvelope> response = po.asyncRequest(request, 5000, false); response.onSuccess(result -> { // result is the response event // Timeout exception is returned as a response event with status=408 }); // example-3 with the \"rpc\" boolean parameter set to true Future<EventEnvelope> response = po.asyncRequest(request, 5000, \"http://peer/api/event\", true); response.onSuccess(result -> { // result is the response event }); response.onFailure(e -> { // handle timeout exception }); Example-1 makes a RPC call with a 5-second timeout to \"another.function\". Example-2 sets the \"timeoutException\" to false, telling system to return timeout exception as a regular event. Example-3 makes an \"event over HTTP\" RPC call to \"another.function\" in another application instance called \"peer\". \"Event over HTTP\" is an important topic. Please refer to Chapter 7 for more details. Perform a fork-n-join RPC call to multiple functions In a similar fashion, you can make a fork-n-join call that sends request events in parallel to more than one function. // example-1 EventEnvelope request1 = new EventEnvelope().setTo(\"this.function\") .setHeader(\"hello\", \"world\").setBody(\"test message\"); EventEnvelope request2 = new EventEnvelope().setTo(\"that.function\") .setHeader(\"good\", \"day\").setBody(somePoJo); List<EventEnvelope> requests = new ArrayList<>(); requests.add(request1); requests.add(request2); Future<List<EventEnvelope>> responses = po.asyncRequest(requests, 5000); response.onSuccess(results -> { // results contains the response events }); response.onFailure(e -> { // handle timeout exception }); // example-2 Future<List<EventEnvelope>> responses = po.asyncRequest(requests, 5000, false); response.onSuccess(results -> { // results contains the response events. // Partial result list is returned if one or more functions did not respond. }); Make a sequential non-blocking RPC call You can make a sequential non-blocking RPC call from one function to another. The most convenient method to make a sequential non-blocking RPC call is to use the PostOffice's request API. // for a single RPC call PostOffice po = PostOffice.trackable(headers, instance); EventEnvelope result = po.request(requestEvent, timeoutInMills).get(); // for a fork-n-join call PostOffice po = PostOffice.trackable(headers, instance); List<EventEnvelope> result = po.request(requestEvents, timeoutInMills).get(); Note : the \"eRequest\" is a reactive version of the above \"request\" methods. Its return type is a CompletableFuture, thus allowing the use of \"nextAccept\" method to process result asynchronously. Check if a function with a named route exists The PostOffice provides the \"exists()\" method that is similar to the \"platform.hasRoute()\" command. The difference is that the \"exists()\" method can discover functions of another application instance when running in the \"service mesh\" mode. If your application is not deployed in a service mesh, the PostOffice's \"exists\" and Platform's \"hasRoute\" APIs will provide the same result. boolean found = po.exists(\"another.function\"); if (found) { // do something } Retrieve trace ID and path If you want to know the route name and optional trace ID and path, you can use the following APIs. For example, if tracing is enabled, the trace ID will be available. You can put the trace ID in application log messages. This would group log messages of the same transaction together when you search the trace ID from a centralized logging dashboard such as Splunk. String myRoute = po.getRoute(); String traceId = po.getTraceId(); String tracePath = po.getTracePath(); Trace annotation To annotate additional information in the trace of your function, please obtain a trackable PostOffice instance using PostOffice.trackable(headers, instance) and follow the following API signatures: // API signatures public PostOffice annotateTrace(String key, String value); public PostOffice annotateTrace(String key, Map<String, Object> value); public PostOffice annotateTrace(String key, List<Object> value); // For example, var po = PostOffice.trackable(headers, instance); po.annotateTrace(\"hello\", \"world\"); Annotations of key-values, if any, will be recorded in the trace and they are not accessible by another function. Please be moderate to attach only small amount of transaction specific information to the performance metrics of your functions. Note : Don't annotate sensitive information or secrets such as PII, PHI, PCI data because the trace is visible in the application log. It may also be forwarded to a centralized telemetry dashboard for visualization and analytics. Configuration API Your function can access the main application configuration from the platform like this: AppConfigReader config = AppConfigReader.getInstance(); // the value can be string or a primitive Object value = config.get(\"my.parameter\"); // the return value will be converted to a string String text = config.getProperty(\"my.parameter\"); The system uses the standard dot-bracket format for a parameter name. e.g. hello.world some.key[2] You can override the main application configuration at run-time using the Java argument \"-D\". e.g. java -Dserver.port=8080 -jar myApp.jar Additional configuration files can be added with the ConfigReader constructor like this: // filePath should have location prefix \"classpath:/\" or \"file:/\" ConfigReader reader = new ConfigReader(filePath); The configuration system supports environment variable or reference to the main application configuration using the dollar-bracket syntax ${reference:default_value} . e.g. some.key=${MY_ENV_VARIABLE} another.key=${my.key:12345} complex.key=first ${FIRST_ENV_VAR}, second ${SECOND_ENV_VAR} In the above example, a parameter may contain references to more than one environment variable. Default value, if not given, will be assumed to be an empty string. Custom serializer We are using GSON as the underlying serializer to handle common use cases. However, there may be situation that you want to use your own custom serialization library. To do that, you may write a serializer that implements the CustomSerializer interface: public interface CustomSerializer { public Map<String, Object> toMap(Object obj); public <T> T toPoJo(Object obj, Class<T> toValueType); } You may configure a user function to use a custom serializer by adding the \"customSerializer\" parameter in the PreLoad annotation. For example, @PreLoad(route=\"my.user.function\", customSerializer = JacksonSerializer.class) public class MyUserFunction implements TypedLambdaFunction<SimplePoJo, SimplePoJo> { @Override public SimplePoJo handleEvent(Map<String, String> headers, SimplePoJo input, int instance) { return input; } } If you register your function dynamically in code, you can use the following platform API to assign a custom serializer. public void setCustomSerializer(String route, CustomSerializer mapper); // e.g. // platform.setCustomSerializer(\"my.function\", new JacksonSerializer()); If you use the PostOffice to programmatically send event or make event RPC call and you need a custom serializer, you can create a PostOffice instance like this: // this should be the first statement in the \"handleEvent\" method. PostOffice po = PostOffice.withSerializer(headers, instance, new MyCustomSerializer()); The outgoing event using the PostOffice will use the custom serializer automatically. To interpret an event response from a RPC call, you can use the following PostOffice API: MyPoJo result = po.getEventBodyAsPoJo(responseEvent, MyPoJo.class); Minimalist API design As a best practice, we advocate a minimalist approach in API integration. To build powerful composable applications, the above set of APIs is sufficient to perform \"event orchestration\" where you write code to coordinate how the various functions work together as a single \"executable\". Please refer to Chapter-4 for more details about event orchestration. Since Mercury is used in production installations, we will exercise the best effort to keep the core API stable. Other APIs in the toolkits are used internally to build the engine itself, and they may change from time to time. They are mostly convenient methods and utilities. The engine is fully encapsulated and any internal API changes are not likely to impact your applications. Event Scripting To further reduce coding effort, you can perform \"event choreography\" by configuration using \"Event Script\". Please refer to Event Script syntax in Chapter 4 Co-existence with other development frameworks Mercury libraries are designed to co-exist with your favorite frameworks and tools. Inside a class implementing the LambdaFunction or TypedLambdaFunction , you can use any coding style and frameworks as you like, including sequential, object-oriented and reactive programming styles. The core-engine has a built-in lightweight non-blocking HTTP server, but you can also use Spring Boot and other application server framework with it. A sample Spring Boot integration is provided in the \"rest-spring-3\" project. It is an optional feature, and you can decide to use a regular Spring Boot application with Mercury Composable or to pick the customized Spring Boot in the \"rest-spring-3\" library. Application template for quick start We recommend using the composable-example project as a template to start writing your own Composable applications. You can follow the Composable methodology where you draw event flow diagrams to represent various use cases, convert them into event scripts that carry out event choreography for your self-contained functions. For more information, please refer to Event Script syntax in Chapter 4 . If you prefer to do low-level event-driven programming, you can use the lambda-example project as a template. It is preconfigured to support kernel threads and virtual threads. Source code update frequency This project is licensed under the Apache 2.0 open sources license. We will update the public codebase after it passes regression tests and meets stability and performance benchmarks in our production systems. Mercury Composable is developed as an engine for you to build the latest cloud native applications. Composable technology is evolving rapidly. We would exercise best effort to keep the essential internals and core APIs stable. Please browse the latest Developer Guide, release notes and Javadoc for any breaking API changes. Technical support For enterprise clients, technical support is available. Please contact your Accenture representative for details. Chapter-8 Home Minimalist Service Mesh Table of Contents","title":"Chapter-9"},{"location":"guides/CHAPTER-9/#api-overview","text":"","title":"API Overview"},{"location":"guides/CHAPTER-9/#main-application","text":"Each application has an entry point. You may implement an entry point in a main application like this: @MainApplication public class MainApp implements EntryPoint { public static void main(String[] args) { AutoStart.main(args); } @Override public void start(String[] args) { // your startup logic here log.info(\"Started\"); } } In your main application, you must implement the EntryPoint interface to override the \"start\" method. Typically, a main application is used to initiate some application start up procedure. In some case when your application does not need any start up logic, you can just print a message to indicate that your application has started. You may want to keep the static \"main\" method which can be used to run your application inside an IDE. The pom.xml build script is designed to run the AutoStart class that will execute your main application's start method. In some case, your application may have more than one main application module. You can decide the sequence of execution using the \"sequence\" parameter in the MainApplication annotation. The module with the smallest sequence number will run first. Duplicated sequence numbers are allowed. Normal startup sequence must be between 1 and 999. Note : It is the \"start\" method of each EntryPoint implementation that follows the execution sequence of the MainApplication annotation. The optional \"main\" method is used only to kick off the application bootstrap and it must include only the following statement: public static void main(String[] args) { AutoStart.main(args); } Therefore, even when the default sequence of the MainApplication annotation is 10 and you invoke the \"main\" method from an IDE, the \"start\" method of each MainApplication modules will execute orderly.","title":"Main Application"},{"location":"guides/CHAPTER-9/#setup-before-the-main-application","text":"Sometimes, it may be required to set up some environment configuration before your main application starts. You can implement a BeforeApplication module. Its syntax is similar to the MainApplication . @BeforeApplication public class EnvSetup implements EntryPoint { @Override public void start(String[] args) { // your environment setup logic here log.info(\"initialized\"); } } The BeforeApplication logic will run before your MainApplication module(s). This is useful when you want to do special handling of environment variables. For example, decrypt an environment variable secret, construct an X.509 certificate, and save it in the \"/tmp\" folder before your main application starts. Note : Sequence 0 is reserved by the EssentialServiceLoader and 2 reserved by Event Script. Your user functions should use a number between 3 and 999.","title":"Setup before the Main Application"},{"location":"guides/CHAPTER-9/#event-envelope","text":"Mercury is an event engine that encapsulates Eclipse Vertx and Java 21 virtual thread technology. A composable application is a collection of functions that communicate with each other in events. Each event is transported by an event envelope. Let's examine the envelope. There are 3 elements in an event envelope: Element Type Purpose 1 metadata Includes unique ID, target function name, reply address correlation ID, status, exception, trace ID and path 2 headers User defined key-value pairs 3 body Event payload (primitive, hash map or PoJo) Note : Headers and body are optional, but you should provide at least one of them.","title":"Event envelope"},{"location":"guides/CHAPTER-9/#pojo-transport","text":"Your function can implement the TypedLambdaFunction interface if you want to use PoJo as input and output and the system will restore PoJo payload accordingly. However, if you use the EventEnvelope as an input in the TypedLambdaFunction, PoJo payload is mapped as a HashMap in the event's body. The original class name of the PoJo payload is saved in the event's type attribute. You can compare and restore the PoJo like this: if (SamplePoJo.class.getName().equals(input.getType())) { SamplePoJo pojo = input.getBody(SamplePoJo.class); // do something with your input PoJo }","title":"PoJo transport"},{"location":"guides/CHAPTER-9/#list-of-pojo-transport","text":"When sending events programmatically, you can send a list of PoJo to a user function. However, the list of pojo will be converted as a list of maps as input to the target function. Since type information is lost at runtime, you may add the inputPojoClass parameter in the PreLoad annotation of the target function. The system will then render the list of pojo as input to the target function. This applies to both untyped LambdaFunction and TypedLambdaFunction . In untyped LambdaFunction, the input is an object. In TypedLambdaFunction, you should configure the input as list of pojo and add the \"inputPojoClass\" hint in the PreLoad annotation. For example, the following unit test illustrates this: @PreLoad(route = \"input.list.of.pojo.java\", inputPojoClass = PoJo.class) public class InputAsListOfPoJo implements TypedLambdaFunction<List<PoJo>, Object> { @Override public Object handleEvent(Map<String, String> headers, List<PoJo> input, int instance) throws Exception { List<String> names = new ArrayList<>(); // prove that the list of pojo is correctly deserialized for (PoJo o: input) { if (o != null) { names.add(o.getName()); } else { names.add(\"null\"); } } Map<String, Object> result = new HashMap<>(); result.put(\"names\", names); return result; } } Note : List of PoJo as input to a composable function is not supported by input data mapping of Event Script. This is only allowed when sending events programmatically for certain use cases.","title":"List of Pojo transport"},{"location":"guides/CHAPTER-9/#pojo-deserialization-hints","text":"The PoJo class definition in the TypedLambdaFunction has precedence over the \"inputPojoClass\" hint in the PreLoad annotation. For PoJo transport, the \"inputPojoClass\" parameter in the PreLoad annotation will only be used when the untyped \"LambdaFunction\" is declared. This is for backward compatibility with legacy version 2 and 3 where pojo transport restores pojo as input to user function written as untyped LambdaFunction. The list of pojo is handled differently as a deserialization hint in both the untyped LambdaFunction and the TypedLambdaFunction use cases as discussed earlier.","title":"PoJo deserialization hints"},{"location":"guides/CHAPTER-9/#custom-exception-using-appexception","text":"To reject an incoming request, you can throw an AppException like this: // example-1 throw new AppException(400, \"My custom error message\"); // example-2 throw new AppException(400, \"My custom error message\", ex); Example-1 - a simple exception with status code (400) and an error message Example-2 - includes a nested exception As a best practice, we recommend using error codes that are compatible with HTTP status codes.","title":"Custom exception using AppException"},{"location":"guides/CHAPTER-9/#defining-a-user-function-in-java","text":"You can write a function in Java like this: @PreLoad(route = \"hello.simple\", instances = 10) public class SimpleDemoEndpoint implements TypedLambdaFunction<AsyncHttpRequest, Object> { @Override public Object handleEvent(Map<String, String> headers, AsyncHttpRequest input, int instance) { // business logic here return result; } } The PreLoad annotation tells the system to preload the function into memory and register it into the event loop. You must provide a \"route name\" and configure the number of concurrent workers (\"instances\"). Route name is used by the event loop to find your function in memory. A route name must use lower letters and numbers, and it must have at least one dot as a word separator. e.g. \"hello.simple\" is a proper route name but \"HelloSimple\" is not. You can implement your function using the LambdaFunction or TypedLambdaFunction. The latter allows you to define the input and output classes. The system will map the event body into the input argument and the event headers into the headers argument. The instance argument informs your function which worker is serving the current request. Optionally, you can specify an envInstances parameter. This tells the system to use a parameter from the application.properties (or application.yml) to configure the number of workers for the function. When the parameter defined in \"envInstances\" is not found, the \"instances\" parameter is used as the default value. You can override special services such as no.op , reslience.handler , and simple.exception.handler using the following properties: worker.instances.no.op worker.instances.resilience.handler worker.instances.simple.exception.handler","title":"Defining a user function in Java"},{"location":"guides/CHAPTER-9/#inspect-event-metadata","text":"There are some reserved metadata such as route name (\"my_route\"), trace ID (\"my_trace_id\") and trace path (\"my_trace_path\") in the \"headers\" argument. They do not exist in the incoming event envelope. Instead, the system automatically insert them as read-only metadata. They are used to create a trackable instance of the PostOffice. e.g. var po = PostOffice.trackable(headers, instance); To inspect all metadata, you can declare the input as \"EventEnvelope\" in a TypedLambdaFunction. The system will map the whole event envelope into the \"input\" argument. You can retrieve the replyTo address and other useful items. Note : The \"replyTo\" address is optional. It is only required when the caller is making an RPC call. If the caller sends an asynchronous request, the \"replyTo\" value is null.","title":"Inspect event metadata"},{"location":"guides/CHAPTER-9/#platform-api","text":"You can obtain a singleton instance of the Platform object to do the following:","title":"Platform API"},{"location":"guides/CHAPTER-9/#register-a-function","text":"We recommend using the PreLoad annotation in a class to declare the function route name, number of worker instances and whether the function is public or private. In some use cases where you want to create and destroy functions on demand, you can register them programmatically. In the following example, it registers \"my.function\" using the MyFunction class as a public function and \"another.function\" with the AnotherFunction class as a private function. Platform platform = Platform.getInstance(); // register a public function platform.register(\"my.function\", new MyFunction(), 10); // register a private function platform.registerPrivate(\"another.function\", new AnotherFunction(), 20);","title":"Register a function"},{"location":"guides/CHAPTER-9/#what-is-a-public-function","text":"A public function is visible by any application instances in the same network. When a function is declared as \"public\", the function is reachable through the Event-over-HTTP REST endpoint or a service mesh. A private function is invisible outside the memory space of the application instance that it resides. This allows application to encapsulate business logic according to domain boundary. You can assemble closely related functions as a composable application that can be deployed independently.","title":"What is a public function?"},{"location":"guides/CHAPTER-9/#release-a-function","text":"In some use cases, you want to release a function on-demand when it is no longer required. platform.release(\"another.function\"); The above API will unload the function from memory and release it from the \"event loop\".","title":"Release a function"},{"location":"guides/CHAPTER-9/#check-if-a-function-is-available","text":"You can check if a function with the named route has been deployed. if (platform.hasRoute(\"another.function\")) { // do something }","title":"Check if a function is available"},{"location":"guides/CHAPTER-9/#wait-for-a-function-to-be-ready","text":"Functions are registered asynchronously. For functions registered using the PreLoad annotation, they are available to your application when the MainApplication starts. For functions that are registered on-demand, you can wait for the function to get ready like this: platform.waitForProvider(\"cloud.connector\", 10) .onSuccess(ready -> { // business logic when \"cloud.connector\" is ready }); Note that the \"onFailure\" method is not required. The onSuccess will return true or false. In the above example, your application waits for up to 10 seconds. If the function (i.e. the \"provider\") is available, the API will invoke the \"onSuccess\" method immediately.","title":"Wait for a function to be ready"},{"location":"guides/CHAPTER-9/#obtain-the-unique-application-instance-id","text":"When an application instance starts, a unique ID is generated. We call this the \"Origin ID\". var originId = po.getOrigin(); When running the application in a minimalist service mesh using Kafka or similar network event stream system, the origin ID is used to uniquely identify the application instance. The origin ID is automatically appended to the \"replyTo\" address when making a RPC call over a network event stream so that the system can send the response event back to the \"originator\" or \"calling\" application instance.","title":"Obtain the unique application instance ID"},{"location":"guides/CHAPTER-9/#set-application-personality","text":"An application may have one of the following personality: REST - the deployed application is user facing APP - the deployed application serves business logic RESOURCES - this is a resource-tier service. e.g. database service, MQ gateway, legacy service proxy, utility, etc. You can change the application personality like this: // the default value is \"APP\" ServerPersonality.getInstance().setType(ServerPersonality.Type.REST); The personality setting is for documentation purpose only. It does not affect the behavior of your application. It will appear in the application \"/info\" endpoint.","title":"Set application personality"},{"location":"guides/CHAPTER-9/#postoffice-api","text":"You can obtain an instance of the PostOffice from the input \"headers\" and \"instance\" parameters in the input arguments of your function. var po = PostOffice.trackable(headers, instance); The PostOffice is the event manager that you can use to send asynchronous events or to make RPC requests. The constructor uses the READ only metadata in the \"headers\" argument in the \"handleEvent\" method of your function. For end-to-end traceability, please use the PostOffice instance to make requests to a composable library. It maintains the same traceId and tracePath in the traceability graph. If your handleEvent method calls another method in your class, you should pass this PostOffice instance so that any event calls from the other method can propagate the tracing information. For Unit Tests, since a test does not start with the handleEvent of a LambdaFunction, you can use the following to create a PostOffice with your own traceId. The \"myRoute\" is the caller's route name. In this case, you can set it to \"unit.test\". public PostOffice(String myRoute, String myTraceId, String myTracePath);","title":"PostOffice API"},{"location":"guides/CHAPTER-9/#send-an-asynchronous-event-to-a-function","text":"You can send an asynchronous event like this. // example-1 po.send(\"another.function\", \"test message\"); // example-2 po.send(\"another.function\", new Kv(\"some_key\", \"some_value\"), new kv(\"another_key\", \"another_value\")); // example-3 po.send(\"another.function\", somePoJo, new Kv(\"some_key\", \"some_value\")); // example-4 EventEnvelope event = new EventEnvelope().setTo(\"another.function\") .setHeader(\"some_key\", \"some_value\").setBody(somePoJo); po.send(event) // example-5 po.sendLater(event, new Date(System.currentTimeMillis() + 5000)); Example-1 sends the text string \"test message\" to the target service named \"another.function\". Example-2 sends two key-values as \"headers\" parameters to the same service. Example-3 sends a PoJo and a key-value pair to the same service. Example-4 is the same as example-3. It is using an EventEnvelope to construct the request. Example-5 schedules an event to be sent 5 seconds later. The first 3 APIs are convenient methods and the system will automatically create an EventEnvelope to hold the target route name, key-values and/or event payload.","title":"Send an asynchronous event to a function"},{"location":"guides/CHAPTER-9/#make-an-asynchronous-rpc-call","text":"You can make RPC call like this: // example-1 EventEnvelope request = new EventEnvelope().setTo(\"another.function\") .setHeader(\"some_key\", \"some_value\").setBody(somePoJo); Future<EventEnvelope> response = po.asyncRequest(request, 5000); response.onSuccess(result -> { // result is the response event }); response.onFailure(e -> { // handle timeout exception }); // example-2 Future<EventEnvelope> response = po.asyncRequest(request, 5000, false); response.onSuccess(result -> { // result is the response event // Timeout exception is returned as a response event with status=408 }); // example-3 with the \"rpc\" boolean parameter set to true Future<EventEnvelope> response = po.asyncRequest(request, 5000, \"http://peer/api/event\", true); response.onSuccess(result -> { // result is the response event }); response.onFailure(e -> { // handle timeout exception }); Example-1 makes a RPC call with a 5-second timeout to \"another.function\". Example-2 sets the \"timeoutException\" to false, telling system to return timeout exception as a regular event. Example-3 makes an \"event over HTTP\" RPC call to \"another.function\" in another application instance called \"peer\". \"Event over HTTP\" is an important topic. Please refer to Chapter 7 for more details.","title":"Make an asynchronous RPC call"},{"location":"guides/CHAPTER-9/#perform-a-fork-n-join-rpc-call-to-multiple-functions","text":"In a similar fashion, you can make a fork-n-join call that sends request events in parallel to more than one function. // example-1 EventEnvelope request1 = new EventEnvelope().setTo(\"this.function\") .setHeader(\"hello\", \"world\").setBody(\"test message\"); EventEnvelope request2 = new EventEnvelope().setTo(\"that.function\") .setHeader(\"good\", \"day\").setBody(somePoJo); List<EventEnvelope> requests = new ArrayList<>(); requests.add(request1); requests.add(request2); Future<List<EventEnvelope>> responses = po.asyncRequest(requests, 5000); response.onSuccess(results -> { // results contains the response events }); response.onFailure(e -> { // handle timeout exception }); // example-2 Future<List<EventEnvelope>> responses = po.asyncRequest(requests, 5000, false); response.onSuccess(results -> { // results contains the response events. // Partial result list is returned if one or more functions did not respond. });","title":"Perform a fork-n-join RPC call to multiple functions"},{"location":"guides/CHAPTER-9/#make-a-sequential-non-blocking-rpc-call","text":"You can make a sequential non-blocking RPC call from one function to another. The most convenient method to make a sequential non-blocking RPC call is to use the PostOffice's request API. // for a single RPC call PostOffice po = PostOffice.trackable(headers, instance); EventEnvelope result = po.request(requestEvent, timeoutInMills).get(); // for a fork-n-join call PostOffice po = PostOffice.trackable(headers, instance); List<EventEnvelope> result = po.request(requestEvents, timeoutInMills).get(); Note : the \"eRequest\" is a reactive version of the above \"request\" methods. Its return type is a CompletableFuture, thus allowing the use of \"nextAccept\" method to process result asynchronously.","title":"Make a sequential non-blocking RPC call"},{"location":"guides/CHAPTER-9/#check-if-a-function-with-a-named-route-exists","text":"The PostOffice provides the \"exists()\" method that is similar to the \"platform.hasRoute()\" command. The difference is that the \"exists()\" method can discover functions of another application instance when running in the \"service mesh\" mode. If your application is not deployed in a service mesh, the PostOffice's \"exists\" and Platform's \"hasRoute\" APIs will provide the same result. boolean found = po.exists(\"another.function\"); if (found) { // do something }","title":"Check if a function with a named route exists"},{"location":"guides/CHAPTER-9/#retrieve-trace-id-and-path","text":"If you want to know the route name and optional trace ID and path, you can use the following APIs. For example, if tracing is enabled, the trace ID will be available. You can put the trace ID in application log messages. This would group log messages of the same transaction together when you search the trace ID from a centralized logging dashboard such as Splunk. String myRoute = po.getRoute(); String traceId = po.getTraceId(); String tracePath = po.getTracePath();","title":"Retrieve trace ID and path"},{"location":"guides/CHAPTER-9/#trace-annotation","text":"To annotate additional information in the trace of your function, please obtain a trackable PostOffice instance using PostOffice.trackable(headers, instance) and follow the following API signatures: // API signatures public PostOffice annotateTrace(String key, String value); public PostOffice annotateTrace(String key, Map<String, Object> value); public PostOffice annotateTrace(String key, List<Object> value); // For example, var po = PostOffice.trackable(headers, instance); po.annotateTrace(\"hello\", \"world\"); Annotations of key-values, if any, will be recorded in the trace and they are not accessible by another function. Please be moderate to attach only small amount of transaction specific information to the performance metrics of your functions. Note : Don't annotate sensitive information or secrets such as PII, PHI, PCI data because the trace is visible in the application log. It may also be forwarded to a centralized telemetry dashboard for visualization and analytics.","title":"Trace annotation"},{"location":"guides/CHAPTER-9/#configuration-api","text":"Your function can access the main application configuration from the platform like this: AppConfigReader config = AppConfigReader.getInstance(); // the value can be string or a primitive Object value = config.get(\"my.parameter\"); // the return value will be converted to a string String text = config.getProperty(\"my.parameter\"); The system uses the standard dot-bracket format for a parameter name. e.g. hello.world some.key[2] You can override the main application configuration at run-time using the Java argument \"-D\". e.g. java -Dserver.port=8080 -jar myApp.jar Additional configuration files can be added with the ConfigReader constructor like this: // filePath should have location prefix \"classpath:/\" or \"file:/\" ConfigReader reader = new ConfigReader(filePath); The configuration system supports environment variable or reference to the main application configuration using the dollar-bracket syntax ${reference:default_value} . e.g. some.key=${MY_ENV_VARIABLE} another.key=${my.key:12345} complex.key=first ${FIRST_ENV_VAR}, second ${SECOND_ENV_VAR} In the above example, a parameter may contain references to more than one environment variable. Default value, if not given, will be assumed to be an empty string.","title":"Configuration API"},{"location":"guides/CHAPTER-9/#custom-serializer","text":"We are using GSON as the underlying serializer to handle common use cases. However, there may be situation that you want to use your own custom serialization library. To do that, you may write a serializer that implements the CustomSerializer interface: public interface CustomSerializer { public Map<String, Object> toMap(Object obj); public <T> T toPoJo(Object obj, Class<T> toValueType); } You may configure a user function to use a custom serializer by adding the \"customSerializer\" parameter in the PreLoad annotation. For example, @PreLoad(route=\"my.user.function\", customSerializer = JacksonSerializer.class) public class MyUserFunction implements TypedLambdaFunction<SimplePoJo, SimplePoJo> { @Override public SimplePoJo handleEvent(Map<String, String> headers, SimplePoJo input, int instance) { return input; } } If you register your function dynamically in code, you can use the following platform API to assign a custom serializer. public void setCustomSerializer(String route, CustomSerializer mapper); // e.g. // platform.setCustomSerializer(\"my.function\", new JacksonSerializer()); If you use the PostOffice to programmatically send event or make event RPC call and you need a custom serializer, you can create a PostOffice instance like this: // this should be the first statement in the \"handleEvent\" method. PostOffice po = PostOffice.withSerializer(headers, instance, new MyCustomSerializer()); The outgoing event using the PostOffice will use the custom serializer automatically. To interpret an event response from a RPC call, you can use the following PostOffice API: MyPoJo result = po.getEventBodyAsPoJo(responseEvent, MyPoJo.class);","title":"Custom serializer"},{"location":"guides/CHAPTER-9/#minimalist-api-design","text":"As a best practice, we advocate a minimalist approach in API integration. To build powerful composable applications, the above set of APIs is sufficient to perform \"event orchestration\" where you write code to coordinate how the various functions work together as a single \"executable\". Please refer to Chapter-4 for more details about event orchestration. Since Mercury is used in production installations, we will exercise the best effort to keep the core API stable. Other APIs in the toolkits are used internally to build the engine itself, and they may change from time to time. They are mostly convenient methods and utilities. The engine is fully encapsulated and any internal API changes are not likely to impact your applications.","title":"Minimalist API design"},{"location":"guides/CHAPTER-9/#event-scripting","text":"To further reduce coding effort, you can perform \"event choreography\" by configuration using \"Event Script\". Please refer to Event Script syntax in Chapter 4","title":"Event Scripting"},{"location":"guides/CHAPTER-9/#co-existence-with-other-development-frameworks","text":"Mercury libraries are designed to co-exist with your favorite frameworks and tools. Inside a class implementing the LambdaFunction or TypedLambdaFunction , you can use any coding style and frameworks as you like, including sequential, object-oriented and reactive programming styles. The core-engine has a built-in lightweight non-blocking HTTP server, but you can also use Spring Boot and other application server framework with it. A sample Spring Boot integration is provided in the \"rest-spring-3\" project. It is an optional feature, and you can decide to use a regular Spring Boot application with Mercury Composable or to pick the customized Spring Boot in the \"rest-spring-3\" library.","title":"Co-existence with other development frameworks"},{"location":"guides/CHAPTER-9/#application-template-for-quick-start","text":"We recommend using the composable-example project as a template to start writing your own Composable applications. You can follow the Composable methodology where you draw event flow diagrams to represent various use cases, convert them into event scripts that carry out event choreography for your self-contained functions. For more information, please refer to Event Script syntax in Chapter 4 . If you prefer to do low-level event-driven programming, you can use the lambda-example project as a template. It is preconfigured to support kernel threads and virtual threads.","title":"Application template for quick start"},{"location":"guides/CHAPTER-9/#source-code-update-frequency","text":"This project is licensed under the Apache 2.0 open sources license. We will update the public codebase after it passes regression tests and meets stability and performance benchmarks in our production systems. Mercury Composable is developed as an engine for you to build the latest cloud native applications. Composable technology is evolving rapidly. We would exercise best effort to keep the essential internals and core APIs stable. Please browse the latest Developer Guide, release notes and Javadoc for any breaking API changes.","title":"Source code update frequency"},{"location":"guides/CHAPTER-9/#technical-support","text":"For enterprise clients, technical support is available. Please contact your Accenture representative for details. Chapter-8 Home Minimalist Service Mesh Table of Contents","title":"Technical support"},{"location":"guides/METHODOLOGY/","text":"Background The high level concept of composable architecture was advocated by Gartner in 2022. At the platform level, composable architecture refers to loosely coupled platform services, utilities, and business applications. With modular design, you can assemble platform components and applications to create new use cases or to adjust for ever-changing business environment and requirements. Domain driven design (DDD), Command Query Responsibility Segregation (CQRS) and Microservices patterns are the popular tools that architects use to build composable architecture. You may deploy application in container, serverless or other means. At the application level, composable application means that an application is assembled from modular software components or functions that are self-contained and pluggable. You can mix-n-match functions to form new applications. You can retire outdated functions without adverse side effect to a production system. Multiple versions of a function can exist, and you can decide how to route user requests to different versions of a function. Applications would be easier to design, develop, maintain, deploy, and scale. In 2023, Accenture extended its event-driven development framework, codename \"Mercury\", to become the first implementation of a Composable framework to realize the goal of composable architecture and application design. Before we take a deep dive of the mercury-composable framework, let's review Composable Methodology first. Methodology The Composable Methodology takes a different approach in software development that empowers and aligns product owners, business analysts, technology architects and software engineers. Historically, there is a disconnect between product features and application design because product is user and business focused but application design is technology oriented. Product owners have almost no direct control over the quality of the user applications that are supposed to address business requirements. There is a communication barrier between the two domains in most projects. It requires a lot of iterations to get things right. The lack of direct connection between business domain and technical domain also leads to higher technical debts that are not just limited to imperfect coding. Composable methodology addresses this fundamental issue by connecting the two domains seamlessly. Before developers write a line of code, we start a project from product design. The output from a product design is a business transaction event flow diagram, ideally from a tactical Event Storming workshop or a more relaxed white boarding discussion among product owners, business domain experts and technology architects. Figure 1 - Event Flow Diagram As shown in Figure 1, a transaction flow diagram has a start point and an end point. Each processing step is called a \"task\". Some tasks would do calculation and some tasks would make decision to pass to the next steps. Note : Task types include decision, response, end, sequential, parallel, fork-n-join, sink and pipeline. The product team designs a transaction flow to address a business use case and presents it as a diagram. Naturally, this is how a human designer thinks. We don't want to prematurely adjust the business requirements to any infrastructural limitation. Events at the center of the modern enterprise In Domain Driven Design, it is well accepted that business transactions and their intermediate objects can be represented as \"events\". An event is a holder of a business object that may be a transaction request, a transaction response, a data object or any intermediate data representation. Generally, a data object is a structure of key-values. For simplicity, we will call a transaction flow diagram as \"Event Flow Diagram\" from now on. First principle - \"input-process-output\" The first principle of composable application design is that each task (also called \"function\") is self-contained and its input and output are immutable. Self-containment means that the task or function does not need to know the world outside its functional scope, thus making the function as pure as \"input-process-output\". i.e. given some input, the task or function will carry out some business logic to generate some output according to functional specification. Immutability is an important guardrail for clean code design. A function cannot change the business object outside its functional scope, thus eliminating the chance of unintended side effects of data interference. Note : In composable design, task and function can be used interchangeably. In an event flow diagram, we call each step a \"task\". In application design, we call each step a \"function\". Second principle - zero to one dependency The second principle is that each function should have zero dependency with other user functions. To connect to the world outside its functional scope, a function may have one and only one dependency with a platform or infrastructure component function. In composable design, a library can be packaged as a reusable composable function. A function consumes a platform component or library by sending an event to the component instead of tight coupling with a direct method call. Decoupling between functions and components promotes non-blocking, asynchronous and parallel operation that yields higher performance and throughput. Third principle - platform abstraction Since composable functions are self-contained and independent, they can be reused and repackaged into different applications to serve different purposes. Therefore, composable functions are, by definition, plug-n-play. The platform and infrastructure layers are encapsulated as adapters , gateways and wrappers . Figure 1 illustrates this architectural principle by connecting an event flow to the user through an event flow adapter. For example, a \"HTTP Flow Adapter\" serves both inbound request and outbound response. A \"Kafka Flow Adapter\" uses one topic for inbound and another topic for outbound events. Fourth principle - event choreography Without direct coupling, a composable development framework must support \"Event Choreography\" so that we can connect the various user composable functions, platform components and libraries together and route the events according to an event flow diagram for each use case. Application development Once we have drawn the event flow diagrams for different use cases, we can create user stories for each composable function and assign the development of each function to a developer or a pair of developers if using pair-programming. Composable methodology embraces \"Test Driven Development (TDD)\". Since each function is self-contained, it is TDD friendly because the developer does not need to deal with external dependencies for unit tests. It is the first principle of \"input-process-output\". This methodology allows us to scale our application development resources much better than traditional approach because developers do not need to know details of dependencies, thus avoiding hard code and reducing technical debts. For integration tests, it is easy to mock platform, infrastructure, database and HTTP resources by assigning mock functions to some tasks in an event flow. This greatly simplifies integration mocking. Seamless transition from product to application design Event flow diagram describes the product and each composable function contains the specific processing logic. Data attributes in an event flowing from one function to another provide the clarity for product designers and application engineers to communicate effectively. Application packaging and deployment Composable functions are independent and isolated. We can package related functions for an event flow in a single executable. A senior developer or architect can decide how to package related functions to reduce memory footprint and to scale efficiently and horizontally. Smaller memory footprint By design, composable functions are granular in nature. They are registered to an \"event loop\" on-demand. i.e. a function is executed when an event arrives. Without multiple levels of tight coupling, each piece of user code consumes memory efficiently. Memory is released to the system as soon as the function finishes execution. Composable framework To realize the composable design principles, a low-latency in-memory event system is available at the core of the Mercury-Composable framework. Figure 2 - Composable Framework As illustrated in Figure 2, event choreography for an event flow is described as an \"Event Flow Configuration\". An \"Event Manager\" is implemented as part of a low-latency in-memory event system. Composable functions (aka \"task\" in an event flow) ride on the in-memory event system so that the event manager can invoke them by sending events. Since composable functions are self-contained, the event manager performs input/output data mapping to and from the functional scope of a composable function. This flexible data mapping allows developers to write more generic code that responds to different input dataset, thus promoting software reusability. An in-memory state machine will be created for each execution of a transaction or \"event flow\". It is used for holding transaction \"states\" and temporary data objects. Sometimes a transaction may be suspended and restarted with different event flows. The in-memory state machine can be extended to an external datastore so that transaction states can be monitored and recovered operationally. Note : In Java, we combine the use of native Java 21 virtual thread management with Eclipse Vertx event bus. In Node.js, we use the EventEmitter event bus from the standard library. Event Envelope We use a standard \"Event Envelope\" to transport an event over the in-memory event bus. An event envelope contains three parts: (1) body, (2) headers and (3) metadata. Event body is used to transport a business object. In Java, it may be a PoJo or a HashMap. In node.js, it is a JSON object. Headers can be used to carry additional parameters to tell the user composable function what to do. Examples for metadata include performance metrics, status, exception, optional tracing information, and correlation ID. Language neutral The event flow configuration syntax and event envelope serialization scheme are standardized for polyglot deployment. The event flow configuration rides on YAML and event envelope serialization uses binary JSON (\"MsgPack\"). For higher serialization efficiency, the intermediate format is using \"key-value\" maps. JSON string is only used at the inbound and outbound flow adapters. Functional isolation Each function must implement the Composable interface. In Java, it is called \"TypedLambdaFunction\". In Node.js, it is called \"Composable\". The composable interface enforces a single \"handleEvent\" method where the function can access the event's headers and body. By design, each composable function is invoked by events. It is running parallel with other user functions. This non-blocking asynchronous execution architecture delivers higher performance than traditional coding approach with direct method invocation. While each function is executed in an event-driven and reactive manner, the user application code inside each composable function may use any coding style including object-oriented design, functional or reactive. Functional isolation means that you can use any open source or commercial library or framework in your user function without concerns about thread safety or unintended side effect. In Java, a composable function may look like this: @PreLoad(route = \"my.first.function\", instances = 10) public class MyFirstFunction implements TypedLambdaFunction<MyPoJo, AnotherPoJo> { @Override public AnotherPojo handleEvent(Map<String, String> headers, MyPoJo input, int instance) { // your business logic here return result; } } In Node.js, it may look like this: export class MyFirstFunction implements Composable { @preload('my.first.function', 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // your business logic here return result; } A composable function is declared with a \"route name\" using the \"preload\" annotation. While Java and Node.js have different syntax for annotation, they use a similar declarative approach. Note : the \"instance\" count for each composable function controls execution concurrency in a single application. It can be used with horizontal scaling to optimize use of computing resources. Home Chapter-1 Table of Contents Introduction","title":"Methodology"},{"location":"guides/METHODOLOGY/#background","text":"The high level concept of composable architecture was advocated by Gartner in 2022. At the platform level, composable architecture refers to loosely coupled platform services, utilities, and business applications. With modular design, you can assemble platform components and applications to create new use cases or to adjust for ever-changing business environment and requirements. Domain driven design (DDD), Command Query Responsibility Segregation (CQRS) and Microservices patterns are the popular tools that architects use to build composable architecture. You may deploy application in container, serverless or other means. At the application level, composable application means that an application is assembled from modular software components or functions that are self-contained and pluggable. You can mix-n-match functions to form new applications. You can retire outdated functions without adverse side effect to a production system. Multiple versions of a function can exist, and you can decide how to route user requests to different versions of a function. Applications would be easier to design, develop, maintain, deploy, and scale. In 2023, Accenture extended its event-driven development framework, codename \"Mercury\", to become the first implementation of a Composable framework to realize the goal of composable architecture and application design. Before we take a deep dive of the mercury-composable framework, let's review Composable Methodology first.","title":"Background"},{"location":"guides/METHODOLOGY/#methodology","text":"The Composable Methodology takes a different approach in software development that empowers and aligns product owners, business analysts, technology architects and software engineers. Historically, there is a disconnect between product features and application design because product is user and business focused but application design is technology oriented. Product owners have almost no direct control over the quality of the user applications that are supposed to address business requirements. There is a communication barrier between the two domains in most projects. It requires a lot of iterations to get things right. The lack of direct connection between business domain and technical domain also leads to higher technical debts that are not just limited to imperfect coding. Composable methodology addresses this fundamental issue by connecting the two domains seamlessly. Before developers write a line of code, we start a project from product design. The output from a product design is a business transaction event flow diagram, ideally from a tactical Event Storming workshop or a more relaxed white boarding discussion among product owners, business domain experts and technology architects. Figure 1 - Event Flow Diagram As shown in Figure 1, a transaction flow diagram has a start point and an end point. Each processing step is called a \"task\". Some tasks would do calculation and some tasks would make decision to pass to the next steps. Note : Task types include decision, response, end, sequential, parallel, fork-n-join, sink and pipeline. The product team designs a transaction flow to address a business use case and presents it as a diagram. Naturally, this is how a human designer thinks. We don't want to prematurely adjust the business requirements to any infrastructural limitation.","title":"Methodology"},{"location":"guides/METHODOLOGY/#events-at-the-center-of-the-modern-enterprise","text":"In Domain Driven Design, it is well accepted that business transactions and their intermediate objects can be represented as \"events\". An event is a holder of a business object that may be a transaction request, a transaction response, a data object or any intermediate data representation. Generally, a data object is a structure of key-values. For simplicity, we will call a transaction flow diagram as \"Event Flow Diagram\" from now on.","title":"Events at the center of the modern enterprise"},{"location":"guides/METHODOLOGY/#first-principle-input-process-output","text":"The first principle of composable application design is that each task (also called \"function\") is self-contained and its input and output are immutable. Self-containment means that the task or function does not need to know the world outside its functional scope, thus making the function as pure as \"input-process-output\". i.e. given some input, the task or function will carry out some business logic to generate some output according to functional specification. Immutability is an important guardrail for clean code design. A function cannot change the business object outside its functional scope, thus eliminating the chance of unintended side effects of data interference. Note : In composable design, task and function can be used interchangeably. In an event flow diagram, we call each step a \"task\". In application design, we call each step a \"function\".","title":"First principle - \"input-process-output\""},{"location":"guides/METHODOLOGY/#second-principle-zero-to-one-dependency","text":"The second principle is that each function should have zero dependency with other user functions. To connect to the world outside its functional scope, a function may have one and only one dependency with a platform or infrastructure component function. In composable design, a library can be packaged as a reusable composable function. A function consumes a platform component or library by sending an event to the component instead of tight coupling with a direct method call. Decoupling between functions and components promotes non-blocking, asynchronous and parallel operation that yields higher performance and throughput.","title":"Second principle - zero to one dependency"},{"location":"guides/METHODOLOGY/#third-principle-platform-abstraction","text":"Since composable functions are self-contained and independent, they can be reused and repackaged into different applications to serve different purposes. Therefore, composable functions are, by definition, plug-n-play. The platform and infrastructure layers are encapsulated as adapters , gateways and wrappers . Figure 1 illustrates this architectural principle by connecting an event flow to the user through an event flow adapter. For example, a \"HTTP Flow Adapter\" serves both inbound request and outbound response. A \"Kafka Flow Adapter\" uses one topic for inbound and another topic for outbound events.","title":"Third principle - platform abstraction"},{"location":"guides/METHODOLOGY/#fourth-principle-event-choreography","text":"Without direct coupling, a composable development framework must support \"Event Choreography\" so that we can connect the various user composable functions, platform components and libraries together and route the events according to an event flow diagram for each use case.","title":"Fourth principle - event choreography"},{"location":"guides/METHODOLOGY/#application-development","text":"Once we have drawn the event flow diagrams for different use cases, we can create user stories for each composable function and assign the development of each function to a developer or a pair of developers if using pair-programming. Composable methodology embraces \"Test Driven Development (TDD)\". Since each function is self-contained, it is TDD friendly because the developer does not need to deal with external dependencies for unit tests. It is the first principle of \"input-process-output\". This methodology allows us to scale our application development resources much better than traditional approach because developers do not need to know details of dependencies, thus avoiding hard code and reducing technical debts. For integration tests, it is easy to mock platform, infrastructure, database and HTTP resources by assigning mock functions to some tasks in an event flow. This greatly simplifies integration mocking.","title":"Application development"},{"location":"guides/METHODOLOGY/#seamless-transition-from-product-to-application-design","text":"Event flow diagram describes the product and each composable function contains the specific processing logic. Data attributes in an event flowing from one function to another provide the clarity for product designers and application engineers to communicate effectively.","title":"Seamless transition from product to application design"},{"location":"guides/METHODOLOGY/#application-packaging-and-deployment","text":"Composable functions are independent and isolated. We can package related functions for an event flow in a single executable. A senior developer or architect can decide how to package related functions to reduce memory footprint and to scale efficiently and horizontally.","title":"Application packaging and deployment"},{"location":"guides/METHODOLOGY/#smaller-memory-footprint","text":"By design, composable functions are granular in nature. They are registered to an \"event loop\" on-demand. i.e. a function is executed when an event arrives. Without multiple levels of tight coupling, each piece of user code consumes memory efficiently. Memory is released to the system as soon as the function finishes execution.","title":"Smaller memory footprint"},{"location":"guides/METHODOLOGY/#composable-framework","text":"To realize the composable design principles, a low-latency in-memory event system is available at the core of the Mercury-Composable framework. Figure 2 - Composable Framework As illustrated in Figure 2, event choreography for an event flow is described as an \"Event Flow Configuration\". An \"Event Manager\" is implemented as part of a low-latency in-memory event system. Composable functions (aka \"task\" in an event flow) ride on the in-memory event system so that the event manager can invoke them by sending events. Since composable functions are self-contained, the event manager performs input/output data mapping to and from the functional scope of a composable function. This flexible data mapping allows developers to write more generic code that responds to different input dataset, thus promoting software reusability. An in-memory state machine will be created for each execution of a transaction or \"event flow\". It is used for holding transaction \"states\" and temporary data objects. Sometimes a transaction may be suspended and restarted with different event flows. The in-memory state machine can be extended to an external datastore so that transaction states can be monitored and recovered operationally. Note : In Java, we combine the use of native Java 21 virtual thread management with Eclipse Vertx event bus. In Node.js, we use the EventEmitter event bus from the standard library.","title":"Composable framework"},{"location":"guides/METHODOLOGY/#event-envelope","text":"We use a standard \"Event Envelope\" to transport an event over the in-memory event bus. An event envelope contains three parts: (1) body, (2) headers and (3) metadata. Event body is used to transport a business object. In Java, it may be a PoJo or a HashMap. In node.js, it is a JSON object. Headers can be used to carry additional parameters to tell the user composable function what to do. Examples for metadata include performance metrics, status, exception, optional tracing information, and correlation ID.","title":"Event Envelope"},{"location":"guides/METHODOLOGY/#language-neutral","text":"The event flow configuration syntax and event envelope serialization scheme are standardized for polyglot deployment. The event flow configuration rides on YAML and event envelope serialization uses binary JSON (\"MsgPack\"). For higher serialization efficiency, the intermediate format is using \"key-value\" maps. JSON string is only used at the inbound and outbound flow adapters.","title":"Language neutral"},{"location":"guides/METHODOLOGY/#functional-isolation","text":"Each function must implement the Composable interface. In Java, it is called \"TypedLambdaFunction\". In Node.js, it is called \"Composable\". The composable interface enforces a single \"handleEvent\" method where the function can access the event's headers and body. By design, each composable function is invoked by events. It is running parallel with other user functions. This non-blocking asynchronous execution architecture delivers higher performance than traditional coding approach with direct method invocation. While each function is executed in an event-driven and reactive manner, the user application code inside each composable function may use any coding style including object-oriented design, functional or reactive. Functional isolation means that you can use any open source or commercial library or framework in your user function without concerns about thread safety or unintended side effect. In Java, a composable function may look like this: @PreLoad(route = \"my.first.function\", instances = 10) public class MyFirstFunction implements TypedLambdaFunction<MyPoJo, AnotherPoJo> { @Override public AnotherPojo handleEvent(Map<String, String> headers, MyPoJo input, int instance) { // your business logic here return result; } } In Node.js, it may look like this: export class MyFirstFunction implements Composable { @preload('my.first.function', 10) initialize(): Composable { return this; } async handleEvent(evt: EventEnvelope) { // your business logic here return result; } A composable function is declared with a \"route name\" using the \"preload\" annotation. While Java and Node.js have different syntax for annotation, they use a similar declarative approach. Note : the \"instance\" count for each composable function controls execution concurrency in a single application. It can be used with horizontal scaling to optimize use of computing resources. Home Chapter-1 Table of Contents Introduction","title":"Functional isolation"},{"location":"guides/TABLE-OF-CONTENTS/","text":"Developer's Guide Mercury Composable is a software development toolkit for writing composable applications. Methodology Chapter 1 - Getting Started Chapter 2 - Function Execution Strategies Chapter 3 - REST Automation Chapter 4 - Event Script Syntax Chapter 5 - Build, Test and Deploy Chapter 6 - Spring Boot Chapter 7 - Event over HTTP Chapter 8 - Minimalist Service Mesh Chapter 9 - API Overview Appendix I - application.properties Appendix II - Reserved names and headers Appendix III - Actuators, HTTP client and More","title":"Contents"},{"location":"guides/TABLE-OF-CONTENTS/#developers-guide","text":"Mercury Composable is a software development toolkit for writing composable applications. Methodology Chapter 1 - Getting Started Chapter 2 - Function Execution Strategies Chapter 3 - REST Automation Chapter 4 - Event Script Syntax Chapter 5 - Build, Test and Deploy Chapter 6 - Spring Boot Chapter 7 - Event over HTTP Chapter 8 - Minimalist Service Mesh Chapter 9 - API Overview Appendix I - application.properties Appendix II - Reserved names and headers Appendix III - Actuators, HTTP client and More","title":"Developer's Guide"}]}